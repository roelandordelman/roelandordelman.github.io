---
---
@article{Noord2021,
   author = {Nanne Van Noord and Christian Gosvig Olesen and Roeland Ordelman and Julia Noordegraaf},
   doi = {10.5220/0010387706330640},
   isbn = {9789897584848},
   issue = {Icaart},
   pages = {633-640},
   title = {Automatic Annotations and Enrichments for Audiovisual Archives},
   volume = {1},
   year = {2021},
}
@misc{ordelman_roeland_2022_6597110,
  author       = {Ordelman, Roeland and
                  Sanders, Willemien and
                  Zijdeman, Richard and
                  Klein, Rana and
                  Noordegraaf, Julia and
                  Van Gorp, Jasmijn and
                  Wigham, Mari and
                  Windhouwer, Menzo},
  title        = {{Data Stories in CLARIAH — Developing a Research
                   Infrastructure for Storytelling with Heritage and
                   Culture Data}},
  month        = may,
  year         = 2022,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.6597110},
  url          = {https://doi.org/10.5281/zenodo.6597110}
}
@article{Noord2021,
   author = {Nanne Van Noord and Christian Gosvig Olesen and Roeland Ordelman and Julia Noordegraaf},
   doi = {10.5220/0010387706330640},
   isbn = {9789897584848},
   issue = {Icaart},
   pages = {633-640},
   title = {Automatic Annotations and Enrichments for Audiovisual Archives},
   volume = {1},
   year = {2021},
}
@inproceedings{,
   abstract = {© 2019 Copyright held by the owner/author(s). The practices of digital humanists are evolving, highly diversified and experimental. There is also a lack of agreement about whether or not digital humanists should have data and programming skills. Thus, their underlying needs for higher levels of flexibility and transparency may be contradicted by their explicit requests for user-friendly graphic user interfaces (GUIs), creating challenges for designing information systems in the digital humanities. This paper describes the experience of designing the Media Suite, which provides access to important Dutch audiovisual collections and is part of the Dutch infrastructure for digital humanities. We outline a solution to the conflicting needs of scholars, by combining a semi-traditional GUI with Jupyter Notebooks. This solution tackles the needs of both novice and advanced users in digital research methods in the humanities. This demonstration paper explains how the Media Suite and the Jupyter notebooks work together, and elaborates on the rationale behind the design choices. We also outline the implications this hybrid and extensible approach has for interface design for the information science and scholarly community.},
   author = {L. Melgar-Estrada and M. Wigham and M. Koolen and K. Beelen and C. Martinez-Ortiz and J. Blom and H. Huurdeman and R. Ordelman},
   doi = {10.1145/3295750.3298918},
   isbn = {9781450360258},
   journal = {CHIIR 2019 - Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
   keywords = {Complex Tasks,Digital Humanities,Research Information Systems},
   title = {The clariah media suite: A hybrid approach to system design in the humanities},
   year = {2019},
}
@inproceedings{Wigham2019,
   abstract = {© 2018 IEEE. To help scholars to extract meaning, knowledge and value from large volumes of archival content, such as the Dutch Common Lab Research Infrastructure for the Arts and Humanities (CLARIAH), we need to provide more 'generous' access to the data than can be provided with generalised search and visualisation tools alone. Our approach is to use Jupyter Notebooks in combination with the existing archive APIs (Application Programming Interface). This gives access to both the archive metadata and a wide range of analysis and visualisation techniques. We have created notebooks and modules of supporting functions that enable the overview, investigation and analysis of the archive. We demonstrate the value of our approach in preliminary tests of its use in scholarly research, and give our observations of the potential value for archivists. Finally, we show that good archive knowledge is essential to create correct and meaningful visualisations and statistics.},
   author = {M. Wigham and L. Melgar and R. Ordelman},
   doi = {10.1109/BigData.2018.8622203},
   isbn = {9781538650356},
   journal = {Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
   title = {Jupyter Notebooks for Generous Archive Interfaces},
   year = {2019},
}
@inproceedings{Reidsma2006,
   abstract = {We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.},
   author = {D. Reidsma and D. Heylen and R. Ordelman},
   journal = {Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC 2006},
   title = {Annotating emotion in meetings},
   year = {2006},
}
@inproceedings{Eskevich2015,
   abstract = {The Search and Anchoring in Video Archives (SAVA) task at MediaEval 2015 consists of two sub-tasks: (i) search for multimedia content within a video archive using multimodal queries referring to information contained in the audio and visual streams/content, and (ii) automatic selection of video segments within a list of videos that can be used as anchors for further hyperlinking within the archive. The task used a collection of roughly 2700 hours of the BBC broadcast TV material for the former sub-task, and about 70 files taken from this collection for the latter sub-task. The search subtask is based on an ad-hoc retrieval scenario, and is evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk (MTurk). The evaluation used metrics that are variations of MAP adjusted for this task. For the anchor selection sub-task overlapping regions of interest across participants submissions were assessed using MTurk workers, and mean reciprocal rank (MRR), precision and recall were calculated for evaluation.},
   author = {M. Eskevich and R. Aly and R. Ordelman and D.N. Racca and S. Chen and G.J.F. Jones},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {SAVA at MediaEval 2015: Search and anchoring in video archives},
   volume = {1436},
   year = {2015},
}
@book{Eskevich2017,
   abstract = {© Springer International Publishing AG 2017. Video-to-video linking systems allow users to explore and exploit the content of a large-scale multimedia collection interactively and without the need to formulate specific queries. We present a short introduction to video-to-video linking (also called ‘video hyperlinking’), and describe the latest edition of the Video Hyperlinking (LNK) task at TRECVid 2016. The emphasis of the LNK task in 2016 is on multi- modality as used by videomakers to communicate their intended message. Crowdsourcing makes three critical contributions to the LNK task. First, it allows us to verify the multimodal nature of the anchors (queries) used in the task. Second, it enables u s to evaluate the performance of video-to- video linking systems at large scale. Third, it gives us insights into how people understand the relevance relationship between two linked video segments. These insights are valuable since the relationship between video segments can manifest itself at different levels of abstraction.},
   author = {M. Eskevich and M. Larson and R. Aly and S. Sabetghadam and G.J.F. Jones and R. Ordelman and B. Huet},
   doi = {10.1007/978-3-319-51814-5_24},
   isbn = {9783319518138},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Crowdsourcing,Link evaluation,Verbal-visual information,Video-to-video linking},
   title = {Multimodal video-to-video linking: Turning to the crowd for insight and evaluation},
   volume = {10133 LNCS},
   year = {2017},
}
@inproceedings{Eskevich2016,
   abstract = {Individual tasks carried out within benchmarking initiatives, or campaigns, enable direct comparison of alternative approaches to tackling shared research challenges and ideally promote new research ideas and foster communities of researchers interested in common or related scientific topics. When a task has a clear predefined use case, it might straightforwardly adopt a well established framework and methodology. For example, an ad hoc information retrieval task adopting the standard Cranfield paradigm. On the other hand, in cases of new and emerging tasks which pose more complex challenges in terms of use scenarios or dataset design, the development of a new task is far from a straightforward process. This letter summarises our reections on our experiences as task organisers of the Search and Hyperlinking task from its origins as a Brave New Task at the MediaEval benchmarking campaign (2011-2014) to its current instantiation as a task at the NIST TRECVid benchmark (since 2015). We highlight the challenges encountered in the development of the task over a number of annual iterations, the solutions found so far, and our process for maintaining a vision for the ongoing advancement of the task's ambition.},
   author = {M. Eskevich and G.J.F. Jones and R. Aly and R. Ordelman and B. Huet},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {Pursuing a moving target: Iterative use of benchmarking of a task to understand the task},
   volume = {1739},
   year = {2016},
}
@inproceedings{Ordelman2001,
   abstract = {In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: The collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.},
   author = {R. Ordelman and A. Van Hessen and F. De Jong},
   isbn = {8790834100},
   journal = {EUROSPEECH 2001 - SCANDINAVIA - 7th European Conference on Speech Communication and Technology},
   title = {Lexicon optimization for Dutch speech recognition in spoken document retrieval},
   year = {2001},
}
@inproceedings{Ordelman2003,
   abstract = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
   author = {R. Ordelman and A. Van Hessen and F. De Jong},
   journal = {EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
   title = {Compound decomposition in Dutch large vocabulary speech recognition},
   year = {2003},
}
@book{Hain2006,
   abstract = {In this paper we describe the 2005 AMI system for the transcription of speech in meetings used in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different frontends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve competitive performance. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {T. Hain and L. Burget and J. Dines and G. Garau and M. Karafiat and M. Lincoln and I. McCowan and D. Moore and V. Wan and R. Ordelman and S. Renals},
   doi = {10.1007/11677482_38},
   isbn = {3540325492 | 9783540325499},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {The 2005 AMI system for the transcription of speech in meetings},
   volume = {3869 LNCS},
   year = {2006},
}
@article{Heeren2009,
   abstract = {Given the enormous backlog at audiovisual archives and the generally global level of item description, collection disclosure and item access are both at risk. At the same time, archival practice is seeking to evolve from the analogue to the digital world. CHoral investigates the role automatic annotation and search technology can play in improving disclosure and access of digitized spoken word collections during and after this transfer. The core business of the CHoral project is to design and build technology for spoken document retrieval for heritage collections. In this paper, we will argue that in addition to solving technological issues, closer attention is needed for the work-fl ow and daily practice at audiovisual archives on the one hand, and the state-of-the-art in technology on the other. Analysis of the interplay is needed to ensure that new developments are mutually benefi cial and that continuing cooperation can indeed bring envisioned advancements. © Institute of Materials, Minerals and Mining 2009.},
   author = {W. Heeren and L. Van Der Werff and F. De Jong and R. Ordelman and T. Verschoor and A. Van Hessen and M. Langelaar},
   doi = {10.1179/174327909X441135},
   issn = {03080188 17432790},
   issue = {2-3},
   journal = {Interdisciplinary Science Reviews},
   title = {Easy listening: Spoken document retrieval in CHoral},
   volume = {34},
   year = {2009},
}
@article{Larson2012,
   abstract = {The MediaEval Multimedia Benchmark leveraged community cooperation and crowdsourcing to develop a large Internet video dataset for its Genre Tagging and Rich Speech Retrieval tasks. © 2012 IEEE.},
   author = {M. Larson and M. Soleymani and M. Eskevich and P. Serdyukov and R. Ordelman and G. Jones},
   doi = {10.1109/MMUL.2012.27},
   issn = {1070986X},
   issue = {3},
   journal = {IEEE Multimedia},
   title = {The community and the crowd: Multimedia benchmark dataset development},
   volume = {19},
   year = {2012},
}
@inproceedings{Eskevich2012,
   abstract = {We present an exploratory study of the retrieval of semiprofessional user-generated Internet video. The study is based on the MediaEval 2011 Rich Speech Retrieval (RSR) task for which the dataset was taken from the Internet sharing platform blip.tv, and search queries associated with specific speech acts occurring in the video. We compare results from three participant groups using: automatic speech recognition system transcript (ASR), metadata manually assigned to each video by the user who uploaded it, and their combination. RSR 2011 was a known-item search for a single manually identified ideal jump-in point in the video for each query where playback should begin. Retrieval effectiveness is measured using the MRR and mGAP metrics. Using different transcript segmentation methods the participants tried to maximize the rank of the relevant item and to locate the nearest match to the ideal jump-in point. Results indicate that best overall results are obtained for topically homogeneous segments which have a strong overlap with the relevant region associated with the jump-in point, and that use of metadata can be beneficial when segments are unfocused or cover more than one topic. © 2012 IEEE.},
   author = {M. Eskevich and G.J.F. Jones and C. Wartena and M. Larson and R. Aly and T. Verschoor and R. Ordelman},
   doi = {10.1109/CBMI.2012.6269810},
   isbn = {9781467323697},
   issn = {19493991},
   journal = {Proceedings - International Workshop on Content-Based Multimedia Indexing},
   title = {Comparing retrieval effectiveness of alternative content segmentation methods for Internet video search},
   year = {2012},
}
@inproceedings{Ordelman2008,
   author = {R. Ordelman and W. Heeren and A. van Hessen and D. Hiemstra and H. Hondorp and M. Huijbregts and F. de Jong and T. Verschoor},
   issn = {15687805},
   journal = {Belgian/Netherlands Artificial Intelligence Conference},
   title = {Browsing and Searching the Spoken Words of Buchenwald Survivors},
   year = {2008},
}
@book{Hain2006,
   abstract = {The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. This paper describes the development of a baseline automatic speech transcription system for meetings in the context of the AMI (Augmented Multiparty Interaction) project. We present several techniques important to processing of this data and show the performance in terms of word error rates (WERs). An important aspect of transcription of this data is the necessary flexibility in terms of audio pre-processing. Real world systems have to deal with flexible input, for example by using microphone arrays or randomly placed microphones in a room. Automatic segmentation and microphone array processing techniques are described and the effect on WERs is discussed. The system and its components presented in this paper yield competitive performance and form a baseline for future research in this domain. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {T. Hain and L. Burget and J. Dines and I. McCowan and G. Garau and M. Karafiat and M. Lincoln and D. Moore and V. Wan and R. Ordelman and S. Renals},
   doi = {10.1007/11677482_30},
   isbn = {3540325492 | 9783540325499},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {The development of the AMI system for the transcription of speech in meetings},
   volume = {3869 LNCS},
   year = {2006},
}
@book{Aly2007,
   abstract = {In this paper the XML Information Retrieval System PF/Tijah is applied to retrieval tasks on large spoken document collections. The used example setting is the English CLEF-2006 CL-SR collection together with given English topics and self produced Dutch topics. The main findings presented in this paper are the easy way of adapting queries to use different kinds and combinations of metadata. Furthermore simple ways of combining different metadata kinds are shown to be beneficial in terms of mean average precision. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {R. Aly and D. Hiemstra and R. Ordelman and L. Van Der Werff and F. De Jong},
   isbn = {9783540749981},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {XML information retrieval from spoken word archives},
   volume = {4730 LNCS},
   year = {2007},
}
@inproceedings{Ordelman2015,
   abstract = {Multimedia hyperlinking is an emerging research topic in the context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hy- perlinking from a video search perspective in the context of the MediaEval evaluation benchmark for several years. Our task considers a use case of exploring large quantities of video content via an automatically created hyperlink struc- ture at the media fragment level. In this paper we report on our findings, examine the features of the definition of video hyperlinking based on results, and discuss lessons learned with respect to evaluation of hyperlinking in real-life use scenarios.},
   author = {R.J.F. Ordelman and M. Eskevich and R. Aly and B. Huet and G.J.F. Jones},
   doi = {10.1145/2740908.2742915},
   isbn = {9781450334730},
   journal = {WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web},
   title = {Defining and evaluating video hyperlinking for navigating multimedia archives},
   year = {2015},
}
@inproceedings{Ordelman2015,
   abstract = {© 2015 ACM.This paper overviews ongoing work that aims to support end-users in conveniently exploring and exploiting large audiovisual archives by deploying multiple multimodal linking approaches. We present ongoing work on multimodal video hyperlinking, from a perspective of unconstrained link anchor identification and based on the identification of named entities, and recent attempts to implement and validate the concept of outside-in linking that relates current events to archive content. Although these concepts are not new, current work is revealing novel insights, more mature technology, development of benchmark evaluations and emergence of dedicated workshops which are opening many interesting research questions on various levels that require closer collaboration between research communities.},
   author = {R.J.F. Ordelman and R. Aly and M. Eskevich and B. Huet and G.J.F. Jones},
   doi = {10.1145/2802558.2814652},
   isbn = {9781450337496},
   journal = {SLAM 2015 - Proceedings of the 2015 Workshop on Speech, Language and Audio in Multimedia, co-located with ACM MM 2015},
   title = {Convenient discovery of archived video using audiovisual hyperlinking},
   year = {2015},
}
@inproceedings{Eskevich2013,
   abstract = {The Search and Hyperlinking Task formed part of the MediaEval 2013 evaluation campaign. The Task consisted of two sub-tasks: (1) answering known-item queries from a collection of roughly 1200 hours of broadcast TV material, and (2) linking anchors within the known-item to other parts of the video collection. We provide an overview of the task and the data sets used.},
   author = {M. Eskevich and R. Aly and R. Ordelman and S. Chen and G.J.F. Jones},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {The search and hyperlinking task at mediaeval 2013},
   volume = {1043},
   year = {2013},
}
@inproceedings{Eskevich2014,
   abstract = {The search and hyperlinking task at MediaEval 2014 is the third edition of this task. As in previous versions, it consisted of two sub-tasks: (i) answering search queries from a collection of roughly 2700 hours of BBC broadcast TV material, and (ii) linking anchor segments from within the videos to other target segments within the video collection. For MediaEval 2014, both sub-tasks were based on an ad-hoc retrieval scenario, and were evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk.},
   author = {M. Eskevich and R. Aly and D.N. Racca and R. Ordelman and S. Chen and G.J.F. Jones},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {The search and hyperlinking task at MediaEval 2014},
   volume = {1263},
   year = {2014},
}
@inproceedings{Eskevich2012,
   abstract = {The Search and Hyperlinking Task was one of the Brave New Tasks at MediaEval 2012. The Task consisted of two sub- tasks which focused on search and linking in retrieval from a collection of semi-professional video content. These tasks followed up on research carried out within the MediaEval 2011 Rich Speech Retrieval (RSR) Task and the VideoCLEF 2009 Linking Task.},
   author = {M. Eskevich and G.J.F. Jones and S. Chen and R. Aly and R. Ordelman and M. Larson},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {Search and hyperlinking task at mediaeval 2012},
   volume = {927},
   year = {2012},
}
@article{Ordelman2009,
   abstract = {This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken heritage archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, we at least want to provide search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition -supporting e.g., within-document search- are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is not yet satisfactory, and requires additional research.},
   author = {R. Ordelman and W. Heeren and F. de Jong and M. Huijbregts and D. Hiemstra},
   issn = {13687506},
   issue = {6},
   journal = {Journal of Digital Information},
   title = {Towards affordable disclosure of spoken heritage archives},
   volume = {10},
   year = {2009},
}
@inproceedings{Aly2013,
   abstract = {Although linking video to additional information sources seems to be a sensible approach to satisfy information needs of user, the perspective of users is not yet analyzed on a fundamental level in real-life scenarios. However, a better understanding of the motivation of users to follow links in video, which anchors users prefer to link from within a video, and what type of link targets users are typically interested in, is important to be able to model automatic linking of audiovisual content appropriately. In this paper we report on our methodology towards eliciting user requirements with respect to video linking in the course of a broader study on user requirements in searching and a series of benchmark evaluations on searching and linking.},
   author = {R. Aly and R.J.F. Ordelman and M. Eskevich and G.J.F. Jones and S. Chen},
   isbn = {9781450320382},
   journal = {WWW 2013 Companion - Proceedings of the 22nd International Conference on World Wide Web},
   title = {Linking inside a video collection - What and how to measure?},
   year = {2013},
}
@inproceedings{Hiemstra2006,
   abstract = {This report presents the University of Twente's first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English crosslanguage spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.},
   author = {D. Hiemstra and R. Ordelman and R. Aly and L. Van Der Werff and F. De Jong},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {Speech retrieval experiments using XML information retrieval},
   volume = {1172},
   year = {2006},
}
@inproceedings{Larson2009,
   author = {M. Larson and R. Ordelman and F. De Jong and W. Kraaij and J. Kohler},
   doi = {10.1145/1631272.1631549},
   isbn = {9781605586083},
   journal = {MM'09 - Proceedings of the 2009 ACM Multimedia Conference, with Co-located Workshops and Symposiums},
   title = {Searching multimedia content with a spontaneous conversational speech track},
   year = {2009},
}
@inproceedings{Larson2010,
   author = {M. Larson and R. Ordelman and F. Metze and W. Kraaij and F. De Jong},
   isbn = {9781450301626},
   journal = {SSCS'10 - Proceedings of the 2010 ACM Workshop on Searching Spontaneous Conversational Speech, Co-located with ACM Multimedia 2010},
   title = {SSCS'10 - Proceedings of the 2010 ACM Workshop on Searching Spontaneous Conversational Speech, Co-located with ACM Multimedia 2010: Foreword},
   year = {2010},
}
@book{Kemman2014,
   abstract = {Scholars are yet to make optimal use of Oral History collections. For the uptake of digital research tools in the daily working practice of researchers, practices and conventions commonly adhered to in the subfields of the humanities should be taken into account during development, in order to facilitate the uptake of digital research tools in the daily working practice of researchers. To this end, in the Oral History Today project a research tool for exploring Oral History collections is developed in close collaboration with scholarly researchers. This paper describes four stages of scholarly research and the first steps undertaken to incorporate requirements of these stages in a digital research environment. © Springer International Publishing Switzerland 2014.},
   author = {M. Kemman and S. Scagliola and F. de Jong and R. Ordelman},
   doi = {10.1007/978-3-319-08425-1_22},
   isbn = {9783319084244},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   title = {Talking with Scholars: Developing a Research Environment for Oral History Collections},
   volume = {416 CCIS},
   year = {2014},
}
@article{Theune2006,
   abstract = {Work on expressive speech synthesis has long focused on the expression of basic emotions. In recent years, however, interest in other expressive styles has been increasing. The research presented in this paper aims at the generation of a storytelling speaking style, which is suitable for storytelling applications and more in general, for applications aimed at children. Based on an analysis of human storytellers' speech, we designed and implemented a set of prosodic rules for converting "neutral" speech, as produced by a text-to-spccch system, into storytelling speech. An evaluation of our storytelling speech generation system showed encouraging results. © 2006 IEEE.},
   author = {M. Theune and K. Meijs and D. Heylen and R. Ordelman},
   doi = {10.1109/TASL.2006.876129},
   issn = {15587916},
   issue = {4},
   journal = {IEEE Transactions on Audio, Speech and Language Processing},
   title = {Generating expressive speech for storytelling applications},
   volume = {14},
   year = {2006},
}
@inproceedings{Larson2009,
   author = {M. Larson and R. Ordelman and F. De Jong and J. Kohler and W. Kraaij},
   isbn = {9781605587622},
   journal = {3rd Workshop on Searching Spontaneous Conversational Speech, SSCS'09, Co-located with the 2009 ACM International Conference on Multimedia, MM'09},
   title = {3rd Workshop on Searching Spontaneous Conversational Speech, SSCS 09, Co-located with the 2009 ACM International Conference on Multimedia, MM 09: Foreword},
   year = {2009},
}
@inproceedings{Larson2010,
   author = {M. Larson and R. Ordelman and F. Metze and W. Kraaij and F. De Jong},
   doi = {10.1145/1873951.1874351},
   isbn = {9781605589336},
   journal = {MM'10 - Proceedings of the ACM Multimedia 2010 International Conference},
   title = {Multimedia content with a speech track: ACM multimedia 2010 workshop on searching spontaneous conversational speech},
   year = {2010},
}
@book{Zhang2009,
   abstract = {StreetTiVo is a project that aims at bringing research results into the living room; in particular, a mix of current results in the areas of Peer-to-Peer XML Database Management System (P2P XDBMS), advanced multimedia analysis techniques, and advanced information retrieval techniques. The project develops a plug-in application for the so-called Home Theatre PCs, such as set-top boxes with MythTV or Windows Media Center Edition installed, that can be considered as programmable digital video recorders. StreetTiVo distributes computeintensive multimedia analysis tasks over multiple peers (i.e., StreetTiVo users) that have recorded the same TV program, such that a user can search in the content of a recorded TV program shortly after its broadcasting; i.e., it enables near real-time availability of the meta-data (e.g., speech recognition) required for searching the recorded content. Street- TiVo relies on our P2P XDBMS technology, which in turn is based on a DHT overlay network, for distributed collaborator discovery, work coordination and meta-data exchange in a volatile WAN environment. The technologies of video analysis and information retrieval are seamlessly integrated into the system as XQuery functions. © Springer-Verlag Berlin Heidelberg 2009.},
   author = {Y. Zhang and A.D. Vries and P. Boncz and D. Hiemstra and R. Ordelman},
   doi = {10.1007/978-3-642-00672-2-36},
   isbn = {9783642006715},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {StreetTiVo: Using a P2P xml database system to manage multimedia data in your living room},
   volume = {5446},
   year = {2009},
}
@book{Ordelman2001,
   abstract = {© Springer-Verlag Berlin Heidelberg 2001.In this paper, ongoing work on the development of the speech recognition modules of MMIR environment for Dutch is described. The work on the generation of acoustic models and language models along with their current performance is presented. Some characteristics of the Dutch language and of the target video archives that require special treatment are discussed.},
   author = {R. Ordelman and A. van Hessen and F. de Jong},
   isbn = {9783540425571},
   issn = {16113349 03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Speech recognition issues for Dutch spoken document retrieval},
   volume = {2166},
   year = {2001},
}
@book{Baltussen2013,
   abstract = {Video hyperlinking is regarded as a means to enrich interactive television experiences. Creating links manually however has limitations. In order to be able to automate video hyperlinking and increase its potential we need to have a better understanding of how both broadcasters that supply interactive television and the end-users approach and perceive hyperlinking. In this paper we report on the development of an editor tool for supervised automatic video hyperlinking that will allow us to investigate video hyperlinking in a real-life scenario. © Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2013.},
   author = {L.B. Baltussen and J. Blom and R. Ordelman},
   doi = {10.1007/978-3-319-03892-6_5},
   isbn = {9783319038919},
   issn = {18678211},
   journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
   title = {VideoHypE: An editor tool for supervised automatic video hyperlinking},
   volume = {124 LNICST},
   year = {2013},
}
@inproceedings{Larson2009,
   abstract = {Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative flow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers' feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches are tied in delivering top narrative peak detection results. One uses counts of first and second person pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language.},
   author = {M. Larson and B. Jochems and E. Smits and R. Ordelman},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {Exploiting speech recognition transcripts for narrative peak detection in short-form documentaries},
   volume = {1175},
   year = {2009},
}
@book{Nadeem2013,
   abstract = {This paper reports on the results of a quantitative analysis of user requirements for audiovisual search that allow the categorisation of requirements and to compare requirements across user groups. The categorisation provides clear directions with respect to the prioritisation of system features from the perspective of the development of systems for specific, single user groups and systems that have a more general target user group. © 2013 Springer-Verlag.},
   author = {D. Nadeem and R. Ordelman and R. Aly and E. Verbruggen},
   doi = {10.1007/978-3-642-40501-3_24},
   isbn = {9783642405006},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Users requirements in audiovisual search: A quantitative approach},
   volume = {8092 LNCS},
   year = {2013},
}
@book{Dadvar2012,
   abstract = {Friendships, relationships and social communications have all gone to a new level with new definitions as a result of the invention of online social networks. Meanwhile, alongside this transition there is increasing evidence that online social applications have been used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the users involved in cyberbullying. We hypothesis that incorporation of the users' profile, their characteristics, and post-harassing behaviour, for instance, posting a new status in another social network as a reaction to their bullying experience, will improve the accuracy of cyberbullying detection. Cross-system analyses of the users' behaviour - monitoring users' reactions in different online environments - can facilitate this process and could lead to more accurate detection of cyberbullying. This paper outlines the framework for this faceted approach. © 2012 Springer-Verlag.},
   author = {M. Dadvar and R. Ordelman and F. De Jong and D. Trieschnigg},
   doi = {10.1007/978-3-642-31178-9_34},
   isbn = {9783642311772},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Towards user modelling in the combat against cyberbullying},
   volume = {7337 LNCS},
   year = {2012},
}
@inproceedings{Nadeem2015,
   abstract = {© 2014 IEEE.Semantic linking has a potential to enrich the audiovisual experience for users of television or radio broadcast archives. Recently, automatic semantic linking, has received increased attention, especially as second screen applications for television broadcasts are emerging. Semantic linking for radio broadcasts can enrich radio listening experience in a similar manner in combination with second screen-like applications. While the development of such applications is gaining popularity, little is known about the information in a radio program that may be interesting for link creation from a user perspective. We conducted a user study on semantic linking for radio broadcasts in order to know what information users regard as suitable anchors and what kind of information they like as targets. We found that users often regard topic and person as the best link anchors in the program. Additionally, we found that frequency and timing of information elements in a radio program do not dominate the users' selection of anchors. Furthermore, we found that there is a low agreement among users on regarding certain information elements as anchors. For practical reasons the study is conducted with 10 minutes of radio broadcast material of a particular program type, and with a total of 22 participants. The insights gained in the user study will help the understanding of user perspectives on semantic linking in the audio domain.},
   author = {D. Nadeem and R. Ordelman and R. Aly and F.D. De Jong},
   doi = {10.1109/SITIS.2014.47},
   isbn = {9781479979783},
   journal = {Proceedings - 10th International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2014},
   title = {User perspectives on semantic linking in the audio domain},
   year = {2015},
}
@inproceedings{Nadeem2012,
   abstract = {In this paper we report our experiments and results for the brave new searching and hyperlinking tasks for the MediaEval Benchmark Initiative 2012. The searching task involves finding target video segments based on a short natural language sentence query and the hyperlinking task involves finding links from the target video segments to other related video segments in the collection using a set of anchor segments in the videos that correspond to the textual search queries. To find the starting points in the video, we only used speech transcripts and metadata as evidence source, however, other visual features (for e.g., faces, shots and keyframes) might also affect results for a query. We indexed speech transcripts and metadata, furthermore, the speech transcripts were indexed at speech segment level and at sentence level to improve the likelihood of finding jump-inpoints. For linking video segments, we computed k-nearest neighbours of video segments using euclidean distance.},
   author = {D. Nadeem and R. Aly and R. Ordelman},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {UTwente does brave new tasks for MediaEval 2012: Searching and hyperlinking},
   volume = {927},
   year = {2012},
}
@inproceedings{Huijbregts2008,
   abstract = {Decoders that make use of token-passing restrict their search space by various types of token pruning. With use of the Language Model Look-Ahead (LMLA) technique it is possible to increase the number of tokens that can be pruned without loss of decoding precision. Unfortunately, for token passing decoders that use single static pronunciation prefix trees, full n-gram LMLA increases the needed number of language model probability calculations considerably. In this paper a method for applying full n-gram LMLA in a decoder with a single static pronunciation tree is introduced. The experiments show that this method improves the speed of the decoder without an increase of search errors. Copyright © 2008 ISCA.},
   author = {M. Huijbregts and R. Ordelman and F. De Jong},
   issn = {19909772},
   journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
   title = {Fast N-gram language model look-ahead for decoders with static pronunciation prefix trees},
   year = {2008},
}
@inproceedings{Schouten2013,
   abstract = {This paper reports a set of experiments performed in the context of the Searching and Hyperlinking task of the MediaEval Benchmark Initiative 2013. The Searching part challenges to return a ranked list of video segments that are relevant given some textual user query, while for the Hy-perlinking task the aim is to return a ranked list of video segments that are relevant given some video segment. The main focus is on finding a way to compute flexible segment boundaries. This is performed by extending the term frequency part of tf-idf to include the temporal dimension of videos. Although the contribution is theoretically sound its performance is relatively poor, which we attribute to the focus on speech data and the hyperlinking process. We plan to refine our method in the future overcome these limitations.},
   author = {K. Schouten and R. Aly and R. Ordelman},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   title = {Searching and hyperlinking using word importance segment boundaries in MediaEval 2013},
   volume = {1043},
   year = {2013},
}
@book{Larson2010,
   abstract = {Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative flow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers' feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches deliver top narrative peak detection results. One uses counts of personal pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {M. Larson and B. Jochems and E. Smits and R. Ordelman},
   doi = {10.1007/978-3-642-15751-6_50},
   isbn = {3642157505 | 9783642157509},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Exploiting speech recognition transcripts for narrative peak detection in short-form documentaries},
   volume = {6242 LNCS},
   year = {2010},
}
@inproceedings{Gravier2015,
   abstract = {© 2015 ACM.The Workshop on Speech, Language and Audio in Multimedia (SLAM) positions itself at at the crossroad of multiple scientific fields?music and audio processing, speech processing, natural language processing and multimedia?to discuss and stimulate research results, projects, datasets and benchmarks initiatives where audio, speech and language are applied to multimedia data. While the first two editions were collocated with major speech events, SLAM'15 is deeply rooted in the multimedia community, opening up to computer vision and multimodal fusion. To this end, the workshop emphasizes video hyperlinking as an showcase where computer vision meets speech and language. Such techniques provide a powerful illustration of how multimedia technologies incorporating speech, language and audio can make multimedia content collections better accessible, and thereby more useful, to users.},
   author = {G. Gravier and G.J.F. Jones and M. Larson and R. Ordelman},
   doi = {10.1145/2733373.2806414},
   isbn = {9781450334594},
   journal = {MM 2015 - Proceedings of the 2015 ACM Multimedia Conference},
   keywords = {[Audio, Language, Multimedia, Speech]},
   title = {Overview of the 2015 workshop on speech, language and audio in multimedia},
   year = {2015},
}
@inproceedings{Huijbregts2009,
   abstract = {In this paper we present our primary submission to the first Dutch and Flemish large vocabulary continuous speech recognition benchmark, N-Best. We describe our system workflow, the models we created for the four evaluation tasks and how we approached the problem of compounding that is typical for a language such as Dutch. We present the evaluation results and our post-evaluation analysis. Copyright © 2009 ISCA.},
   author = {M. Huijbregts and R. Ordelman and L. Van Der Werff and F. De Jong},
   issn = {19909772},
   journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
   keywords = {[Benchmark, LVCSR]},
   title = {SHoUT, the University of Twente submission to the N-best 2008 speech recognition evaluation for Dutch},
   year = {2009},
}
@inproceedings{Gravier2015,
   author = {G. Gravier and G.F. Jones and M. Larson and R. Ordelman},
   isbn = {9781450337496},
   journal = {SLAM 2015 - Proceedings of the 2015 Workshop on Speech, Language and Audio in Multimedia, co-located with ACM MM 2015},
   title = {Chairs' welcome message},
   year = {2015},
}
@inproceedings{Baltussen2016,
   abstract = {© 2015 IEEE.Archives of cultural heritage organisations typically consist of collections in various formats (e.g. photos, video, texts) that are inherently related. Often, such disconnected collections represent value in itself but effectuating links between 'core' and 'context' collection items in various levels of granularity could result in a 'one-plus-one-makes-three' scenario both from a contextualisation perspective (public presentations, research) and access perspective. A key issue is the identification of contextual objects that can be associated with objects in the core collections, or the other way around. Traditionally, such associations have been created manually. For most organizations however, this approach does not scale. In this paper, we describe a case in which a semi-automatic approach was employed to create contextual links between television broadcast schedules in program guides (context collection) and the programs in the archive (core collection) of a large audiovisual heritage organisation.},
   author = {L.B. Baltussen and T. Karavellas and R.J.F. Ordelman},
   doi = {10.1109/DigitalHeritage.2015.7419477},
   isbn = {9781509000487},
   journal = {2015 Digital Heritage International Congress, Digital Heritage 2015},
   title = {Exploiting program guides for contextualisation},
   year = {2016},
}
@book{Dadvar2013,
   abstract = {The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited. Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users' characteristics and profile information. In this paper we show that taking user context into account improves the detection of cyberbullying. © 2013 Springer-Verlag.},
   author = {M. Dadvar and D. Trieschnigg and R. Ordelman and F. De Jong},
   doi = {10.1007/978-3-642-36973-5_62},
   isbn = {9783642369728},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Improving cyberbullying detection with user context},
   volume = {7814 LNCS},
   year = {2013},
}
@inproceedings{Ordelman2011,
   abstract = {Safeguarding the massive body of audiovisual content, including rich music collections, in audiovisual archives and enabling access for various types of user groups is a prerequisite for unlocking the social-economic value of these collections. Data quantities and the need for specific content descriptors however, force archives to re-evaluate their annotation strategies and access models, and incorporate technology in the archival workflow. It is argued that this can only be successfully done provided that user requirements are studied well and that new approaches are introduced in a well-balanced manner, fitting in with traditional archival perspectives, and by bringing the archivist in the technology loop by means of education and by deploying hybrid work-flows for technology aided annotation. © 2011 ACM.},
   author = {R. Ordelman},
   doi = {10.1145/2072529.2072535},
   isbn = {9781450309868},
   journal = {MM'11 - Proceedings of the 2011 ACM Multimedia Conference and Co-Located Workshops - MIRUM 2011 Workshop, MIRUM'11},
   title = {Audiovisual archive exploitation in the networked information society},
   year = {2011},
}
@book{Huijbregts2007,
   abstract = {This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {M. Huijbregts and R. Ordelman and F. De Jong},
   isbn = {9783540770336},
   issn = {03029743 16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Annotation of heterogeneous multimedia content using automatic speech recognition},
   volume = {4816 LNCS},
   year = {2007},
}
@inproceedings{,
   abstract = {Access to historical audio collections is typically very restricted: content is often only available on physical (analog) media and the metadata is usually limited to keywords, giving access at the level of relatively large fragments, e.g., an entire tape. Many spoken word heritage collections are now being digitized, which allows the introduction of more advanced search technology. This paper presents an approach that supports online access and search for recordings of historical speeches. A demonstrator has been built, based on the so-called Radio Oranje collection, which contains radio speeches by the Dutch Queen Wilhelmina that were broadcast during World War II. The audio has been aligned with its original 1940s manual transcriptions to create a time-stamped index that enables the speeches to be searched at the word level. Results are presented together with related photos from an external database.},
   author = {L. Van Der Werff and W. Heeren and R. Ordelman and F. De Jong},
   isbn = {9789078328414},
   journal = {Proceedings of the 17th Meeting of Computational Linguistics in the Netherlands, CLIN17},
   title = {Radio oranje: Enhanced access to a historical spoken word collection},
   year = {2007},
}
@inproceedings{Ordelman2006,
   author = {R. Ordelman and F. de Jong and W. Heeren and A. van Hessen},
   issn = {15687805},
   journal = {Belgian/Netherlands Artificial Intelligence Conference},
   title = {Audio indexing technology for the exploration of audiovisual heritage collections},
   year = {2006},
}
@book{,
   abstract = {© Springer International Publishing Switzerland 2015.In this paper we report on an evaluation of unsupervised labeling of audiovisual content using collateral text data sources to investigate how such an approach can provide acceptable results given requirements with respect to archival quality, authority and service levels to external users. We conclude that with parameter settings that are optimized using a rigorous evaluation of precision and accuracy, the quality of automatic term-suggestion are sufficiently high. Having implemented the procedure in our production work-flow allows us to gradually develop the system further and also assess the effect of the transformation from manual to automatic from an end-user perspective. Additional future work will be on deploying different information sources including annotations based on multimodal video analysis such as speaker recognition and computer vision.},
   author = {V. De Boer and R.J.F. Ordelman and J. Schuurman},
   doi = {10.1007/978-3-319-24592-8_4},
   isbn = {9783319245911},
   issn = {16113349 03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Practice-oriented evaluation of unsupervised labeling of audiovisual content in an archive production environment},
   volume = {9316},
   year = {2015},
}
@inproceedings{Ordelman2015,
   abstract = {Hyperlinking is an emerging research topic in the context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hyperlinking in the context of a benchmark evaluation task considering a use case of exploring large quantities of video content via a link structure on video segment level. In this paper we report on our ndings, focus on the de nition of video hyperlinking on the basis of our results in previous evaluations, and discuss our evaluation paradigms for automatic anchor selection and link target generation in real-life use scenarios.},
   author = {Roeland JF Ordelman and Maria Eskevich and Robin Aly and Benoit Huet and Gareth Jones},
   city = {Florence, Italy},
   doi = {10.1145/2740908.2742915},
   journal = {Proceedings of the 24th International Conference on World Wide Web Companion},
   note = {ISBN: 978-1-4503-3473-0},
   pages = {727--732},
   title = {Defining and Evaluating Video Hyperlinking for Navigating Multimedia Archives},
   year = {2015},
}
@inproceedings{Eskevich2013,
   author = {Maria Eskevich and Tom de Nies and Pedro Debevere and Rik Van de Walle and Petra Galuscakova and Pavel Pecina and Martha Larson and Gareth J.F. Jones and Robin Aly and Roeland J.F. Ordelman and Shu Chen and Danish Nadeem and Camille Guinaudeau and Guillaume Gravier and Pascale Sébillot},
   city = {New York, New York, USA},
   doi = {10.1145/2461466.2461511},
   isbn = {9781450320337},
   journal = {Proceedings of the 3rd ACM conference on International conference on multimedia retrieval - ICMR '13},
   keywords = {audio-visual hyperlinking,audio-visual search},
   month = {4},
   pages = {287},
   publisher = {ACM Press},
   title = {Multimedia information seeking through search and hyperlinking},
   url = {http://dl.acm.org/citation.cfm?id=2461466.2461511},
   year = {2013},
}
@inproceedings{eemcs8075,
   abstract = {This paper discusses audio indexing tools that have been implemented for the disclosure of Dutch audiovisual cultural heritage collections. It explains the role of language models and their adaptation to historical settings and the adaptation of acoustic models for homogeneous audio collections. In addition to the benefits of cross-media linking, the requirements for successful tuning and improvement of available tools for indexing the heterogeneous A/V collections from the cultural heritage domain are reviewed. And finally the paper argues that research is needed to cope with the varying information needs for different types of users. },
   author = {R J F Ordelman and F M G de Jong and W F L Heeren},
   city = {Amsterdam},
   editor = {G Brewka and S Coradeschi and A Perini and P Traverso},
   isbn = {1-58603-642-4},
   journal = {Proceedings of the 17th European Conference on Artificial Intelligence (ECAI2006), Trento, Italy},
   publisher = {IOS Press},
   title = {Exploration of audiovisual heritage using audio indexing technology},
   year = {2006},
}
@report{eemcs15978,
   author = {R J F Ordelman and M A H Huijbregts and F M G de Jong},
   city = {Enschede},
   issn = {1381-3625},
   issue = {TR-CTIT-05-72},
   institution = {Centre for Telematics and Information Technology University of Twente},
   month = {9},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Unravelling the Voice of Willem Frederik Hermans: an Oral History Indexing Case Study},
   year = {2009},
}
@inproceedings{eemcs10825,
   abstract = {The `Radio Oranje' demonstrator shows an attractive multimedia user experience in the cultural heritage domain based on a collection of mono-media audio documents. It supports online search and browsing of the collection using indexing techniques, specialized content visualizations and a related photo database. },
   author = {W F L Heeren and L B van der Werff and R J F Ordelman and A J van Hessen and F M G de Jong},
   city = {New York},
   editor = {C L A Clarke and N Fuhr and N Kando and W Kraaij and A P de Vries},
   journal = {Proceedings of the 30th ACM SIGIR, Amsterdam},
   keywords = {Graphical User Interfaces (GUI),Information Visualization,Multimedia Retrieval,Spoken Document Retrieval},
   month = {7},
   pages = {903},
   publisher = {ACM},
   title = {Radio Oranje: Searching the Queen's speech(es)},
   year = {2007},
}
@article{Ordelman137:2000,
   abstract = {On attaching automatic search functionality to historical video archives},
   author = {R J F Ordelman},
   city = {Amsterdam},
   issn = {1385-5328},
   issue = {12},
   journal = {Informatie professional},
   keywords = {audio search,cultural heritage},
   pages = {24-28},
   publisher = {Otto Cramwinckel Uitgever},
   title = {Zoeken in historisch videomateriaal},
   volume = {4},
   year = {2000},
}
@inproceedings{hain05_amisystemtran,
   author = {T Hain and J Dines and G Gaurau and M Karafiat and D Moore and V Wan and R J F Ordelman and S Renals},
   journal = {Proc. Rich Transcription 2005 Spring (RT05s) Meeting Recognition Evaluation},
   title = {The 2005 AMI System for the Transcription of Speech in Meetings},
   year = {2005},
}
@inproceedings{eemcs8073,
   abstract = {The deployment and integration of audio processing tools can enhance the semantic annotation of multimedia content, and as a consequence, improve the effectiveness of conceptual access tools. This paper overviews the various ways in which automatic speech and audio analysis can contribute to increased granularity of automatically extracted metadata. A number of techniques will be presented, including the alignment of speech and text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value of the techniques and their potential contribution to the content value chain will be illustrated by the description of two (complementary) demonstrators for browsing broadcast news archives.},
   author = {F M G de Jong and R J F Ordelman and M A H Huijbregts},
   city = {Berlin},
   editor = {Y Avrithis and Y Kompatsiaris and S Staab and N E O'Connor},
   isbn = {3-540-49335-2},
   journal = {Proceedings of the First International Conference on Semantic and Digital Media Technologies, SAMT 2006, Athens, Greece},
   month = {12},
   pages = {226-240},
   publisher = {Springer Verlag},
   title = {Automated speech and audio analysis for semantic access to multimedia},
   volume = {4306},
   year = {2006},
}
@inproceedings{Larson2011,
   author = {M Larson and M Eskevich and R Ordelman and C Kofler and S Schmiedeke and G J F Jones},
   city = {Pisa, Italy},
   journal = {MediaEval 2011 Workshop},
   month = {9},
   title = {Overview of MediaEval 2011 Rich Speech Retrieval Task and Genre Tagging Task},
   year = {2011},
}
@inproceedings{eemcs17000,
   abstract = {Spoken document retrieval research effort invested into developing broadcast news retrieval systems has yielded impressive results. This paper is the introduction the proceedings of the 3rd workshop aiming at the advancement of the field in less explored domains (SSCS2009) which was organized in conjunction to the ACM Multimedia Conference in Beijing. },
   author = {M A Larson and R J F Ordelman and F M G de Jong and W Kraaij and J Kohler},
   journal = {Proceedings of the Third Workshop on Searching Spontaneous Conversational Speech (SSCS2009), Beijing},
   keywords = {audio-visual retrieval,multimedia access,speech,speech recognition,spoken content},
   month = {10},
   pages = {2-4},
   publisher = {ACM},
   title = {Searching Multimedia Content with a Spontaneous Conversational Speech Track},
   year = {2009},
}
@inproceedings{eemcs11003,
   abstract = {In this paper we discuss the speech activity detection system that we used for detecting speech regions in the Dutch TRECVID video collection. The system is designed to filter non-speech like music or sound effects out of the signal without the use of predefined non-speech models. Because the system trains its models on-line, it is robust for handling out-of-domain data. The speech activity error rate on an out-of-domain test set, recordings of English conference meetings, was 4.4%. The overall error rate on twelve randomly selected five minute TRECVID fragments was 11.5%.},
   author = {M A H Huijbregts and C Wooters and R J F Ordelman},
   city = {Antwerp},
   issn = {1990-9772},
   journal = {Proceedings of Interspeech 2007, Antwerp, Belgium},
   keywords = {speech activity detection},
   month = {8},
   pages = {FrC.P3-4},
   publisher = {International Speech Communication Association},
   title = {Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections},
   year = {2007},
}
@inproceedings{eemcs12872,
   abstract = {The re-use of spoken word audio collections maintained by audiovisual archives is severely hindered by their generally limited access. The CHoral project, which is part of the CATCH program funded by the Dutch Research Council, aims to provide users of speech archives with online, instead of on-location, access to relevant fragments, instead of full documents. To meet this goal, a spoken document retrieval framework is being developed. In this paper the evaluation efforts undertaken so far to assess and improve various aspects of the framework are presented. These efforts include (i) evaluation of the automatically generated textual representations of the spoken word documents that enable word-based search, (ii) the development of measures to estimate the quality of the textual representations for use in information retrieval, and (iii) studies to establish the potential user groups of the to-be-developed technology, and the first versions of the user interface supporting online access to spoken word collections.},
   author = {W F L Heeren and F M G de Jong and L B van der Werff and M A H Huijbregts and R J F Ordelman},
   isbn = {2-9517408-4-0},
   journal = {Proceedings of LREC 2008, Marrakech},
   pages = {520},
   publisher = {European Language Resources Association (ELRA)},
   title = {Evaluation of spoken document retrieval for historic speech collections},
   year = {2008},
}
@inproceedings{eemcs12904,
   abstract = {In addition to multimedia collections and their metadata,
there often is a variety of collateral data sources available on
(parts of) a collection. Collateral data - secondary information
objects that relate to the primary multimedia documents
- can be very useful in the process of automated generation of
annotations for multimedia archives in that they reduce both
costs and effort in annotation and access. Furthermore, they
can be used to enhance result presentation in retrieval engines.
To optimally exploit collateral data, methods for automatic indexing
as well as changes in the current archiving workflow
are proposed.},
   author = {W F L Heeren and R J F Ordelman and F M G de Jong},
   journal = {Proceedings of CBMI 2008, London},
   month = {6},
   pages = {542-550},
   publisher = {IEEE},
   title = {Affordable access to multimedia by exploiting collateral data},
   year = {2008},
}
@inproceedings{eemcs16069,
   abstract = {Techniques for automatic annotation of spoken
content making use of speech recognition technology have long
been characterized as holding unrealized promise to provide
access to archives inundated with undisclosed multimedia material.
This paper provides an overview of techniques and trends
in semantic speech retrieval, which is taken to encompass
all approaches offering meaning-based access to spoken word
collections. We present descriptions, examples and insights for
current techniques, including facing real-world heterogenity,
aligning parallel resources and exploiting collateral collections.
We also discuss ways in which speech recognition technology
can be used to create multimedia connections that make new
modes of access available to users. We conclude with an
overview of the challenges for semantic speech retrieval in the
workflow of a real-world archive and perspectives on future
tasks in which speech retrieval integrates information related
to affect and appeal, dimensions that transcend topic.},
   author = {R J F Ordelman and F M G de Jong and M A Larson},
   city = {Berkeley},
   journal = {Proceedings of the Third IEEE International Conference on Semantic Computing, Berkeley, US},
   keywords = {multimedia retrieval and access,semantics,speech recognition,speech retrieval,spoken content},
   month = {9},
   pages = {521-528},
   publisher = {IEEE Computer Society},
   title = {Enhanced multimedia content access and exploitation using semantic speech retrieval},
   year = {2009},
}
@article{eemcs15098,
   abstract = {This contribution describes the Twente News Corpus (TwNC), a multifaceted corpus for Dutch that is being deployed in a number of NLP research projects among which tracks within the Dutch national research programme MultimediaN, the NWO programme CATCH, and the Dutch-Flemish programme STEVIN.

The development of the corpus started in 1998 within a predecessor project DRUID and has currently a size of 530M words. The text part has been built from texts
of four different sources: Dutch national newspapers, television subtitles, teleprompter
(auto-cues) files, and both manually and automatically generated broadcast news transcripts
along with the broadcast news audio. TwNC plays a crucial role in the
development and evaluation of a wide range of tools and applications for the
domain of multimedia indexing, such as large vocabulary speech recognition,
cross-media indexing, cross-language information retrieval etc. Part of the corpus was fed into
the Dutch written text corpus in the context of the Dutch-Belgian STEVIN project D-COI that
was completed in 2007. The sections below will describe the rationale that was the starting point
for the corpus development; it will outline the cross-media linking approach adopted within
MultimediaN, and finally provide some facts and figures about the corpus.},
   author = {R J F Ordelman and F M G de Jong and A J van Hessen and G H W Hondorp},
   issn = {not assigned},
   journal = {ELRA Newsleter},
   keywords = {multimedia retrieval,speech recognition,text corpora},
   publisher = {ELRA},
   title = {TwNC: a Multifaceted Dutch News Corpus},
   volume = {12},
   year = {2007},
}
@article{eemcs11498,
   abstract = {The ACM SIGIR Workshop on Searching Spontaneous Conversational Speech was held as part of the 2007 ACM SIGIR Conference in Amsterdam.
The workshop program was a mix of elements, including a keynote speech, paper presentations and panel discussions. This brief report describes the organization of this workshop and summarizes the discussions.},
   author = {F M G de Jong and D W Oard and R J F Ordelman and S Raaijmakers},
   issn = {0163-5840},
   issue = {2},
   journal = {ACM SIGIR Forum},
   month = {12},
   pages = {104-108},
   publisher = {ACM SIGIR},
   title = {Searching Spontaneous Conversational Speech},
   volume = {41},
   year = {2007},
}
@book{eemcs14960,
   abstract = {The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM
SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the
close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval.
The proceedings contain papers on a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech
recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. A workshop reprot has been published in ACM SIGIR Forum, issue December 2008.},
   city = {Enschede},
   editor = {J Köhler and M Larson and F M G de Jong and R J F Ordelman and W Kraaij},
   keywords = {speech recognition,spoken document retrieval},
   month = {7},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Searching Spontaneous Conversational Speech. Proceedings of ACM SIGIR Workshop (SSCS2008)},
   year = {2008},
}
@book{eemcs19423,
   abstract = {The spoken word is a valuable source of semantic information. Techniques that exploit the spoken word by making use of speech recognition or spoken audio analysis hold clear potential for improving multimedia search. Nonetheless, speech technology remains underexploited by systems that provide access to spoken audio or video with a speech track. Indexing the spoken audio produced by speakers engaging in conversation or otherwise speaking spontaneously is particularly challenging. The challenges arise due to the wide variability and highly unstructured nature of unplanned, informal speech. Development of approaches that can effectively exploit the semantic content of spontaneous, conversational speech requires integration of speech recognition, audio processing, multimedia analysis and information retrieval. The SSCS workshop series is devoted to providing a forum where scientists engaged in spoken content retrieval research at the intersection of these disciplines can meet, present and discuss recent research results and also formulate a common vision on the future of spoken content retrieval.

The research papers presented at SSCS 2010 cluster around topics that are central for spoken content retrieval. Two papers focus on specific indexing techniques applied to spontaneous speech: speaker role recognition and concept detection. Two papers treat Spoken Term Detection, addressing the challenge of terms that cannot be indexed using conventional approaches since they are not contained in the speech recognizer vocabulary (i.e., the so-called Out-Of-Vocabulary problem). Finally, three papers are devoted to topics related to the automatic segmentation of spontaneous conversational content and deal with issues involving the combination of automatic segmentation and information retrieval. SSCS 2010 continues the tradition of past years by including a demonstration session that allows hands-on interaction with systems implementing state-of-the-art approaches to spoken content retrieval. Five demonstration papers give the details of the systems presented. SSCS 2010 includes a number of presentations by invited speakers who address topics related to the user perspective on spoken content retrieval and to domains that are anticipated to give rise to future issues faced by scientists working in the field.


},
   city = {New York},
   editor = {M Larson and R J F Ordelman and F Metze and W Kraaij and F M G de Jong},
   month = {10},
   publisher = {ACM},
   title = {Proceedings of the 2010 international workshop on searching spontaneous conversational speech. (Workshop at ACM Multimedia 2010, Florence.)},
   year = {2010},
}
@article{eemcs13538,
   author = {R J F Ordelman},
   city = {Nijmegen},
   issn = {1572-6037},
   issue = {3},
   journal = {DIXIT, Tijdschrift voor toegepaste Taal- en Spraaktechnologie},
   month = {10},
   pages = {15-17},
   publisher = {Dialoog Uitgevers},
   title = {De stem van Willem Frederik Hermans ontrafeld. Audiovisuele archieven ge\{ï\}ndexeerd},
   volume = {3e jaargan},
   year = {2005},
}
@inproceedings{aly2007,
   abstract = { Bridging the semantic gap is one of the big challenges in multimedia
information retrieval. It exists between the extraction of low-level
features of a video and its conceptual contents. In order to understand the
conceptual content of a video a common approach is building concept
detectors. A problem of this approach is that the number of detectors is
impossible to determine. This paper presents a set of 8 methods on how to
combine two existing concepts into a new one, which occurs when both
concepts appear at the same time. The scores for each shot of a video for
the combined concept are computed from the output of the underlying
detectors. The findings are evaluated on basis of the output of the 101
detectors including a comparison to the theoretical possibility to train a
classifier on each combined concept. The precision gains are significant,
specially for methods which also consider the chronological surrounding of a
shot promising.},
   author = {R B N Aly and D Hiemstra and R J F Ordelman},
   city = {Amsterdam},
   isbn = {not assigned},
   journal = {Proceedings of the Multimedia Information Retrieval Workshop, Amsterdam, The Netherlands},
   keywords = {concept combination,semantic concept},
   month = {8},
   pages = {40-45},
   publisher = {Yahoo! Research},
   title = {Building Detectors to Support Searches on Combined Semantic Concepts},
   year = {2007},
}
@inproceedings{hain05_develamisyste,
   author = {T Hain and J Dines and G Gaurau and M Karafiat and D Moore and V Wan and R J F Ordelman and S Renals},
   isbn = {not assigned},
   journal = {Proc. 2nd Joint Workshop on Multimodal Interaction and Related Machine Learning Algorithms},
   title = {The Development of the AMI System for the Transcription of Speech in Meetings},
   year = {2005},
}
@inproceedings{Ordelman2011,
   author = {R Ordelman},
   city = {Pisa, Italy},
   journal = {MediaEval 2011 Workshop},
   month = {9},
   title = {Rich Speech Retrieval at MediaEval 2011: Challenges, Dataset and Evaluation},
   year = {2011},
}
@thesis{eemcs6793,
   author = {R J F Ordelman},
   institution = {Utrecht University},
   month = {5},
   publisher = {Utrecht University},
   title = {Klinkers en Medeklinkers in woordreconstructie},
   year = {1998},
}
@inproceedings{eemcs11464,
   abstract = {Access to historical audio collections is typically very restricted:
content is often only available on physical (analog) media and the
metadata is usually limited to keywords, giving access at the level
of relatively large fragments, e.g., an entire tape. Many spoken
word heritage collections are now being digitized, which allows the
introduction of more advanced search technology. This paper presents
an approach that supports online access and search for recordings of
historical speeches. A demonstrator has been built, based on the
so-called Radio Oranje collection, which contains radio speeches by
the Dutch Queen Wilhelmina that were broadcast during World War II.
The audio has been aligned with its original 1940s manual
transcriptions to create a time-stamped index that enables the speeches to be
searched at the word level. Results are presented together with
related photos from an external database.},
   author = {L B van der Werff and W F L Heeren and R J F Ordelman and F M G de Jong},
   city = {Utrecht},
   editor = {P Dirx and I Schuurman and V Vandeghinste and F Van Eynde},
   issue = {7},
   journal = {Proceedings of the 17th Meeting of Computational Linguistics in the Netherlands, Leuven, Belgium},
   month = {1},
   pages = {207-218},
   publisher = {Landelijke Onderzoekschool Taalwetenschap},
   title = {Radio Oranje: Enhanced Access to a Historical Spoken Word Collection},
   year = {2007},
}
@inproceedings{so74046,
   abstract = {We carry out two studies on affective state modeling for communication settings that involve unilateral intent on the part of one participant (the evoker) to shift the affective state of another participant (the experiencer). The first investigates viewer response in a narrative setting using a corpus of documentaries annotated with viewer-reported narrative peaks. The second investigates affective triggers in a conversational setting using a corpus of recorded interactions, annotated with continuous affective ratings, between a human interlocutor and an emotionally colored agent. In each case, we build a "one-sided" model using indicators derived from the speech of one participant. Our classification experiments confirm the viability of our models and provide insight into useful features. },
   author = {Bart Jochems and Martha Larson and Roeland Ordelman and Ronald Poppe and Khiet P Truong},
   journal = {Proceedings of Interspeech 2010},
   keywords = {Affect recognition,speech analysis},
   month = {9},
   pages = {490-493},
   publisher = {International Speech Communication Association (ISCA)},
   title = {Towards affective state modeling in narrative and conversational settings},
   url = {http://doc.utwente.nl/74046/},
   year = {2010},
}
@inproceedings{eemcs8381,
   abstract = {We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.},
   author = {D Reidsma and D K J Heylen and R J F Ordelman},
   city = {Paris},
   isbn = {2-9517408-2-4},
   journal = {Proc. of the fifth international conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy},
   month = {5},
   pages = {1117-1122},
   publisher = {ELRA},
   title = {Annotating Emotions in Meetings},
   year = {2006},
}
@inproceedings{eemcs16997,
   abstract = {Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, given the growing amount of materials that are being created on a daily basis and the digitization of existing analogue collections, the traditional manual annotation of collections puts heavy demands on resources, especially for large audiovisual archives. One way to address this challenge, is to introduce (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords form textual resources related to the TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. Besides the descriptions of the programs published by the broadcasters on their Websites, Automatic Speech Transcription (ASR) techniques from the CATCH-CHoral project, also provide textual resources that might be relevant for suggesting keywords. This paper investigates the suitability of ASR for generating such keywords, which we evaluate against manual annotations of the documents and against keywords automatically generated from context documents.},
   author = {V Malaisé and L Gazendam and W F L Heeren and R J F Ordelman and H Brugman},
   city = {Paris},
   isbn = {not assigned},
   journal = {Actes de la 16\{è\}me conf\{é\}rence sur le Traitement Automatique des Langues Naturelles, Senlis, France},
   keywords = {Audiovisual Documents,Automatic Speech Recognition,Keyword extraction},
   publisher = {Association pour le traitement automatique des langues},
   title = {Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs},
   year = {2009},
}
@article{eemcs12910,
   abstract = {Recorded interviews form a rich basis for scholarly inquiry. Examples include oral histories, community memory projects, and interviews conducted for broadcast media. Emerging technologies offer the potential to radically transform the way in which recorded interviews are made accessible, but this vision will demand substantial investments from a broad range of research communities. This article reviews the present state of practice for making recorded interviews available and the state-of-the-art for key component technologies. A large number of important research issues are identified, and from that set of issues, a coherent research agenda is proposed.},
   author = {F M G de Jong and D W Oard and W F L Heeren and R J F Ordelman},
   city = {New York},
   issn = {1556-4673},
   issue = {1},
   journal = {ACM Journal on Computing and Cultural Heritage (JOCCH)},
   month = {6},
   pages = {3:1-3:27},
   publisher = {ACM},
   title = {Access to recorded interviews: A research agenda},
   volume = {1},
   year = {2008},
}
@article{eemcs19888,
   abstract = {News-related content is nowadays among the most popular types of content for users in everyday applications. Although the generation and distribution of news content has become commonplace, due to the availability of inexpensive media capturing devices and the development of media sharing services targeting both professional and user-generated news content, the automatic analysis and annotation that is required for supporting intelligent search and delivery of this content remains an open issue. In this paper, a complete architecture for knowledge-assisted multimodal analysis of news-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately and proposes a novel fusion technique based on the particular characteristics of news-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic annotations.},
   author = {V Mezaris and S Gidaros and W Kasper and J Steffen and R J F Ordelman and M A H Huijbregts and F M G de Jong and I Kompatsiaris and M G Strintzis},
   city = {New York},
   issn = {1110-8657},
   issue = {47},
   journal = {EURASIP Journal on Advances in Signal Processing},
   month = {2},
   pages = {645052},
   publisher = {Hindawi Publishing Corp},
   title = {A system for the semantic multimodal analysis of news audio-visual content},
   volume = {2010},
   year = {2010},
}
@report{eemcs9783,
   abstract = {This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.},
   author = {M A H Huijbregts and R J F Ordelman and F M G de Jong},
   city = {Enschede},
   issn = {1381-3625},
   issue = {TR-CTIT-07-30},
   institution = {Centre for Telematics and Information Technology University of Twente},
   keywords = {Automatic Speech Recognition,Information Retrieval},
   month = {5},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Speech-based Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition},
   year = {2007},
}
@article{Mezaris:2010:SSM:1840667.1928462,
   author = {Vasileios Mezaris and Spyros Gidaros and Walter Kasper and Jörg Steffen and Roeland Ordelman and Marijn Huijbregts and Franciska de Jong and Ioannis Kompatsiaris and Michael G Strintzis},
   city = {New York, NY, United States},
   doi = {http://dx.doi.org/10.1155/2010/645052},
   issn = {1110-8657},
   journal = {EURASIP J. Adv. Signal Process},
   month = {2},
   pages = {47:1--47:16},
   publisher = {Hindawi Publishing Corp.},
   title = {A system for the semantic multimodal analysis of news audio-visual content},
   volume = {2010},
   url = {http://dx.doi.org/10.1155/2010/645052},
   year = {2010},
}
@inproceedings{eemcs8337,
   abstract = {This paper overviews the various ways in which automatic speech and audio analysis can be deployed to enhance the semantic annotation of multimedia content, and as a consequence to improve the effectiveness of conceptual access tools.
A number of techniques will be presented, including the alignment of text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value will be illustrated by the description of two complementary demonstrators for browsing broadcast news archieves.},
   author = {F M G de Jong and R J F Ordelman and A J van Hessen},
   city = {Bangalore, India},
   isbn = {0-86341-671-3},
   journal = {International IET Conference on Visual Information Engineering (VIE 2006), Bangalore, India},
   month = {9},
   pages = {6},
   publisher = {The Institute of Engineering and Technology, London},
   title = {The role of automated speech and audio analysis in semantic multimedia annotation},
   year = {2006},
}
@inproceedings{Ordelman124:2001,
   abstract = {In this paper, ongoing work on the development of the speech recognition modules of a multimedia retrieval environment for Dutch is described. The work on the generation of acoustic models and language models along with their current performance is presented. Some characteristics of the Dutch language and of the target video archives that require special treatment are discussed. },
   author = {R J F Ordelman and A J van Hessen and F M G de Jong},
   city = {Brno},
   issn = {0302-9743},
   journal = {Proceedings of 4th International Conference on Text Speech and Dialogue, Czech Republic},
   keywords = {audio search,speech recognition,spoken document retrieval},
   pages = {258-266},
   publisher = {Springer-Verlag},
   title = {Speech Recognition Issues for Dutch Spoken Document Retrieval},
   volume = {2166/ 2001},
   year = {2001},
}
@inproceedings{eemcs8376,
   abstract = {We discuss the annotation procedure for mental state and
emotion that is under development for the AMI
(Augmented Multiparty Interaction) corpus. The categories
that were found to be most appropriate relate not only to
emotions but also to (meta-)cognitive states and
interpersonal variables. The history of the development of
the annotation scheme is briefly described. The discussion
centers around the presentation of the procedure.},
   author = {D K J Heylen and D Reidsma and R J F Ordelman},
   city = {Paris},
   editor = {L Devillers and J C Martin and R Cowie and A Batliner},
   isbn = {not assigned},
   journal = {Proc. of the LREC2006 Workshop on Corpora for Research on Emotion and Affect, Genoa, Italy},
   month = {5},
   pages = {84-87},
   publisher = {ELRA},
   title = {Annotating State of Mind in Meeting Data},
   year = {2006},
}
@inproceedings{Snoek:2010:CRN:1873951.1874278,
   author = {Cees G M Snoek and Bauke Freiburg and Johan Oomen and Roeland Ordelman},
   city = {New York, NY, USA},
   doi = {http://doi.acm.org/10.1145/1873951.1874278},
   isbn = {978-1-60558-933-6},
   journal = {Proceedings of the international conference on Multimedia},
   keywords = {information visualization,semantic indexing,video retrieval},
   pages = {1535-1538},
   publisher = {ACM},
   title = {Crowdsourcing rock n' roll multimedia retrieval},
   url = {http://doi.acm.org/10.1145/1873951.1874278},
   year = {2010},
}
@article{eemcs14801,
   abstract = {The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval. At the workshop, talks and posters were presented covering a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. Demonstrations of speech-based retrieval systems from a variety of application domains introduced
a strong practical emphasis into the workshop program. The workshop concluded with a panel discussion, whose goal it was to identify future research directions for speech retrieval.
Among the important challenges identified during the panel discussions were: dealing with large scale multimedia collections, representing audio/video content effectively in the user interface, focusing on perfecting the component technologies on which speech retrieval systems are based, and developing systems and approaches that will enable users (both content seekers and content providers) to actively create their own speech search applications or
contribute to the indexability of their content.},
   author = {J Kohler and M A Larson and F M G de Jong and W Kraaij and R J F Ordelman},
   city = {New York},
   issn = {0163-5840},
   issue = {2},
   journal = {ACM SIGIR Forum},
   keywords = {spoken document retrieval},
   month = {12},
   pages = {67-77},
   publisher = {ACM},
   title = {Spoken Content Retrieval: Searching Spontaneous Conversational Speech},
   volume = {42},
   year = {2008},
}
@inproceedings{Ordelman188:2003,
   abstract = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of out-of- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
   author = {R J F Ordelman and A J van Hessen and F M G de Jong},
   city = {Geneva},
   issn = {1018-4074},
   journal = {Eurospeech 2003, Geneva, Switzerland},
   keywords = {audio search,spoken document retrieval},
   publisher = {ISCA},
   title = {Compound Decomposition in Dutch Large Vocabulary Speech Recognition},
   year = {2003},
}
@article{10.1109/MMUL.2011.60,
   author = {Johan Oomen and Roeland Ordelman},
   city = {Los Alamitos, CA, USA},
   doi = {http://doi.ieeecomputersociety.org/10.1109/MMUL.2011.60},
   issn = {1070-986X},
   journal = {IEEE Multimedia},
   pages = {4-10},
   publisher = {IEEE Computer Society},
   title = {Accessing Audiovisual Heritage: A Roadmap for Collaborative Innovation},
   volume = {18},
   year = {2011},
}
@book_section{eemcs11008,
   abstract = {This chapter will focus on the automatic extraction of information from the speech in multimedia documents. This approach is often referred to as speech indexing and it can be regarded as a subfield of audio indexing that also incorporates for example the analysis of music and sounds. If the objective of the recognition of the words spoken is to support retrieval, one commonly speaks of spoken document retrieval (SDR). If the objective is on the coupling of various media types the term media mining or even cross-media mining is used. Most attention in this chapter will go to SDR. The focus is less on searching (an index of ) a multimedia database, but on enabling multiple views on the data by cross-linking all the available multifaceted information sources in a multimedia database. In section 1.6 cross-media mining will be discussed in more detail. },
   author = {R J F Ordelman and F M G de Jong and D A van Leeuwen},
   city = {Heidelberg},
   editor = {H M Blanken and A P de Vries and H E Blok and L Feng},
   journal = {Multimedia Retrieval},
   keywords = {audio search,speech indexing,speech recognition,spoken document retrieval},
   pages = {199-224},
   publisher = {Springer Verlag},
   title = {Speech Indexing},
   year = {2007},
}
@inproceedings{eemcs10989,
   abstract = {This report presents the University of Twente's first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English cross-language spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.},
   author = {R B N Aly and D Hiemstra and R J F Ordelman and L B van der Werff and F M G de Jong},
   city = {Berlin},
   issn = {0302-9743},
   journal = {Evaluation of Multilingual and Multi-modal Information Retrieval},
   month = {9},
   pages = {770-777},
   publisher = {Springer Verlag},
   title = {XML Information Retrieval from Spoken Word Archives},
   volume = {4730},
   year = {2007},
}
@inproceedings{eemcs11664,
   abstract = {This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data. },
   author = {M A H Huijbregts and R J F Ordelman and F M G de Jong},
   city = {Berlin},
   isbn = {3-540-77033-X},
   issn = {0302-9743},
   journal = {Proceedings of the Second International Conference on Semantic and Digital Media Technologies, SAMT 2007, Genoa, Italy},
   month = {12},
   pages = {78-90},
   publisher = {Springer Verlag},
   title = {Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition},
   volume = {4816},
   year = {2007},
}
@article{eemcs15930,
   abstract = {Given the enormous backlog at audiovisual archives and the generally global level of item description, collection disclosure and item access are both at risk. At the same time, archival practice is seeking to evolve from the analogue to the digital world. CHoral investigates the role automatic annotation and search technology can play in improving disclosure and access of digitized spoken word collections during and after this transfer. The core business of the CHoral project is to design and build technology for spoken document retrieval for heritage collections. In this paper, we will argue that in addition to solving technological issues, closer attention is needed for the work-flow and daily practice at audiovisual archives on the one hand, and the state-of-the-art in technology on the other. Analysis of the interplay is needed to ensure that new developments are mutually beneficial and that continuing cooperation can indeed bring envisioned advancements.
},
   author = {W F L Heeren and L B van der Werff and F M G de Jong and R J F Ordelman and T Verschoor and A J van Hessen and M Langelaar},
   city = {London},
   issn = {0308-0188},
   issue = {2-3},
   journal = {Interdisciplinary Science Reviews},
   month = {9},
   pages = {236-252},
   publisher = {Springer Verlag},
   title = {Easy Listening: Spoken Document Retrieval in CHoral},
   volume = {34},
   year = {2009},
}
@inproceedings{eemcs15021,
   abstract = {Decoders that make use of token-passing restrict their search space by various types of token pruning. With use of the Language Model Look-Ahead (LMLA) technique it is possible to increase the number of tokens that can be pruned without loss of decoding precision. Unfortunately, for token passing decoders that use single static pronunciation prefix trees, full n-gram LMLA increases the needed number of language model probability calculations considerably. In this paper a method for applying full n-gram LMLA in a decoder with a single static pronunciation tree is introduced. The experiments show that this method improves the speed of the decoder without an increase of search errors.
},
   author = {M A H Huijbregts and R J F Ordelman and F M G de Jong},
   issn = {1990-9772},
   journal = {Proceedings of Interspeech, Brisbane, Australia},
   keywords = {Automatic Speech Recognition,Decoding,Language Model Look-Ahead,Language modeling},
   month = {9},
   pages = {91},
   publisher = {International Speech Communication Association, Brisbane},
   title = {Fast N-Gram Language Model Look-Ahead for Decoders With Static Pronunciation Prefix Trees},
   year = {2008},
}
@inproceedings{Ord05_robusaudioinde,
   abstract = {Abstract---Whereas the growth of storage capacity is in accordance with widely acknowledged predictions, the possibilities to index and access the archives created is lagging behind. This is especially the case in the oral history domain and much of the rich content in these collections runs the risk to remain inaccessible for lack of robust search technologies. This paper addresses the history and development of robust audio indexing technology for searching Dutch spoken-word collections and compares Dutch audio indexing in the well-studied broadcast news domain with an oral-history case-study. It is concluded that despite significant advances in Dutch audio indexing technology and demonstrated applicability in several domains, further research is indispensable for successful automatic disclosure of spoken-word collections.},
   author = {R J F Ordelman and F M G de Jong and M A H Huijbregts and D A van Leeuwen},
   city = {Amsterdam},
   isbn = {90-6984-456-7},
   journal = {Proceedings of the XVIth International Conference of the Association for History and Computing (AHC2005), Amsterdam, The Netherlands},
   pages = {215-223},
   publisher = {KNAW},
   title = {Robust Audio Indexing for Dutch Spoken-word Collections},
   year = {2005},
}
@inproceedings{eemcs15099,
   abstract = {The computational linguistics community in The Netherlands and Belgium has long recognized the dire need for a major reference corpus of written Dutch. In part to answer this need, the STEVIN programme was established. To pave the way for the effective building of a 500-million-word reference corpus of written Dutch, a pilot project was established. The Dutch Corpus Initiative project or D-Coi was highly successful in that it not only realized about 10% of the projected large reference corpus, but also established the best practices and developed all the protocols and the necessary tools for building the larger corpus within the confines of a necessarily limited budget. We outline the steps involved in an endeavour of this kind, including the major highlights and possible pitfalls. Once converted to a suitable XML format, further linguistic annotation based on the state-of-the-art tools developed either before or during the pilot by the consortium partners proved easily and fruitfully applicable. Linguistic enrichment of the corpus includes PoS tagging, syntactic parsing and semantic annotation, involving both semantic role labeling and spatiotemporal annotation. D-Coi is expected to be followed by SoNaR, during which the 500-million-word reference corpus of Dutch should be built.},
   author = {N Oostdijk and M Reynaert and P Monachesi and G van Noord and R J F Ordelman and I Schuurman and V Vandeghinste},
   isbn = {2-9517408-4-0},
   journal = {Proceedings on the sixth international conference on language resources and evaluation (LREC 2008), Marrakech, Marocco},
   keywords = {Corpus (creation,LR national/international projects,Standards for LRs,annotation,etc.),organizational/policy issues},
   month = {5},
   pages = {1437-1444},
   publisher = {ELRA},
   title = {From D-Coi to SoNaR: A reference corpus for Dutch},
   year = {2008},
}
@book{eemcs11332,
   abstract = {The Proceedings contain the contributions to the workshop on Searching Spontaneous Conversational Speech organized in conjunction with the 30th ACM SIGIR, Amsterdam 2007.
The papers reflect some of the emerging focus areas and cross-cutting research topics, together addressing evaluation metrics, segmentation methods, workflow aspects, rich transcription, and robustness.},
   city = {Enschede},
   editor = {F M G de Jong and D Oard and R J F Ordelman and S Raaijmakers},
   issue = {63},
   month = {7},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Proceedings of the ACM SIGIR Workshop ''Searching Spontaneous Conversational Speech''},
   year = {2007},
}
@inproceedings{marijn05_spokendocum,
   author = {M A H Huijbregts and R J F Ordelman and F M G de Jong},
   isbn = {5-7452-0110-x},
   journal = {Proceedings of 10th international conference Speech and Computer, Patras, Greece (SPECOM 2005)},
   pages = {699-702},
   publisher = {University of Patras, Wire Communications Laboratory Moscow State Linguistics University},
   title = {A Spoken Document Retrieval Application in the Oral History Domain},
   year = {2005},
}
@inproceedings{eemcs10826,
   abstract = {Within the context of international benchmarks and collection specific projects, much work on spoken document retrieval has been done in recent years. In 2000 the issue of automatic speech recognition for spoken document retrieval was declared 'solved' for the broadcast news domain. Many collections, however, are not in this domain and automatic speech recognition for these collections may contain specific new challenges. This requires a method to evaluate automatic speech recognition optimization schemes for these application areas. Traditional measures such as word error rate and story word error rate are not ideal for this. In this paper, three new metrics are proposed. Their behaviour is investigated on a cultural heritage collection and performance is compared to traditional measurements on TREC broadcast news data.},
   author = {L B van der Werff and W F L Heeren},
   city = {Enschede},
   editor = {F M G de Jong and D W Oard and R J F Ordelman and S Raaijmakers},
   journal = {Proceedings of the ACM SIGIR Workshop on Searching Spontaneous Conversational Speech, Amsterdam, The Netherlands},
   keywords = {Automatic Speech Recognition,Evaluation,Lattices,Spoken Document Retrieval},
   month = {7},
   pages = {7-14},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Evaluating ASR Output for Information Retrieval},
   year = {2007},
}
@inproceedings{morang05_infol,
   abstract = {In this paper, a cross-media browsing demonstrator named InfoLink is described. InfoLink automatically links the content of Dutch broadcast news videos to related information sources in parallel collections containing text and/or video. Automatic segmentation, speech recognition and available meta-data are used to index and link items. The concept is visualised using SMIL-scripts for presenting the streaming broadcast news video and the information links.},
   author = {J Morang and R J F Ordelman and F M G de Jong and A J van Hessen},
   city = {Los Alamitos},
   isbn = {0-7803-9331-7},
   journal = {Proceedings of IEEE International Conference on Multimedia and Expo (ICME 2005)},
   pages = {1582-1585},
   publisher = {IEEE Computer Society},
   title = {InfoLink: analysis of Dutch broadcast news and cross-media browsing},
   year = {2005},
}
@inproceedings{eemcs13526,
   abstract = {This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken word archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, the least we want to be able to provide is search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition - supporting e.g., within-document search- are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is still far from satisfactory, and requires additional research. },
   author = {R J F Ordelman and W F L Heeren and M A H Huijbregts and D Hiemstra and F M G de Jong},
   city = {Amsterdam, The Netherlands},
   editor = {M Larson and K Fernie and J Oomen and J Cigarran},
   journal = {Proceedings of the ECDL 2008 Workshop on Information Access to Cultural Heritage (IACH2008), Aarhus, Denmark},
   month = {9},
   publisher = {ILPS, University of Amsterdam},
   title = {Towards Affordable Disclosure of Spoken Word Archives},
   year = {2008},
}
@thesis{Ordelman2003,
   author = {Roeland Ordelman},
   city = {Enschede, the Netherlands},
   institution = {University of Twente},
   month = {10},
   title = {Dutch Speech Recognition in Multimedia Information Retrieval},
   year = {2003},
}
@inproceedings{eemcs8244,
   author = {R J F Ordelman and F M G de Jong and W F L Heeren and A J van Hessen},
   city = {Namur},
   editor = {P Y Schobbens and W Vanhoof and G Schwanen},
   issn = {1568-7805},
   journal = {Proceedings of BNAIC 2006, Namur, Belgium},
   month = {10},
   pages = {413-414},
   publisher = {BNVKI},
   title = {Audio Indexing Technology for the Exploration of Audiovisual Heritage Collections},
   year = {2006},
}
@inproceedings{eemcs20868,
   abstract = {The aim of this paper is to reflect on the factors that impede a clear communication and a more fruitful collaboration between humanities scholars and ICT developers. One of the observations is that ICT-researchers who design tools for humanities researchers, are less inclined to take into account that each stage of the scholarly research process requires ICT-support in a different manner or through different tools. Likewise scholars in the humanities often have prejudices concerning ICT-tools, based on lack of knowledge and fears of technology-driven agendas. If the potential for methodological innovation of the humanities is to be realized, the gap between the mindset of ICT-researchers and that of archivists and scholars in the humanities needs to be bridged. Our assumption is that a better insight into the variety of uses of digital collections and a user-inspired classification of ICT-tools, can help to achieve a greater conceptual clarity among both users and developers. This paper presents such an overview in the form of a typology for the audio-visual realm: examples of what role digital audio-visual archives can play at various research stages, and an inventory of the challenges for the parties involved.},
   author = {F M G de Jong and R J F Ordelman and S Scagliola},
   city = {Copenhagen, Sweden},
   issn = {not assigned},
   journal = {Proceedings of the 2nd Conference on Supporting Digital Humanities (SDH 2011), Copenhagen, Sweden},
   keywords = {audiovisual content,co-development,user requirements},
   month = {11},
   publisher = {Centre for Language Technology, Copenhagen},
   title = {Audio-visual Collections and the User Needs of Scholars in the Humanities: a Case for Co-Development},
   year = {2011},
}
@inproceedings{eemcs8424,
   abstract = {This paper discusses audio indexing tools that have been
implemented for the disclosure of Dutch audiovisual cultural heritage
collections. It explains the role of language models and their
adaptation to historical settings and the adaptation of acoustic models
for homogeneous audio collections. In addition to the benefits of
cross-media linking, the requirements for successful tuning and improvement
of available tools for indexing the heterogeneous A/V collections
from the cultural heritage domain are reviewed. And finally
the paper argues that research is needed to cope with the varying information
needs for different types of users.},
   author = {R J F Ordelman and F M G de Jong and W F L Heeren},
   city = {Trento},
   editor = {L Bordoni and A Krüger and M Zancanaro},
   isbn = {not assigned},
   journal = {Proceedings of the first workshop on intelligent technologies for cultural heritage exploitation, Riva del Garda, Italy},
   month = {8},
   pages = {36-39},
   publisher = {Universit\{à\} di Trento},
   title = {Exploration of audiovisual heritage using audio indexing technology},
   year = {2006},
}
@book{deJong07a,
   editor = {F M G de Jong and D W Oard and R Ordelman and S Raaijmakers},
   isbn = {978-90-365-2542-8},
   title = {Proceedings of ACM SIGIR Workshop on Searching Spontaneous Conversational Speech},
   year = {2007},
}
@inproceedings{eemcs13066,
   abstract = {MediaCampaign's scope is on discovering and inter-relating
advertisements and campaigns, i.e. to relate advertisements
semantically belonging together, across different countries
and different media. The project's main goal is to automate
to a large degree the detection and tracking of advertisement campaigns on television, Internet and in the press. For this purpose we introduce a first prototype of a fully integrated semantic analysis system based on an ontology which automatically detects new creatives and campaigns by utilizing a multimodal analysis system and a framework for the resolution of semantic identity.},
   author = {H Rehatschek and R Sorschag and B Rettenbacher and H Zeiner and J Nioche and F M G de Jong and R J F Ordelman and D A van Leeuwen},
   journal = {Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008., London, UK},
   month = {6},
   pages = {85-92},
   publisher = {IEEE Computer Society},
   title = {Mediacampaign: A Multimodal Semantic Analysis System for Advertisement Campaign Detection},
   year = {2008},
}
@report{eemcs5906,
   author = {R J F Ordelman and A J van Hessen and H de Jong},
   city = {Enschede},
   issn = {1381-3625},
   issue = {TR-CTIT-01-29},
   institution = {Centre for Telematics and Information Technology University of Twente},
   month = {6},
   publisher = {Centre for Telematics and Information Technology University of Twente},
   title = {Lexicon Optimization for Dutch Speech Recognition in Spoken Document Retrieval},
   year = {2001},
}
@article{Larson:2010:MST:1842890.1842901,
   author = {Martha Larson and Roeland Ordelman and Franciska de Jong and Joachim Kohler and Wessel Kraaij},
   city = {New York, NY, USA},
   doi = {http://doi.acm.org/10.1145/1842890.1842901},
   issn = {0163-5840},
   issue = {1},
   journal = {SIGIR Forum},
   month = {8},
   pages = {76-81},
   publisher = {ACM},
   title = {Multimedia with a speech track: searching spontaneous conversational speech},
   volume = {44},
   url = {http://doi.acm.org/10.1145/1842890.1842901},
   year = {2010},
}
@inproceedings{Aly2011b,
   abstract = {This paper describes the participation of the University of Twente team at the Rich Text Retrieval Task of the Media Eval Benchmark Initiative 2011. The goal of the task is to find entry points of relevant parts of videos to reduce the browsing effort of searchers. This is our first participation, therefore our main focus is to create a baseline system which can be improved in the future. We experiment with different evidence sources (ASR and meta data) together with a basic score combination function. We also experiment with different entry points relative to the segments found by the contained evidence. },
   author = {R B N Aly and T Verschoor and R J F Ordelman},
   city = {Aachen, Germany},
   issn = {1613-0073},
   journal = {Working Notes Proceedings of the MediaEval 2011 Workshop, Pisa, Italy},
   month = {9},
   pages = {1},
   publisher = {Sun SITE Central Europe},
   title = {UTwente does Rich Speech Retrieval at MediaEval 2011},
   volume = {807},
   year = {2011},
}
@inproceedings{eemcs20973,
   abstract = {The AXES project participated in the interactive known-item search task (KIS) and the interactive instance search task (INS) for TRECVid 2011. We used the same system architecture and a nearly identical user interface for both the KIS and INS tasks. Both systems made use of text search on ASR, visual concept detectors, and visual similarity search. The user experiments were carried out with media professionals and media students at the Netherlands Institute for Sound and Vision, with media professionals performing the KIS task and media students participating in the INS task. This paper describes the results and findings of our experiments.},
   author = {K McGuinness and R B N Aly and S Chen and M Frappier and M Kleppe and H Lee and R J F Ordelman and R Arandjelovic and M Juneja and C V Jawahar and A Vedaldi and J Schwenninger and S Tschöpel and D Schneider and N E O'Conner and A Zisserman and A Smeaton and H Beunders},
   city = {Geithesburg, U.S.},
   isbn = {not assigned},
   journal = {TREC 2011 Video Retrieval Evaluation Online Proceedings (TRECVid 2011), Geithesburg, U.S.},
   month = {12},
   publisher = {NIST},
   title = {AXES at TRECVid 2011},
   year = {2011},
}
@thesis{ordelman:thesis2003,
   abstract = {As data storage capacities grow to nearly unlimited sizes thanks to ever ongoing hardware and software improvements, an increasing amount of information is being stored in multimedia and spoken-word collections. Assuming that the intention of data storage is to use (portions of) it some later time, these collections must also be searchable in one way or another. For multimedia and spoken-word collections, traditional text-oriented information retrieval (IR) strategies inevitably fall short, as the amount of textual information included with these types of documents is usually very limited. However, when automatic speech recognition (ASR) can be used to convert the speech occurring in these documents into text, textual representations can be created that in turn can be searched using the traditional text-based search strategies. As ASR systems label recognized words with exact time information as a standard accessory, detailed searching within multimedia and spoken-word collections can be enabled. This type of retrieval is commonly referred to as Spoken Document Retrieval (SDR). Typically, large vocabulary speaker independent continuous speech recognition systems (LVCSR) are deployed for creating textual representations of the spoken audio in multimedia an spoken-word collections. For Dutch however, such a system was not available when this research was started. As creating a Dutch system from scratch was not feasible given the available resources, an existing English system, refered to as the ABBOT system, was ported to Dutch. A significant part of this thesis is dedicated to a complete run-down of the porting work, involving the collection and preparation of suitable training data and the actual training and evaluation of the acoustic models and language models. The broadcast news domain was chosen as domain of focus, as this domain has also been extensively used as a benchmark domain for both international ASR research and SDR. A complicating factor for ASR in the news domain, is that word usage is highly variable. As a consequence, besides using large vocabularies, it is important to adjust these vocabularies regularly, so that they reflect the content of the news programs well. Therefore, it has been investigated which word selection strategies are best suited for making these vocabulary adjustments. Moreover, as dynamic vocabularies require a flexible generation of accurate word pronunciations, the development of a grapheme-to-phoneme converter is addressed. Another vocabulary related issue that is investigated, stems from a well-known characteristic of the Dutch language, word compounding: Dutch words can almost freely be joined together to form new words. As a result of this phenomenon, the number of distinct words in Dutch is relatively large, which reduces the coverage of vocabularies compared to those of the same size of other languages, such as English, that do not have word compounding. This thesis investigates whether splitting Dutch compound words could be a remedy for the relatively limited coverage of vocabularies, so that ASR performance could be improved. Next to a brief history of SDR research and a review of possible SDR approaches, this thesis demonstrates the use of a Dutch LVCSR in SDR by providing an illustrative example of an SDR evaluation given a collection of Dutch broadcast news shows. It is shown that Dutch speech recognition can successfully be deployed for content-based retrieval of broadcast news programs. The experience obtained with the research described in this thesis, and the experience that will emerge from future research efforts must contribute to the long-term accessibility of the increasing amount of information being stored in Dutch multimedia and spoken-word collections.},
   author = {R J F Ordelman},
   city = {Enschede},
   isbn = {90-75296-08-8},
   institution = {University of Twente, Enschede},
   keywords = {Multimedia Retrieval,Speech Recognition},
   month = {10},
   publisher = {Twente University Press},
   title = {Dutch Speech Recognition in Multimedia Information Retrieval},
   year = {2003},
}
@article{eemcs17138,
   abstract = {This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken heritage archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, we at least want to provide search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition - supporting e.g., within-document search - are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is not yet satisfactory, and requires additional research.},
   author = {R J F Ordelman and W F L Heeren and F M G de Jong and M A H Huijbregts and D Hiemstra},
   editor = {M Larson and K Fernie and J Oomen},
   issn = {1368-7506},
   issue = {6},
   journal = {Journal of Digital Information},
   keywords = {cultural heritage archives,multimedia retrieval,speech indexing,speech recognition,usability,work-flow},
   month = {12},
   pages = {17},
   publisher = {Texas Digital Library},
   title = {Towards Affordable Disclosure of Spoken Heritage Archives},
   volume = {10},
   year = {2009},
}
@inproceedings{eemcs18431,
   abstract = {Archival practice is shifting from the analogue to the digital world. A specific subset of heritage collections that impose interesting challenges for the field of language and speech technology are spoken word archives. Given the enormous backlog at audiovisual archives of unannotated materials and the generally global level of item description, collection disclosure and item access are both at risk, and (semi-)automated methods for analysis and annotation may help to increase the use and reuse of these rich content collections. In several HMI projects the interplay has been investigated between evolving user scenarios and user requirements for spoken audio collections on the one hand, and the potential of automatic annotation and search technology for the improved accessibility and search paradigms on the other hand. In this paper we will present an overview of the state-of-the-art in metadata generation for audio content and explain the crucial importance of involving user groups in the design of research agendas and road maps for novel applications in this domain.},
   author = {F M G de Jong and W F L Heeren and A J van Hessen and R J F Ordelman and A Nijholt},
   city = {Santiago de Cuba, Cuba},
   editor = {L Ruiz Miyares and M R Alvarez Silva},
   journal = {Proceedings 12th International Symposium on Social Communication, Santiago de Cuba},
   month = {1},
   pages = {896-905},
   publisher = {Centre for Applied Linguistics},
   title = {Automated Metadata Extraction for Semantic Access to Spoken Word Archives},
   year = {2011},
}
@inproceedings{eemcs18625,
   abstract = {We carry out two studies on affective state modeling for communication settings that involve unilateral intent on the part of one participant (the evoker) to shift the affective state of another participant (the experiencer). The first investigates viewer response in a narrative setting using a corpus of documentaries annotated with viewer-reported narrative peaks. The second investigates affective triggers in a conversational setting using a corpus of recorded interactions, annotated with continuous affective ratings, between a human interlocutor and an emotionally colored agent. In each case, we build a "one-sided" model using indicators derived from the speech of one participant. Our classification experiments confirm the viability of our models and provide insight into useful features. },
   author = {B Jochems and M A Larson and R J F Ordelman and R W Poppe and K P Truong},
   issn = {1990-9772},
   journal = {Proceedings of Interspeech 2010, Makuhari, Chiba, Japan},
   keywords = {affect recognition,speech analysis},
   month = {9},
   pages = {490-493},
   publisher = {International Speech Communication Association (ISCA)},
   title = {Towards affective state modeling in narrative and conversational settings},
   year = {2010},
}
@article{meijs05_generexpress,
   abstract = {Work on expressive speech synthesis has long focused on the expression of basic emotions. In recent years, however, interest in other expressive styles has been increasing. The research presented in this paper aims at the generation of a storytelling speaking style, which is suitable for storytelling applications and more in general, for applications aimed at children. Based on an analysis of human storytellers' speech, we designed and implemented a set of prosodic rules for converting "neutral" speech, as produced by a text-to-speech system, into storytelling speech. An evaluation of our storytelling speech generation system showed encouraging results.},
   author = {M Theune and K Meijs and D K J Heylen and R J F Ordelman},
   city = {New York},
   editor = {G Bailly and N Campbell and W Hamza and H Hoge and T Jianhua},
   issn = {1558-7916},
   issue = {4},
   journal = {IEEE transactions on audio, speech and language processing},
   month = {7},
   pages = {1137-1144},
   publisher = {IEEE Signal Processing Society},
   title = {Generating Expressive Speech for Storytelling Applications},
   volume = {14},
   year = {2006},
}
@inproceedings{eemcs15100,
   abstract = {StreetTiVo is a project that aims at bringing research results into the living room; in particular, a mix of current results in the areas of Peer-to-Peer XML Database Management System (P2P XDBMS), advanced multimedia analysis techniques, and advanced information re- trieval techniques. The project develops a plug-in application for the so-called Home Theatre PCs, such as set-top boxes with MythTV or Windows Media Center Edition installed, that can be considered as programmable digital video recorders. StreetTiVo distributes compute- intensive multimedia analysis tasks over multiple peers (i.e., StreetTiVo users) that have recorded the same TV program, such that a user can search in the content of a recorded TV program shortly after its broad- casting; i.e., it enables near real-time availability of the meta-data (e.g., speech recognition) required for searching the recorded content. Street- TiVo relies on our P2P XDBMS technology, which in turn is based on a DHT overlay network, for distributed collaborator discovery, work coor- dination and meta-data exchange in a volatile WAN environment. The technologies of video analysis and information retrieval are seamlessly integrated into the system as XQuery functions.},
   author = {Y Zhang and A P de Vries and P A Boncz and D Hiemstra and R J F Ordelman},
   city = {Berlin},
   editor = {Li, Q. et al.},
   journal = {APWeb/WAIM 2009, Suzhou, China},
   keywords = {XML database,multimedia analysis,peer-to-peer,speech recognition},
   month = {4},
   pages = {404-415},
   publisher = {Springer Verlag},
   title = {StreetTiVo: Using a P2P XML Database System to Manage Multimedia Data in Your Living Room},
   volume = {5446},
   year = {2009},
}
@inproceedings{sad:huijbregts_07,
   author = {Marijn Huijbregts and Chuck Wooters and Roeland Ordelman},
   city = {Antwerp, Belgium},
   journal = {proceedings of Interspeech},
   month = {8},
   title = {Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections},
   year = {2007},
}
@inproceedings{eemcs12164,
   abstract = {In this report we summarize our methods and results for the search tasks in
TRECVID 2007. We employ two different kinds of search: purely ASR based and
purely concept based search. However, there is not significant difference of the
performance of the two systems. Using neighboring shots for the combination of
two concepts seems to be beneficial. General preprocessing of queries increased
the performance and choosing detector sources helped. However, for all automatic
search components we need to perform further investigations.},
   author = {R B N Aly and C Hauff and W F L Heeren and D Hiemstra and F M G de Jong and R J F Ordelman and T Verschoor and A P de Vries},
   city = {Geithesburg, U.S.},
   isbn = {not assigned},
   journal = {TREC Video Retrieval Evaluation Online Proceedings, Geithesburg, U.S.},
   month = {2},
   publisher = {NIST},
   title = {The Lowlands team at TRECVID 2007},
   year = {2008},
}
@inproceedings{hain05_transconferro,
   author = {T Hain and J Dines and G Gaurau and M Karafiat and D Moore and V Wan and R J F Ordelman and S Renals},
   isbn = {not assigned},
   journal = {Proceedings of Interspeech 2005},
   title = {Transcription of Conference Room Meetings: an Investigation},
   year = {2005},
}
@generic{eemcs17863,
   abstract = {The disclosure herein relates generally to methods and devices for use in treating a patient by utilizing a response index associated with a selected organ. The selected organ may be assessed by comparing the response index to psychological parameters monitored in response to one or more perturbations. For example, the status of the selected organ may be assessed by comparing the relationship between one or more perturbations (e.g., electrical stimulation of the efferent vagus nerve) and one or more effects associated with the one or more perturbations (e.g., electrical activity effect of the vagus nerve, effect on the sympathetic efferent pathways, etc.). Information derived by the comparisons using the response index may be used to initiate or adjust therapy delivered by medical devices such as an implantable medical device (IMD) (e.g., a pacemaker (PM), an implantable cardioverter defibrillator (ICD), neuromodulation system, etc.).},
   author = {L Kornet and P H Veltink and J E Burnes and M R S Hill and R N M Cornelussen and S C M A Ordelman and H P J Buschman},
   issue = {61/329,374},
   keywords = {pertubations,response index,vagus nerve},
   month = {4},
   title = {Therapy using perturbation and effect of physiological systems},
   year = {2010},
}
@inproceedings{decoding:huijbregts_08,
   author = {Marijn Huijbregts and Roeland Ordelman and Franciska de Jong},
   city = {Brisbane, Australia},
   journal = {proceedings of Interspeech},
   month = {9},
   title = {Fast \{N-Gram\} Language Model Look-Ahead for Decoders With Sta tic Pronunciation Prefix Trees},
   year = {2008},
}
@article{eemcs19891,
   abstract = {After two successful years at SIGIR in 2007 and 2008, the third workshop on Searching Spontaneous Conversational Speech (SSCS 2009) was held conjunction with the ACM Multimedia 2009. The goal of the SSCS series is to serve as a forum that brings together the disciplines that collaborate on spoken content retrieval, including information retrieval, speech recognition and multimedia analysis. Multimedia collections often contain a speech track, but in many cases it is ignored or not fully exploited for information retrieval. Currently, spoken content retrieval research is expanding beyond highly-conventionalized domains such as broadcast news in to domains involving speech that is produced spontaneously and in conversational settings. Such speech is characterized by wide variability of speaking styles, subject matter and recording conditions. The work presented at SSCS 2009 included techniques for searching meetings, interviews, telephone conversations, podcasts and spoken annotations. The work encompassed a large range of approaches including using subword units, exploiting dialogue structure, fusing retrieval models, modeling topics and integrating visual features. Taken in sum, the workshop demonstrated the high potential of new ideas emerging in the area of speech search and also reinforced the need for concentrated research devoted to the classic challenges of spoken content retrieval, many of which remain yet unsolved.},
   author = {M Larson and R J F Ordelman and F M G de Jong and J Kohler and W Kraaij},
   city = {New York},
   issn = {0163-5840},
   issue = {1},
   journal = {ACM SIGIR Forum},
   month = {8},
   pages = {76-81},
   publisher = {ACM},
   title = {Multimedia with a speech track: searching spontaneous conversational speech},
   volume = {44},
   year = {2010},
}
@inproceedings{eemcs19889,
   abstract = {In this technical demonstration, we showcase a multimedia search engine that facilitates semantic access to archival rock n' roll concert video. The key novelty is the crowdsourcing mechanism, which relies on online users to improve, extend, and share, automatically detected results in video fragments using an advanced timeline-based video player. The user-feedback serves as valuable input to further improve automated multimedia retrieval results, such as automatically detected concepts and automatically transcribed interviews. The search engine has been operational online to harvest valuable feedback from rock n' roll enthusiasts.},
   author = {C G M Snoek and B Freiburg and J Oomen and R J F Ordelman},
   city = {New York},
   journal = {Proceedings of the ACM Multimedia Conference, MM 2010, Firenze, Italy},
   pages = {1535-1538},
   publisher = {ACM},
   title = {Crowdsourcing rock n' roll multimedia retrieval},
   year = {2010},
}
@inproceedings{eemcs13063,
   abstract = {In this paper, a complete architecture for knowledge-assisted cross-media analysis of News-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately, and proposes a fusion technique based on the particular characteristics of News-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic video annotations.},
   author = {V Mezaris and S Gidaros and G Papadopoulos and W Kasper and R J F Ordelman and F M G de Jong and I Kompatsiaris},
   journal = {Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008., London, UK},
   month = {6},
   pages = {280-287},
   publisher = {IEEE Computer Society},
   title = {Knowledge-assisted cross-media analysis of audio-visual content in the news domain},
   year = {2008},
}
