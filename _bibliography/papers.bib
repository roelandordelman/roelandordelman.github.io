@inproceedings{d5e5c3c3506144ff9afb7a2bb315d550,  title     = "Let{\textquoteright}s talk about it: Spoken conversational search with a robot for children",  abstract  = "In this article, we propose a robot to assist children in finding information through conversation. The proposed robot uses clarifying questions to assist children in communicating their information need. We describe the setup of our (unpublished) pilot study that investigated how children perceive robots and the information they provide. We also describe a study we are currently developing that addresses how children{\textquoteright}s experience and search outcomes are impacted by a robot asking clarifying questions. We compare a robot using clarifying questions to a robot that replies by directly presenting information, which is how many currently available voice assistants operate. Finally, we describe our future steps. We intend to contribute to the development of children-centered information search and robot technology.",  author    = "Thomas Beelen and Ella Velner and Roeland Ordelman and Truong, {Khiet Phuong} and Vanessa Evers and Theo Huibers",  year      = "2021",  month     = oct,  day       = "23",  language  = "English",  isbn      = "978-1-4503-6819-3",  booktitle = "CUI@CSCW: Inclusive and Collaborative Child-facing Voice Technologies",  publisher = "ACM Publishing",  note      = "CUI@CSCW 2021 : Inclusive and Collaborative Child-facing Voice Technologies ; Conference date: 23-10-2021 Through 23-10-2021",  url       = "https://www.conversationaluserinterfaces.org/workshops/CSCW2021/", }


@conference{b4c787d6b80c41a0aef1e488e9472e90,  title    = "Does your robot know? Enhancing children{\textquoteright}s information retrieval through spoken conversation with responsible robots",  author   = "Beelen, {Thomas Herman Johan} and Ella Velner and Truong, {Khiet Phuong} and Ordelman, {Roeland J.F.} and Vanessa Evers and Huibers, {Theo W.C.}",  year     = "2021",  month    = jul,  day      = "15",  doi      = "10.48550/arXiv.2106.07931",  language = "English",  note     = "IR for Children, IR4C 2021 : Where Are We Now?, IR4C ; Conference date: 15-07-2021 Through 15-07-2021", }


@inproceedings{2ee24174001b4ab09c5ac9e9c57c3457,  title     = "Automatic Annotations and Enrichments for Audiovisual Archives",  abstract  = "The practical availability of Audiovisual Processing tools to media scholars and heritage institutions remains limited, despite all the technical advancements of recent years. In this article we present the approach chosen in the CLARIAH project to increase this availability, we discuss the challenges encountered, and introduce the technical solutions we are implementing. Through three use cases focused on the enrichment of AV archives, Pose Analysis, and Automatic Speech Recognition, we demonstrate the potential and breadth of using Audiovisual Processing for archives and Digital Humanities research.",  author    = "Noord, {Nanne Van} and Christian Olesen and Roeland Ordelman and Julia Noordegraaf",  note      = "Funding Information: CLARIAH is a distributed data & tools infrastructure for the Humanities and Social Sciences, funded by the Netherlands Organization for Scientific Research in two subsequent projects, respectively CLARIAH-CORE (2014-2018) and CLARIAH-PLUS (2019-2024). In CLARIAH-CORE basic infrastructure and tools were developed following user-centered design principles and by carrying out research pilots so as to develop a detailed understanding of researchers{\textquoteright} needs. Building on the experiences and insights from CLARIAH-CORE, CLARIAH-PLUS simultaneously focuses on scaling up and facilitating the implementation of the CLARIAH infrastructure in research and teaching. CLARIAH{\textquoteright}s data and tools are stored at the partner institutions (so called CLARIAH centers: knowledge centers, libraries, archives and museums) that make the large data collections accessible via online virtual research environments (VREs) for users to access on their home computers. Funding Information: The research described in this paper was made possible by the CLARIAH-PLUS project (www.clariah.nl) financed by NWO. Publisher Copyright: {\textcopyright} 2021 by SCITEPRESS - Science and Technology Publications, Lda.; 13th International Conference on Agents and Artificial Intelligence, ICAART 2021, ICAART 2021 ; Conference date: 04-02-2021 Through 06-02-2021",  year      = "2021",  doi       = "10.5220/0010387706330640",  language  = "English",  pages     = "633--640",  editor    = "Rocha, {Ana Paula} and Luc Steels and {van den Herik}, Jaap",  booktitle = "Proceedings of the 13th International Conference on Agents and Artificial Intelligence - Volume 1: ARTIDIGH",  publisher = "SCITEPRESS",  url       = "http://www.icaart.org/Home.aspx", }


@inproceedings{6c7c8b965268405faae1f258dfe10858,  title     = "Media Suite: Unlocking Audiovisual Archives for Mixed Media Scholarly Research",  abstract  = "This paper discusses the rationale behind and approach towards the development of a research environment –the Media Suite– in a sustainable, dynamic, multi-institutional infrastructure that supports mixed media scholarly research with large audiovisual data collections and available multimedia context collections, serving media scholars and digital humanists in general.",  author    = "Ordelman, {Roeland J.F.} and {Melgar Estrada}, Liliana and {van Gorp}, Jasmijn and Julia Noordegraaf",  year      = "2019",  month     = feb,  day       = "3",  language  = "English",  pages     = "21--25",  editor    = "Inguna Skadina and Maria Eskevich",  booktitle = "CLARIN Annual Conference 2018",  note      = "CLARIN 2018 Annual Conference, CLARIN 2018 ; Conference date: 08-10-2018 Through 10-10-2018", }


@inproceedings{f9522016e8e44fa2b5edcdae8ca6aa74,  title     = "Jupyter Notebooks for Generous Archive Interfaces",  abstract  = "To help scholars to extract meaning, knowledge and value from large volumes of archival content, such as the Dutch Common Lab Research Infrastructure for the Arts and Humanities (CLARIAH), we need to provide more {\textquoteleft}generous{\textquoteright} access to the data than can be provided with generalised search and visualisation tools alone. Our approach is to use Jupyter Notebooks in combination with the existing archive APIs (Application Programming Interface). This gives access to both the archive metadata and a wide range of analysis and visualisation techniques. We have created notebooks and modules of supporting functions that enable the overview, investigation and analysis of the archive. We demonstrate the value of our approach in preliminary tests of its use in scholarly research, and give our observations of the potential value for archivists. Finally, we show that good archive knowledge is essential to create correct and meaningful visualisations and statistics.",  keywords  = "Digital Humanities, Audiovisual archives, multimedia access, Big Data, Visualisation, jupyter notebooks",  author    = "Mari Wigham and {Melgar Estrada}, Liliana and Ordelman, {Roeland J.F.}",  year      = "2019",  month     = jan,  day       = "24",  doi       = "10.1109/BigData.2018.8622203",  language  = "English",  booktitle = "2018 IEEE International Conference on Big Data (Big Data)",  note      = "IEEE International Conference on BIG DATA 2018 ; Conference date: 10-12-2018 Through 13-12-2018",  url       = "http://cci.drexel.edu/bigdata/bigdata2018/index.html", }


@inproceedings{bcec7972c3fd4f42aacebe4cb40a0df1,  title     = "The CLARIAH Media Suite: A Hybrid Approach to System Design in the Humanities",  keywords  = "complex tasks, digital humanities, research information systems",  author    = "Liliana Melgar-Estrada and Marijn Koolen and Kaspar Beelen and Hugo Huurdeman and Mari Wigham and Carlos Martinez-Ortiz and Jaap Blom and Roeland Ordelman",  year      = "2019",  doi       = "10.1145/3295750.3298918",  language  = "English",  isbn      = "978-1-4503-6025-8",  pages     = "373--377",  booktitle = "Proceedings of the 2019 Conference on Human Information Interaction and Retrieval",  publisher = "ACM Siggraph",  note      = "ACM SIGIR Conference on Human Information Interaction and Retrieval, CHIIR 2019, CHIIR ; Conference date: 10-03-2019 Through 14-03-2019",  url       = "https://sigir.org/chiir2019/", }


@inproceedings{75f7d6726f0b48bdbaef8fd91cf2d316,  title     = "Media Suite: Unlocking Archives for Mixed Media Scholarly Research",  abstract  = "This paper discusses the rationale behind the development of a research environment –the Media Suite– in a sustainable, dynamic, multi-institutional infrastructure that supports mixed media scholarly research with large multimedia data collections, serving media scholars and digital humanists in general.",  keywords  = "Digital Humanities, Infrastructure, multimedia access, multimedia archives",  author    = "Ordelman, {Roeland J.F.} and {Melgar Estrada}, Liliana and {Mart{\'i}nez Ort{\'i}z}, Carlos and Julia Noordegraaf",  year      = "2018",  month     = oct,  day       = "10",  language  = "English",  pages     = "21--25",  editor    = "Inguna Skadina and Maria Eskevich",  booktitle = "CLARIN 2018 Annual Conference",  note      = "CLARIN 2018 Annual Conference, CLARIN 2018 ; Conference date: 08-10-2018 Through 10-10-2018", }


@inproceedings{e08835da324c42958ac7d7f7cff13094,  title     = "Speech Recognition and Scholarly Research: Usability and Sustainability",  abstract  = "For years we have been working on speech recognition (ASR) as a tool for scholarly research. The current state-of-the-art can be useful for many scholarly use cases focusing on audiovisual content, but practically applying ASR is often not so straightforward. In the CLARIAH Media Suite, a secured online portal for scholarly research for audiovisual media, we solved the most important hurdles for the practical deployment of ASR by focusing on usability and sustainability aspects.",  keywords  = "Digital Humanities, Speech Recognition, Infrastructure",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.}",  year      = "2018",  month     = oct,  day       = "10",  language  = "English",  pages     = "163--168",  editor    = "Inguna Skadina and Maria Eskevich",  booktitle = "CLARIN 2018 Annual Conference",  note      = "CLARIN 2018 Annual Conference, CLARIN 2018 ; Conference date: 08-10-2018 Through 10-10-2018", }


@inproceedings{4b8387a7d9af43a9b0f6eecb2d0d0159,  title     = "Challenges in Enabling Mixed Media Scholarly Research with Multi-media Data in a Sustainable Infrastructure",  abstract  = "Big-scale infrastructure projects in the humanities and social sciences such as the Digital Research Infrastructure for the Arts and Humanities (DARIAH) (Edmond et al., 2017), or the Common Language Resources and Technology Infrastructure (CLARIN) (Hinrichs and Krauwer, 2014) aim to provide solutions for both preservation and access to collections and data necessary for scholarly research (Zundert, 2012). Some infrastructure projects build decentralized “atomic” software services, e.g., as in the LLS infrastructure project (Buchler et al., 2016), while others prefer to build more centralized virtual research environments, as in the European Holocaust Research Infrastructure (EHRI) (Lauer, 2014). Also, even within a single infrastructure project, these two models can coexist. This is the case of the CLARIAH infrastructure, where different approaches have been taken to date for serving different user groups, i.e., several specialized tools for linguists (Odijk, Broeder & Barbiers, 2015), or a research environment (the Media Suite) that serves the scholarly needs for working with audiovisual data collections and related mixed-media contextual sources that are maintained at cultural heritage and knowledge institutions. This paper discusses the rationale and challenges behind the development of the Media Suite.",  keywords  = "Digital Humanities, Infrastructure",  author    = "Roeland Ordelman and {Mart{\'i}nez Ort{\'i}z}, Carlos and {Melgar Estrada}, Liliana and Marijn Koolen and Jaap Blom and Willem Melder and {van Gorp}, Jasmijn and {De Boer}, Victor and Themistoklis Karavellas and Lora Aroyo and Thomas Poell and Norah Karrouche and Eva Baaren and Johannes Wassenaar and Oana Inel and Julia Noordegraaf",  year      = "2018",  language  = "English",  booktitle = "Digital Humanities 2018 Conference",  note      = "Digital Humanities 2018, DH 2018 ; Conference date: 26-06-2018 Through 29-06-2018",  url       = "https://dh2018.adho.org/en/", }


@inproceedings{b1881c0008ee411886d88e85542e3d11,  title     = "Multimodal video-to-video linking: Turning to the crowd for insight and evaluation",  abstract  = "Video-to-video linking systems allow users to explore and exploit the content of a large-scale multimedia collection interactively and without the need to formulate specific queries. We present a short introduction to video-to-video linking (also called {\textquoteleft}video hyperlinking{\textquoteright}), and describe the latest edition of the Video Hyperlinking (LNK) task at TRECVid 2016. The emphasis of the LNK task in 2016 is on multimodality as used by videomakers to communicate their intended message. Crowdsourcing makes three critical contributions to the LNK task. First, it allows us to verify the multimodal nature of the anchors (queries) used in the task. Second, it enables us to evaluate the performance of video-to-video linking systems at large scale. Third, it gives us insights into how people understand the relevance relationship between two linked video segments. These insights are valuable since the relationship between video segments can manifest itself at different levels of abstraction.",  keywords  = "IR-104402, EWI-27664",  author    = "Laurent Amsaleg and Maria Eskevich and Gu{\dh}mundsson, {Gylfi {\TH}{\'o}r} and Martha Larson and Robin Aly and Cathal Gurrin and J{\'o}nsson, {Bj{\"o}rn {\TH}{\'o}r} and Serwah Sabetghadam and Jones, {Gareth J.F.} and Shin{\textquoteright}ichi Satoh and Ordelman, {Roeland J.F.} and Benoit Huet",  year      = "2017",  month     = jan,  doi       = "10.1007/978-3-319-51814-5_24",  language  = "English",  isbn      = "978-3-319-51813-8",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "280--292",  booktitle = "Multimedia Modeling",  address   = "Netherlands",  note      = "23rd International Conference on Multimedia Modeling, MMM 2017 ; Conference date: 04-01-2017 Through 06-01-2017", }


@conference{2ab06fd4a4b641f6a8cd795b52bff0c6,  title    = "TREC video retrieval evaluation TRECVID 2017",  author   = "George Awad and Ian Soboroff and Asad Butt and Angela Ellis and Darrin Dimmick and Jonathan Fiscus and David Joy and Martial Michel and Andrew Delgado and Alan Smeaton and Wessel Kraaij and Georges Qu{\'e}not and Roeland Ordelman and Maria Eskevich and Gareth Jones and Benoit Huet and Stephanie Strassel and Xuansong Li",  year     = "2017",  language = "English",  note     = "17th annual TREC Video Retrieval Evaluation, TRECVID 2017, TRECVID 2017 ; Conference date: 13-11-2017 Through 15-11-2017",  url      = "http://trecvid.nist.gov ", }


@inproceedings{35f08650f8a04f9a9a8eb5e49ac6712b,  title     = "Trecvid 2017: Evaluating ad-hoc and instance video search, events detection, video captioning and hyperlinking",  author    = "George Awad and Butt, {Asad A.} and Jonathan Fiscus and David Joy and Andrew Delgado and Martial Michel and Smeaton, {Alan F} and Yvette Graham and Wessel Kraaij and Georges Qu{\'e}not and Ordelman, {Roeland J.F.} and Jones, {Gareth J.F.} and Benoit Huet",  year      = "2017",  language  = "English",  volume    = "2017",  booktitle = "Proceedings of TRECVID",  note      = "17th annual TREC Video Retrieval Evaluation, TRECVID 2017, TRECVID 2017 ; Conference date: 13-11-2017 Through 15-11-2017",  url       = "http://trecvid.nist.gov ", }


@conference{357afd8a09824da4b89a25ba3ce96b81,  title    = "Video hyperlinking (LNK) TRECVID 2017",  author   = "Maria Eskevich and Roeland Ordelman and Jones, {Gareth J.F.} and Benoit Huet",  year     = "2017",  language = "English",  note     = "17th annual TREC Video Retrieval Evaluation, TRECVID 2017, TRECVID 2017 ; Conference date: 13-11-2017 Through 15-11-2017",  url      = "http://trecvid.nist.gov ", }


@article{1c9642873ac0420dae48fe2fb0399c8f,  title     = "Evaluating Unsupervised Thesaurus-based Labeling of Audiovisual Content in an Archive Production Environment",  abstract  = "In this paper we report on a two-stage evaluation of unsupervised labeling of audiovisual content using collateral text data sources to investigate how such an approach can provide acceptable results for given requirements with respect to archival quality, authority and service levels to external users. We conclude that with parameter settings that are optimized using a rigorous evaluation of precision and accuracy, the quality of automatic term-suggestion is sufficiently high. We furthermore provide an analysis of the term extraction after being taken into production, where we focus on performance variation with respect to term types and television programs. Having implemented the procedure in our production work-flow allows us to gradually develop the system further and to also assess the effect of the transformation from manual to automatic annotation from an end-user perspective. Additional future work will be on deploying different information sources including annotations based on multimodal video analysis such as speaker recognition and computer vision.",  keywords  = "EWI-27061, HMI-MR: MULTIMEDIA RETRIEVAL, Thesaurus, Practice-oriented evaluation, METIS-318455, Audiovisual archives, Information Extraction, IR-100719, Audiovisual access",  author    = "{de Boer}, Victor and Ordelman, {Roeland J.F.} and Josefien Schuurman",  note      = "Open access ",  year      = "2016",  month     = sep,  doi       = "10.1007/s00799-016-0182-6",  language  = "Undefined",  volume    = "17",  pages     = "189--201",  journal   = "International journal on digital libraries",  issn      = "1432-5012",  publisher = "Springer",  number    = "3", }


@article{cca815c3f6b74363b6e891c1824504b4,  title     = "Pursuing a moving target: Iterative use of benchmarking of a task to understand the task",  abstract  = "Individual tasks carried out within benchmarking initiatives, or campaigns, enable direct comparison of alternative approaches to tackling shared research challenges and ideally promote new research ideas and foster communities of researchers interested in common or related scientific topics. When a task has a clear predefined use case, it might straightforwardly adopt a well established framework and methodology. For example, an ad hoc information retrieval task adopting the standard Cranfield paradigm. On the other hand, in cases of new and emerging tasks which pose more complex challenges in terms of use scenarios or dataset design, the development of a new task is far from a straightforward process. This letter summarises our reections on our experiences as task organisers of the Search and Hyperlinking task from its origins as a Brave New Task at the MediaEval benchmarking campaign (2011-2014) to its current instantiation as a task at the NIST TRECVid benchmark (since 2015). We highlight the challenges encountered in the development of the task over a number of annual iterations, the solutions found so far, and our process for maintaining a vision for the ongoing advancement of the task's ambition.",  author    = "Maria Eskevich and Jones, {Gareth J.F.} and Robin Aly and Roeland Ordelman and Benoit Huet",  year      = "2016",  month     = jan,  day       = "1",  language  = "English",  volume    = "1739",  journal   = "CEUR workshop proceedings",  issn      = "1613-0073",  publisher = "Rheinisch Westf{\"a}lische Technische Hochschule",  note      = "2016 Multimedia Benchmark Workshop, MediaEval 2016, MediaEval 2016 ; Conference date: 20-10-2016 Through 21-10-2016", }


@inproceedings{af8cadd8ce5943a0ae698a529544753c,  title     = "Developing Benchmarks: The Importance of the Process and New Paradigms",  author    = "Ordelman, {Roeland J.F.}",  year      = "2016",  doi       = "10.1145/2983554.2983562",  language  = "English",  booktitle = "MMCommons '16: Proceedings of the 2016 ACM Workshop on Multimedia COMMONS",  publisher = "ACM Publishing",  note      = "Multimedia COMMONS Workshop, MMCommons 2016, MMCommons 2016 ; Conference date: 16-10-2016 Through 16-10-2021", }


@conference{605c27f37b8c4a22a3c5398cb8cece79,  title    = "TREC video retrieval evaluation",  author   = "George Awad and Alan Smeaton and Ian Soboroff and Wessel Kraaij and Angela Ellis and Georges Qu{\'e}not and Darrin Dimmick and Roeland Ordelman and Robin Aly and Maria Eskevich and Martha Larson and Gareth Jones and Jonathan Fiscus and Benoit Huet and David Joy and Marc Ritter and Martial Michel and Stephanie Strassel and Andrew Delgado and Xuansong Li",  year     = "2016",  language = "English",  note     = "TREC Video Retrieval Evaluation, TRECVID 2016, TRECVID 2016 ; Conference date: 14-11-2016 Through 16-11-2016", }


@conference{db55ef0d9cd84e8099754cf7ee6e3c14,  title    = "Video hyperlinking (LNK) TRECVID 2016",  author   = "Maria Eskevich and Martha Larson and Robin Aly and Roeland Ordelman and Jones, {Gareth J.F.} and Benoit Huet",  year     = "2016",  language = "English",  note     = "TREC Video Retrieval Evaluation, TRECVID 2016, TRECVID 2016 ; Conference date: 14-11-2016 Through 16-11-2016", }


@inproceedings{6523b8d139b54f85bed7fcb67ce5c0a1,  title     = "Convenient Discovery of Archived Video Using Audiovisual Hyperlinking",  abstract  = "This paper overviews ongoing work that aims to support end-users in conveniently exploring and exploiting large audiovisual archives by deploying multiple multimodal linking approaches. We present ongoing work on multimodal video hyperlinking, from a perspective of unconstrained link anchor identification and based on the identification of named entities, and recent attempts to implement and validate the concept of outside-in linking that relates current events to archive content. Although these concepts are not new, current work is revealing novel insights, more mature technology, development of benchmark evaluations and emergence of dedicated workshops which are opening many interesting research questions on various levels that require closer collaboration between research communities.",  keywords  = "HMI-MR: MULTIMEDIA RETRIEVAL, EWI-26376, video hyperlinking, IR-98928, multimedia archives, METIS-314986, Digital libraries",  author    = "Ordelman, {Roeland J.F.} and Robin Aly and Maria Eskevich and Benoit Huet and Jones, {Gareth J.F.}",  year      = "2015",  month     = oct,  doi       = "10.1145/2802558.2814652",  language  = "English",  isbn      = "978-1-4503-3749-6",  pages     = "23--26",  booktitle = "Proceedings of the Third Edition Workshop on Speech, Language & Audio in Multimedia (SLAM 2015)",  publisher = "Association for Computing Machinery (ACM)",  address   = "United States",  note      = "3rd Edition Workshop on Speech, Language &amp; Audio in Multimedia, SLAM 2015, SLAM ; Conference date: 30-10-2015 Through 30-10-2015", }


@inproceedings{bb1d465aa67a48d5828e192d0546d041,  title     = "Overview of the 2015 Workshop on Speech, Language and Audio in Multimedia",  abstract  = "The Workshop on Speech, Language and Audio in Multimedia (SLAM) positions itself at at the crossroad of multiple scientific fields - music and audio processing, speech processing, natural language processing and multimedia - to discuss and stimulate research results, projects, datasets and benchmarks initiatives where audio, speech and language are applied to multimedia data. While the first two editions were collocated with major speech events, SLAM{\textquoteright}15 is deeply rooted in the multimedia community, opening up to computer vision and multimodal fusion. To this end, the workshop emphasizes video hyperlinking as an showcase where computer vision meets speech and language. Such techniques provide a powerful illustration of how multimedia technologies incorporating speech, language and audio can make multimedia content collections better accessible, and thereby more useful, to users.",  keywords  = "HMI-MR: MULTIMEDIA RETRIEVAL, EWI-26657, Language, METIS-315141, IR-98930, Audio, Multi-media, Speech",  author    = "Guillaume Gravier and Jones, {Gareth J.F.} and Martha Larson and Ordelman, {Roeland J.F.}",  year      = "2015",  month     = oct,  doi       = "10.1145/2733373.2806414",  language  = "English",  isbn      = "978-1-4503-3459-4",  pages     = "1347--1348",  booktitle = "Proceedings of the 23rd ACM International Conference on Multimedia (MM 2015)",  publisher = "Association for Computing Machinery (ACM)",  address   = "United States",  note      = "23rd ACM Multimedia Conference, MM 2015, MM ; Conference date: 26-10-2015 Through 30-10-2015", }


@inproceedings{4d6f60f91e134146a9250fc06c8f7e68,  title     = "Practice-Oriented Evaluation of Unsupervised Labeling of Audiovisual Content in an Archive Production Environment",  abstract  = "In this paper we report on an evaluation of unsupervised labeling of audiovisual content using collateral text data sources to investigate how such an approach can provide acceptable results given requirements with respect to archival quality, authority and service levels to external users. We conclude that with parameter settings that are optimized using a rigorous evaluation of precision and accuracy, the quality of automatic term-suggestion are sufficiently high. Having implemented the procedure in our production work-flow allows us to gradually develop the system further and also assess the effect of the transformation from manual to automatic from an end-user perspective. Additional future work will be on deploying different information sources including annotations based on multimodal video analysis such as speaker recognition and computer vision.",  keywords  = "EWI-26373, HMI-MR: MULTIMEDIA RETRIEVAL, Thesaurus, Practice-oriented evaluation, METIS-314983, Audiovisual access, Audiovisual archives, Information Extraction, IR-98283",  author    = "{de Boer}, Victor and Ordelman, {Roeland J.F.} and Josefien Schuurman",  note      = "10.1007/978-3-319-24592-8_4 ; null ; Conference date: 14-09-2015 Through 18-09-2015",  year      = "2015",  month     = sep,  doi       = "10.1007/978-3-319-24592-8_4",  language  = "Undefined",  isbn      = "978-3-319-24591-1",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "43--55",  booktitle = "Research and Advanced Technology for Digital Libraries, Proceedings of the 19th International Conference on Theory and Practice of Digital Libraries, TPDL 2015",  address   = "Netherlands",  url       = "http://tpdl2015.info/", }


@inproceedings{130a111503e34981a303386ab358b333,  title     = "SAVA at MediaEval 2015: Search and Anchoring in Video Archives",  abstract  = "The Search and Anchoring in Video Archives (SAVA) task at MediaEval 2015 consists of two sub-tasks: (i) search for multimedia content within a video archive using multimodal queries referring to information contained in the audio and visual streams/content, and (ii) automatic selection of video segments within a list of videos that can be used as anchors for further hyperlinking within the archive. The task used a collection of roughly 2700 hours of the BBC broadcast TV material for the former sub-task, and about 70 les taken from this collection for the latter sub-task. The search sub-task is based on an ad-hoc retrieval scenario, and is evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk (MTurk). The evaluation used metrics that are variations of MAP adjusted for this task. For the anchor selection sub-task overlapping regions of interest across participants submissions were assessed using MTurk workers, and mean reciprocal rank (MRR), precision and recall were calculated for evaluation.",  keywords  = "EWI-26375, IR-98927, METIS-314985, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "Maria Eskevich and Robin Aly and Ordelman, {Roeland J.F.} and Racca, {David N.} and Shu Chen and Jones, {Gareth J.F.}",  year      = "2015",  month     = sep,  language  = "Undefined",  publisher = "CEUR",  pages     = "11",  booktitle = "Working Notes Proceedings of the MediaEval 2015 Workshop",  note      = "null ; Conference date: 14-09-2015 Through 15-09-2015", }


@inproceedings{9258eb74d20c424798f65af643ec3e47,  title     = "Defining and Evaluating Video Hyperlinking for Navigating Multimedia Archives",  abstract  = "Multimedia hyperlinking is an emerging research topic in the context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hyperlinking from a video search perspective in the context of the MediaEval evaluation benchmark for several years. Our task considers a use case of exploring large quantities of video content via an automatically created hyperlink structure at the media fragment level. In this paper we report on our findings, examine the features of the definition of video hyperlinking based on results, and discuss lessons learned with respect to evaluation of hyperlinking in real-life use scenarios.",  keywords  = "EWI-26370, HMI-MR: MULTIMEDIA RETRIEVAL, media fragment, video hyperlinking, METIS-314982, multimedia archives, IR-98282, Benchmark evaluation, Crowdsourcing, Digital libraries",  author    = "Ordelman, {Roeland J.F.} and Maria Eskevich and Robin Aly and Benoit Huet and Jones, {Gareth J.F.}",  note      = "10.1145/2740908.2742915 ; null ; Conference date: 18-05-2015 Through 22-05-2015",  year      = "2015",  month     = may,  doi       = "10.1145/2740908.2742915",  language  = "Undefined",  isbn      = "978-1-4503-3473-0",  publisher = "International World Wide Web Conferences Steering Committee",  pages     = "727--732",  booktitle = "Proceedings of the 24th International Conference on World Wide Web, WWW 2015 Companion",  url       = "http://www2015.wwwconference.org/, http://www.www2015.it/", }


@inproceedings{a039a6b8b4a04e1ab178bd93f66c59e2,  title     = "Exploiting program guides for contextualisation",  abstract  = "Archives of cultural heritage organisations typically consist of collections in various formats (e.g. photos, video, texts) that are inherently related. Often, such disconnected collections represent value in itself but effectuating links between 'core' and 'context' collection items in various levels of granularity could result in a 'one-plus-one-makes-three' scenario both from a contextualisation perspective (public presentations, research) and access perspective. A key issue is the identification of contextual objects that can be associated with objects in the core collections, or the other way around. Traditionally, such associations have been created manually. For most organizations however, this approach does not scale. In this paper, we describe a case in which a semi-automatic approach was employed to create contextual links between television broadcast schedules in program guides (context collection) and the programs in the archive (core collection) of a large audiovisual heritage organisation.",  keywords  = "Contextualisation, Cultural Heritage, Data Analysis, Linked Collections",  author    = "Baltussen, {Lotte Belice} and Themistoklis Karavellas and Ordelman, {Roeland J.F.}",  year      = "2015",  month     = feb,  day       = "25",  doi       = "10.1109/DigitalHeritage.2015.7419477",  language  = "English",  pages     = "161--164",  editor    = "Torres, {Juan Carlos} and Gabriele Guidi and Juan Barcelo and Luciana Duranti and Holger Graf and Pere Brunet and Susan Hazan and Roberto Scopigno and Fabio Remondino",  booktitle = "2015 Digital Heritage International Congress, Digital Heritage 2015",  publisher = "IEEE",  address   = "United States",  note      = "2nd Digital Heritage International Congress: Analysis and Interpretation - Theory, Methodologies, Preservation and Standards, Digital Heritage 2015 ; Conference date: 28-09-2015 Through 02-10-2015", }


@inproceedings{fc56ac0f0031458897090352729ea1d9,  title     = "User perspectives on semantic linking in the audio domain",  abstract  = "Semantic linking has a potential to enrich the audiovisual experience for users of television or radio broadcast archives. Recently, automatic semantic linking, has received increased attention, especially as second screen applications for television broadcasts are emerging. Semantic linking for radio broadcasts can enrich radio listening experience in a similar manner in combination with second screen-like applications. While the development of such applications is gaining popularity, little is known about the information in a radio program that may be interesting for link creation from a user perspective. We conducted a user study on semantic linking for radio broadcasts in order to know what information users regard as suitable anchors and what kind of information they like as targets. We found that users often regard topic and person as the best link anchors in the program. Additionally, we found that frequency and timing of information elements in a radio program do not dominate the users' selection of anchors. Furthermore, we found that there is a low agreement among users on regarding certain information elements as anchors. For practical reasons the study is conducted with 10 minutes of radio broadcast material of a particular program type, and with a total of 22 participants. The insights gained in the user study will help the understanding of user perspectives on semantic linking in the audio domain.",  keywords  = "link anchors, audio domain, audiovisual experience, radio broadcast archives, radio listening experience, second screen applications, METIS-312456, EWI-25386, information elements, television broadcast archives, user perspectives, IR-96793, semantic linking",  author    = "Danish Nadeem and Ordelman, {Roeland J.F.} and Robin Aly and {de Jong}, {Franciska M.G.}",  note      = "10.1109/SITIS.2014.47 ; null ; Conference date: 23-11-2014 Through 27-11-2014",  year      = "2014",  month     = nov,  day       = "24",  doi       = "10.1109/SITIS.2014.47",  language  = "Undefined",  isbn      = "978-1-4799-7978-3",  publisher = "IEEE Computer Society",  pages     = "244--247",  booktitle = "Proceedings of the 10th International Conference on Signal-Image Technology and Internet-Based Systems (SITIS 2014)",  address   = "United States", }


@inproceedings{c15511a5732d4dd7bd98fa9276681870,  title     = "The Search and Hyperlinking Task at MediaEval 2014",  abstract  = "The Search and Hyperlinking Task at MediaEval 2014 is the third edition of this task. As in previous versions, it consisted of two sub-tasks: (i) answering search queries from a collection of roughly 2700 hours of BBC broadcast TV material, and (ii) linking anchor segments from within the videos to other target segments within the video collection. For MediaEval 2014, both sub-tasks were based on an ad-hoc retrieval scenario, and were evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk.",  keywords  = "IR-95307, EWI-25892, METIS-312537",  author    = "Maria Eskevich and Robin Aly and Racca, {David N.} and Ordelman, {Roeland J.F.} and Shu Chen and Jones, {Gareth J.F.}",  year      = "2014",  month     = oct,  language  = "Undefined",  series    = "CEUR workshop proceedings",  publisher = "CEUR",  pages     = "30",  booktitle = "MediaEval 2014: Multimedia Benchmark Workshop",  note      = "null ; Conference date: 01-10-2014", }


@inproceedings{2bba103395f44b158c48ba96e64a1d4f,  title     = "Beyond metadata: searching your archive based on its audio-visual content",  abstract  = "The EU FP7 project AXES aims at better understanding the needs of archive users and supporting them with systems that reach beyond the state-of-the-art. Our system allows users to instantaneously retrieve content using metadata, spoken words, or a vocabulary of reliably detected visual concepts comprising places, objects and events. Additionally, users can query for new concepts, for which models are learned on-the-fly, using training images obtained from an internet search engine. Thanks to advanced analysis and indexation methods, relevant material can be retrieved within seconds. Our system supports different types of models for object categories (e.g. “bus‿ or “house‿), specific objects (landmarks or logos), person categories (e.g. “people with moustaches‿), or specific persons (e.g. “President Obama‿). Next to text queries, we support query-by-example, which retrieves content containing the same location, objects, or faces shown in provided images. Finally, our system provides alternatives to query-based retrieval by allowing users to browse archives using generated links. Here we evaluate the precision of the retrieved results based on textual queries describing visual content, with the queries extracted from user testing query logs.",  keywords  = "audio-visual systems, information retrieval systems, records management, meta data, IR-95305, METIS-312535, EWI-25889",  author    = "T. Tommasi and Robin Aly and K. McGuinness and K. Chatfield and R. Arandjelovic and O. Parkhi and Ordelman, {Roeland J.F.} and A. Zisserman and T. Tuytelaars",  note      = "10.1049/ib.2014.0003 ; null ; Conference date: 01-09-2014",  year      = "2014",  month     = sep,  doi       = "10.1049/ib.2014.0003",  language  = "Undefined",  isbn      = "978-1-84919-927-8",  publisher = "Institution of Engineering and Technology (IET)",  pages     = "1.3",  booktitle = "Proceedings of the 2014 International Broadcasting Convention, IBC 2014", }


@inproceedings{5b0d9f5e326f40c1b65f4c996c54a878,  title     = "Talking With Scholars: Developing a Research Environment for Oral History Collections",  abstract  = "Scholars are yet to make optimal use of Oral History collections. For the uptake of digital research tools in the daily working practice of researchers, practices and conventions commonly adhered to in the subfields of the humanities should be taken into account during development, in order to facilitate the uptake of digital research tools in the daily working practice of researchers. To this end, in the Oral History Today project a research tool for exploring Oral History collections is developed in close collaboration with scholarly researchers. This paper describes four stages of scholarly research and the first steps undertaken to incorporate requirements of these stages in a digital research environment.",  keywords  = "EWI-24673, IR-96792, METIS-312437",  author    = "Max Kemman and Stef Scagliola and {de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.}",  note      = "10.1007/978-3-319-08425-1_22 ; null ; Conference date: 01-07-2014",  year      = "2014",  month     = jul,  doi       = "10.1007/978-3-319-08425-1_22",  language  = "Undefined",  isbn      = "978-3-319-08424-4",  series    = "Communications in Computer and Information Science",  publisher = "Springer",  pages     = "197--201",  booktitle = "Proceedings of the 2nd International Workshop on Supporting Users Exploration of Digital Libraries (SUEDL 2013), Revised Selected Papers",  address   = "Netherlands", }


@inproceedings{f712df0f94ab492ba10989dbd19f0be8,  title     = "Users Requirements in Audiovisual Search: A Quantitative Approach",  abstract  = "This paper reports on the results of a quantitative analysis of user requirements for audiovisual search that allow the categorisation of requirements and to compare requirements across user groups. The categorisation provides clear directions with respect to the prioritisation of system features from the perspective of the development of systems for specific, single user groups and systems that have a more general target user group.",  keywords  = "Audiovisual search, EWI-25166, Quantitative user requirements study, IR-92292, HMI-MR: MULTIMEDIA RETRIEVAL, Multimedia Retrieval, METIS-309606, HMI-HF: Human Factors",  author    = "Danish Nadeem and Ordelman, {Roeland J.F.} and Robin Aly and Erwin Verbruggen",  note      = "http://dx.doi.org/10.1007/978-3-642-40501-3_24 ; null ; Conference date: 22-09-2013 Through 26-09-2013",  year      = "2013",  month     = sep,  day       = "22",  doi       = "10.1007/978-3-642-40501-3_24",  language  = "Undefined",  isbn      = "978-3-642-40500-6",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "241--246",  editor    = "Trond Aalberg and Christos Papatheodorou and Milena Dobreva and Giannis Tsakonas and Farrugia, {Charles J.}",  booktitle = "Proceedings of the International Conference on Theory and Practice of Digital Libraries, TPDL 2013",  address   = "Netherlands",  url       = "http://tpdl2013.upatras.gr/", }


@inproceedings{753fd33b8f1f43f8a044d2f75d9e932d,  title     = "Linking inside a video collection - what and how to measure?",  abstract  = "Although linking video to additional information sources seems to be a sensible approach to satisfy information needs of user, the perspective of users is not yet analyzed on a fundamental level in real-life scenarios. However, a better understanding of the motivation of users to follow links in video, which anchors users prefer to link from within a video, and what type of link targets users are typically interested in, is important to be able to model automatic linking of audiovisual content appropriately. In this paper we report on our methodology towards eliciting user requirements with respect to video linking in the course of a broader study on user requirements in searching and a series of benchmark evaluations on searching and linking.",  keywords  = "EWI-23381, User study, IR-86474, METIS-297661, Video linking",  author    = "Robin Aly and Ordelman, {Roeland J.F.} and M. Eskevich and G.J.F Jones and S. Chen",  note      = "Also available from ACM digital library: http://dl.acm.org/citation.cfm?id=2487967; null ; Conference date: 13-05-2013 Through 17-05-2013",  year      = "2013",  month     = may,  language  = "Undefined",  isbn      = "978-1-4503-2038-2",  publisher = "Association for Computing Machinery (ACM)",  pages     = "457--460",  booktitle = "Proceedings of the 22nd International Conference on World Wide Web Companion, IW3C2 2013",  address   = "United States",  url       = "http://www2013.wwwconference.org/", }


@inproceedings{30054c2f851e4a48bf30e66a62a08fc3,  title     = "Multimedia information seeking through search and hyperlinking",  abstract  = "Searching for relevant webpages and following hyperlinks to related content is a widely accepted and eective approach to information seeking on the textual web. Existing work on multimedia information retrieval has focused on search for individual relevant items or on content linking without specic attention to search results. We describe our research exploring integrated multimodal search and hyperlinking for multimedia data. Our investigation is based on the Medi- aEval 2012 Search and Hyperlinking task. This includes a known-item search task using the Blip10000 internet video collection, where automatically created hyperlinks link each relevant item to related items within the collection. The search test queries and link assessment for this task was generated using the Amazon Mechanical Turk crowdsourc- ing platform. Our investigation examines a range of alterna- tive methods which seek to address the challenges of search and hyperlinking using multimodal approaches. The results of our experiments are used to propose a research agenda for developing eective techniques for search and hyperlinking of multimedia content.",  author    = "Maria Eskevich and Jones, {Gareth J.F} and Robin Aly and Ordelman, {Roeland J.F.} and Shu Chen and Danish Nadeem and Camille Guinaudeau and Guillaume Gravier and Pascale S{\'e}billot and {de Nies}, Tom and Pedro Debevere and {Van de Walle}, Rik and Petra Galu{\v s}{\v c}{\'a}kov{\'a} and Pavel Pecina and Martha Larson",  year      = "2013",  month     = apr,  doi       = "10.1145/2461466.2461511",  language  = "English",  isbn      = "978-1-4503-2033-7",  pages     = "287--294",  booktitle = "ICMR 2013",  publisher = "Association for Computing Machinery (ACM)",  address   = "United States",  note      = "3rd ACM International Conference on Multimedia Retrieval, ICMR 2013, ICMR ; Conference date: 16-04-2013 Through 19-04-2013", }


@inproceedings{36e66c8fd9b64e938331915f1638dd6d,  title     = "Improving cyberbullying detection with user context",  abstract  = "The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited. Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users{\textquoteright} characteristics and profile information. In this paper we show that taking user context into account improves the detection of cyberbullying.",  keywords  = "EWI-23141, METIS-296338, Cyberbullying Detection, Harassment Detection, IR-85503, Sentiment Analysis",  author    = "M. Dadvar and Trieschnigg, {Rudolf Berend} and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  note      = "10.1007/978-3-642-36973-5_62 ; null ; Conference date: 24-03-2013 Through 27-03-2013",  year      = "2013",  month     = mar,  doi       = "10.1007/978-3-642-36973-5_62",  language  = "Undefined",  isbn      = "978-3-642-36972-8",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "693--696",  booktitle = "Proceedings of the 35th European Conference on IR Research, ECIR 2013",  address   = "Netherlands", }


@inproceedings{9fad262aa7c84f1bac8d9b77a5f38fef,  title     = "Search and Hyperlinking Task at MediaEval 2012",  abstract  = "The Search and Hyperlinking Task was one of the Brave New Tasks at MediaEval 2012. The Task consisted of two sub- tasks which focused on search and linking in retrieval from a collection of semi-professional video content. These tasks followed up on research carried out within the MediaEval 2011 Rich Speech Retrieval (RSR) Task and the VideoCLEF 2009 Linking Task.",  keywords  = "EWI-22425, METIS-293185, IR-83390",  author    = "Maria Eskevich and Jones, {Gareth J.F.} and Shu Chen and Robin Aly and Ordelman, {Roeland J.F.} and Martha Larson",  year      = "2012",  month     = oct,  day       = "25",  language  = "Undefined",  publisher = "CEUR",  pages     = "14",  booktitle = "MediaEval 2012 Multimedia Benchmark Workshop",  note      = "null ; Conference date: 04-10-2012 Through 05-10-2012", }


@inproceedings{fbb4662a3f5e497582d8db741b166b03,  title     = "UTwente does Brave New Tasks for MediaEval 2012: Searching and Hyperlinking",  abstract  = "In this paper we report our experiments and results for the brave new searching and hyperlinking tasks for the MediaEval Benchmark Initiative 2012. The searching task involves nding target video segments based on a short natural language sentence query and the hyperlinking task involves nding links from the target video segments to other related video segments in the collection using a set of anchor segments in the videos that correspond to the textual search queries. To nd the starting points in the video, we only used speech transcripts and metadata as evidence source, however, other visual features (for e.g., faces, shots and keyframes) might also aect results for a query. We indexed speech transcripts and metadata, furthermore, the speech transcripts were indexed at speech segment level and at sentence level to improve the likelihood of nding jump-in-points. For linking video segments, we computed k-nearest neighbours of video segments using euclidean distance.",  keywords  = "HMI-MR: MULTIMEDIA RETRIEVAL, EWI-22443, IR-83394, Searching and linking, METIS-296128, Multimedia Retrieval",  author    = "Danish Nadeem and Robin Aly and Ordelman, {Roeland J.F.}",  note      = "eemcs-eprint-22443 ; null ; Conference date: 04-10-2012 Through 05-10-2012",  year      = "2012",  month     = oct,  day       = "5",  language  = "Undefined",  series    = "CEUR Workshop Proceedings",  publisher = "CEUR",  pages     = "44",  booktitle = "MediaEval 2012 Multimedia Benchmark Workshop", }


@article{13d427916a4e4e2b9b00f0bf13e8ae6f,  title     = "The Community and the Crowd: Multimedia Benchmark Dataset Development",  abstract  = "The MediaEval Multimedia Benchmark leveraged community cooperation and crowdsourcing to develop a large Internet video dataset for its Genre Tagging and Rich Speech Retrieval tasks.",  keywords  = "EWI-22370, IR-83367, METIS-296110, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "Martha Larson and Mohammad Soleymani and Maria Eskevich and Pavel Serdyukov and Ordelman, {Roeland J.F.} and Gareth Jones",  note      = "eemcs-eprint-22370 ",  year      = "2012",  month     = jul,  doi       = "10.1109/MMUL.2012.27",  language  = "Undefined",  volume    = "19",  pages     = "15--23",  journal   = "IEEE multimedia",  issn      = "1070-986X",  publisher = "IEEE Computer Society",  number    = "3", }


@inproceedings{8d80ccc149cb4354989448b597ceca2e,  title     = "Comparing Retrieval Effectiveness of Alternative Content Segmentation Methods for Internet Video Search",  abstract  = "We present an exploratory study of the retrieval of semiprofessional user-generated Internet video. The study is based on the MediaEval 2011 Rich Speech Retrieval (RSR) task for which the dataset was taken from the Internet sharing platform blip.tv, and search queries associated with specific speech acts occurring in the video. We compare results from three participant groups using: automatic speech recognition system transcript (ASR), metadata manually assigned to each video by the user who uploaded it, and their combination. RSR 2011 was a known-item search for a single manually identified ideal jump-in point in the video for each query where playback should begin. Retrieval effectiveness is measured using the MRR and mGAP metrics. Using different transcript segmentation methods the participants tried to maximize the rank of the relevant item and to locate the nearest match to the ideal jump-in point. Results indicate that best overall results are obtained for topically homogeneous segments which have a strong overlap with the relevant region associated with the jump-in point, and that use of metadata can be beneficial when segments are unfocused or cover more than one topic.",  author    = "Maria Eskevich and Jones, {Gareth J.F.} and Martha Larson and Christian Wartena and Robin Aly and Thijs Verschoor and Roeland Ordelman",  year      = "2012",  month     = jun,  doi       = "10.1109/CBMI.2012.6269810",  language  = "English",  isbn      = "978-1-4673-2368-0",  series    = "Proceedings International Workshop on Content-Based Multimedia Indexing (CBMI)",  publisher = "IEEE Computer Society",  booktitle = "Proceedings of the 10th Workshop on Content-Based Multimedia Indexing (CMBI 2012)",  address   = "United States",  note      = "10th Workshop on Content-Based Multimedia Indexing, CMBI 2012 ; Conference date: 27-06-2012 Through 29-06-2012", }


@inproceedings{27eacd3607ee44108da16164df41fa64,  title     = "Towards User Modelling in the Combat Against Cyberbullying",  abstract  = "Friendships, relationships and social communications have all gone to a new level with new definitions as a result of the invention of online social networks. Meanwhile, alongside this transition there is increasing evidence that online social applications have been used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the users involved in cyberbullying. We hypothesis that incorporation of the users{\textquoteright} profile, their characteristics, and post-harassing behaviour, for instance, posting a new status in another social network as a reaction to their bullying experience, will improve the accuracy of cyberbullying detection. Cross-system analyses of the users{\textquoteright} behaviour - monitoring users{\textquoteright} reactions in different online environments - can facilitate this process and could lead to more accurate detection of cyberbullying. This paper outlines the framework for this faceted approach.",  keywords  = "METIS-287902, IR-80708, User Profile, HMI-MR: MULTIMEDIA RETRIEVAL, Cyberbullying Detection, EWI-21995, Harassment Detection",  author    = "M. Dadvar and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and Trieschnigg, {Rudolf Berend}",  note      = "10.1007/978-3-642-31178-9_34 ; null ; Conference date: 26-06-2012 Through 28-06-2012",  year      = "2012",  month     = jun,  doi       = "10.1007/978-3-642-31178-9_34",  language  = "Undefined",  isbn      = "978-3-642-31177-2",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "277--283",  booktitle = "Proceedings of the17th International Conference on Applications of Natural Language to Information Systems, NLDB 2012",  address   = "Netherlands", }


@inproceedings{553903cbff15425b8d9c970c2d6b0440,  title     = "Link Anchors in Images: Is there Truth?",  abstract  = "While automatic linking in text collections is well understood, little is known about links in images. In this work, we investigate two aspects of anchors, the origin of a link, in images: 1) the requirements of users for such anchors, e.g. the things users would like more information on, and 2) possible evaluation methods assessing anchor selection algorithms. To investigate these aspects, we perform a study with 102 users. We find that 59% of the required anchors are image segments, as opposed to the whole image, and most users require information on displayed persons. The agreement of users on the required anchors is too low (often below 30%) for a ground truth-based evaluation, which is the standard IR evaluation method. As an alternative, we propose a novel evaluation method based on improved search performance and user experience.",  keywords  = "METIS-286282, EWI-21603, IR-80184",  author    = "Robin Aly and Kevin McGuinness and Martijn Kleppe and Ordelman, {Roeland J.F.} and Noel O'Connor and {de Jong}, {Franciska M.G.}",  year      = "2012",  month     = feb,  day       = "24",  language  = "Undefined",  isbn      = "not assigned",  publisher = "Ghent University",  pages     = "1--4",  booktitle = "Proceedings of the 12th Dutch Belgian Information Retrieval Workshop (DIR 2012)",  note      = "null ; Conference date: 24-02-2012 Through 24-02-2012", }


@inproceedings{af923d24e041439aab3a7fd01c3a7d55,  title     = "Improved cyberbullying detection using gender information",  abstract  = "As a result of the invention of social networks, friendships, relationships and social communication are all undergoing changes and new definitions seem to be applicable. One may have hundreds of {\textquoteleft}friends{\textquoteright} without even seeing their faces. Meanwhile, alongside this transition there is increasing evidence that online social applications are used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the characteristics of the actors involved in cyberbullying. Social studies on cyberbullying reveal that the written language used by a harasser varies with the author{\textquoteright}s features including gender. In this study we used a support vector machine model to train a gender-specific text classifier. We demonstrated that taking gender-specific language features into account improves the discrimination capacity of a classifier to detect cyberbullying.",  keywords  = "Gender distinction, Cyberharassment, IR-79872, METIS-285161, Support vector machine, EC Grant Agreement nr.: FP7/231507, Social Networks, Text Mining, EWI-21608",  author    = "M. Dadvar and {de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and Trieschnigg, {Rudolf Berend}",  year      = "2012",  month     = feb,  day       = "23",  language  = "Undefined",  isbn      = "not assigned",  publisher = "Ghent University",  pages     = "23--25",  booktitle = "Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop (DIR 2012)",  note      = "null ; Conference date: 24-02-2012 Through 24-02-2012", }


@inproceedings{30a832137ac24b08901eaf8afd8cfa5d,  title     = "AXES at TRECVid 2011",  abstract  = "The AXES project participated in the interactive known-item search task (KIS) and the interactive instance search task (INS) for TRECVid 2011. We used the same system architecture and a nearly identical user interface for both the KIS and INS tasks. Both systems made use of text search on ASR, visual concept detectors, and visual similarity search. The user experiments were carried out with media professionals and media students at the Netherlands Institute for Sound and Vision, with media professionals performing the KIS task and media students participating in the INS task. This paper describes the results and findings of our experiments.",  keywords  = "METIS-281633, EC Grant Agreement nr.: FP7/269980, EWI-20973, IR-78926",  author    = "Kevin McGuinness and Robin Aly and Shu Chen and Mathieu Frappier and Martijn Kleppe and Hyowon Lee and Ordelman, {Roeland J.F.} and Relja Arandjelovic",  note      = "eemcs-eprint-20973 ; null ; Conference date: 05-12-2011 Through 07-12-2011",  year      = "2011",  month     = dec,  language  = "Undefined",  isbn      = "not assigned",  series    = "TREC Video Retrieval Evaluation: TRECVID",  publisher = "NIST",  pages     = "--",  booktitle = "TREC 2011 Video Retrieval Evaluation Online Proceedings (TRECVid 2011)", }


@inproceedings{1b26c1869b3e45069ad3833adad701ca,  title     = "Audiovisual Archive Exploitation in the Networked Information Society",  abstract  = "Safeguarding the massive body of audiovisual content, including rich music collections, in audiovisual archives and enabling access for various types of user groups is a prerequisite for unlocking the social-economic value of these collections. Data quantities and the need for specific content descriptors however, force archives to re-evaluate their annotation strategies and access models, and incorporate technology in the archival workflow. It is argued that this can only be successfully done provided that user requirements are studied well and that new approaches are introduced in a well-balanced manner, fitting in with traditional archival perspectives, and by bringing the archivist in the technology loop by means of education and by deploying hybrid work-flows for technology aided annotation.",  keywords  = "METIS-281538, Audiovisual content annotation, IR-78931, HMI-MR: MULTIMEDIA RETRIEVAL, EWI-20629, EC Grant Agreement nr.: FP7/269980, annotation strategies",  author    = "Ordelman, {Roeland J.F.}",  note      = "eemcs-eprint-20629 ; null ; Conference date: 30-11-2011 Through 30-11-2011",  year      = "2011",  month     = nov,  day       = "30",  doi       = "10.1145/2072529.2072535",  language  = "Undefined",  isbn      = "978-1-4503-0986-8",  publisher = "Association for Computing Machinery (ACM)",  pages     = "19--20",  booktitle = "MIRUM '11: Proceedings of the 1st International ACM Workshop on Music Information Retrieval with User-centered and Multimodal Strategies",  address   = "United States", }


@inproceedings{e8e7401babee428c87a5bcb6bfe0ccdc,  title     = "Audio-visual Collections and the User Needs of Scholars in the Humanities: a Case for Co-Development",  abstract  = "The aim of this paper is to reflect on the factors that impede a clear communication and a more fruitful collaboration between humanities scholars and ICT developers. One of the observations is that ICT-researchers who design tools for humanities researchers, are less inclined to take into account that each stage of the scholarly research process requires ICT-support in a different manner or through different tools. Likewise scholars in the humanities often have prejudices concerning ICT-tools, based on lack of knowledge and fears of technology-driven agendas. If the potential for methodological innovation of the humanities is to be realized, the gap between the mindset of ICT-researchers and that of archivists and scholars in the humanities needs to be bridged. Our assumption is that a better insight into the variety of uses of digital collections and a user-inspired classification of ICT-tools, can help to achieve a greater conceptual clarity among both users and developers. This paper presents such an overview in the form of a typology for the audio-visual realm: examples of what role digital audio-visual archives can play at various research stages, and an inventory of the challenges for the parties involved.",  keywords  = "IR-78610, METIS-281584, Audiovisual content, user requirements, EC Grant Agreement nr.: FP7/269980, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, EWI-20868, co-development",  author    = "{de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and Stef Scagliola",  note      = "eemcs-eprint-20868 ; null ; Conference date: 01-11-2011",  year      = "2011",  month     = nov,  language  = "Undefined",  isbn      = "not assigned",  publisher = "Centre for Language Technology, Copenhagen",  pages     = "--",  booktitle = "Proceedings of the 2nd Conference on Supporting Digital Humanities (SDH 2011)", }


@inproceedings{aed466456f3e4c16b826f831329a3836,  title     = "UTwente does Rich Speech Retrieval at MediaEval 2011",  abstract  = "This paper describes the participation of the University of Twente team at the Rich Text Retrieval Task of the Media Eval Benchmark Initiative 2011. The goal of the task is to find entry points of relevant parts of videos to reduce the browsing effort of searchers. This is our first participation, therefore our main focus is to create a baseline system which can be improved in the future. We experiment with different evidence sources (ASR and meta data) together with a basic score combination function. We also experiment with different entry points relative to the segments found by the contained evidence.",  keywords  = "METIS-281622, EWI-20946, IR-78921",  author    = "Robin Aly and T. Verschoor and Ordelman, {Roeland J.F.}",  year      = "2011",  month     = sep,  language  = "Undefined",  series    = "CEUR Workshop Proceedings",  publisher = "Sun SITE Central Europe",  pages     = "1",  booktitle = "Working Notes Proceedings of the MediaEval 2011 Workshop",  note      = "null ; Conference date: 01-09-2011 Through 02-09-2011", }


@inproceedings{2c3f6d63df8a4215b0a7834cd82ab412,  title     = "Distributed Access to Oral History collections: Fitting Access Technology to the needs of Collection Owners and Researchers",  abstract  = "In contrast with the large amounts of potential interesting research material in digital multimedia repositories, the opportunities to unveil the gems therein are still very limited. The Oral History project {\textquoteleft}Verteld Verleden{\textquoteright} (Dutch literal translation of Oral History) that is currently running in The Netherlands, focuses on improving access to spoken testimonies in collections, spread over many Dutch cultural heritage institutions, by deploying modern technology both concerning infrastructure and access. Key objective in the project is mapping the various specific requirements of collection owners and researchers regarding both publishing and access by means of current state-of-the-technology. In order to demonstrate the potential, Verteld Verleden develops an Oral History portal that provides access to distributed collections. At the same time, practical step-by-step plans are provided to get to work with modern access technologies. In this way, a solid starting point for sustained access to Oral History collections can be established.",  keywords  = "METIS-281537, Access Technology, Oral History, E-humanities, EWI-20628, IR-78347",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  year      = "2011",  month     = jun,  day       = "19",  language  = "Undefined",  isbn      = "978-0-911221-47-3",  publisher = "Stanford University Library",  pages     = "347--349",  booktitle = "Digital Humanities 2011: Conference Abstracts",  note      = "null ; Conference date: 19-06-2011 Through 22-06-2011", }


@article{39c4ce097f2f4807a2caf55244c194a8,  title     = "Accessing Audiovisual Heritage: A Roadmap for Collaborative Innovation",  abstract  = "Digitization of audiovisual archives is opening up a wealth of challenges and possibilities for innovations in science, education, and business. The key to unlocking archives for innovation is multimedia technology. In this article the authors zoom in on one of the largest multimedia archives in Europe, highlight collaborative projects with academia, and call for a mutual research agenda.",  keywords  = "EWI-20988, IR-78935, METIS-286270, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "Johan Oomen and Ordelman, {Roeland J.F.}",  note      = "digital libraries, annotation, multimedia retrieval, IEEE MultiMedia, Media Impact, graphics and multimedia",  year      = "2011",  month     = apr,  doi       = "10.1109/MMUL.2011.60",  language  = "Undefined",  volume    = "18",  pages     = "4--10",  journal   = "IEEE multimedia",  issn      = "1070-986X",  publisher = "IEEE Computer Society",  number    = "4", }


@inproceedings{5cf4241a01254a45adb5df2cb1928665,  title     = "Automatic Tagging and Geotagging in Video Collections and Communities",  abstract  = "Automatically generated tags and geotags hold great promise to improve access to video collections and online communi- ties. We overview three tasks oered in the MediaEval 2010 benchmarking initiative, for each, describing its use scenario, denition and the data set released. For each task, a refer- ence algorithm is presented that was used within MediaEval 2010 and comments are included on lessons learned. The Tagging Task, Professional involves automatically matching episodes in a collection of Dutch television with subject la- bels drawn from the keyword thesaurus used by the archive sta. The Tagging Task, Wild Wild Web involves automat- ically predicting the tags that are assigned by users to their online videos. Finally, the Placing Task requires automati- cally assigning geo-coordinates to videos. The specication of each task admits the use of the full range of available in- formation including user-generated metadata, speech recog- nition transcripts, audio, and visual features.",  keywords  = "METIS-279235, HMI-MR: MULTIMEDIA RETRIEVAL, EWI-20630, IR-78255",  author    = "Martha Larson and Mohammad Soleymani and Pavel Serdyukov and Stevan Rudinac and Christian Wartena and Vanessa Murdock and Ordelman, {Roeland J.F.} and Gerald Friedland and Jones, {Gareth J.F.}",  note      = "eemcs-eprint-20630 ; null ; Conference date: 17-04-2011 Through 20-04-2011",  year      = "2011",  month     = apr,  doi       = "10.1145/1991996.1992047",  language  = "Undefined",  isbn      = "978-1-4503-0336-1",  publisher = "Association for Computing Machinery (ACM)",  pages     = "51:1--51:8",  booktitle = "Proceedings of the 1st ACM International Conference on Multimedia Retrieval, ICMR2011",  address   = "United States", }


@inproceedings{b797656a3c6842afb5165e83d6e03d8d,  title     = "Automated Metadata Extraction for Semantic Access to Spoken Word Archives",  abstract  = "Archival practice is shifting from the analogue to the digital world. A specific subset of heritage collections that impose interesting challenges for the field of language and speech technology are spoken word archives. Given the enormous backlog at audiovisual archives of unannotated materials and the generally global level of item description, collection disclosure and item access are both at risk, and (semi-)automated methods for analysis and annotation may help to increase the use and reuse of these rich content collections. In several HMI projects the interplay has been investigated between evolving user scenarios and user requirements for spoken audio collections on the one hand, and the potential of automatic annotation and search technology for the improved accessibility and search paradigms on the other hand. In this paper we will present an overview of the state-of-the-art in metadata generation for audio content and explain the crucial importance of involving user groups in the design of research agendas and road maps for novel applications in this domain.",  keywords  = "METIS-277425, IR-75826, EWI-18431, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology",  author    = "{de Jong}, {Franciska M.G.} and W.F.L. Heeren and {van Hessen}, {Adrianus J.} and Ordelman, {Roeland J.F.} and Antinus Nijholt",  note      = "cultural heritage, spoken audio collection, automatic annotation, speech technology, information retrieval; null ; Conference date: 17-01-2011 Through 21-01-2011",  year      = "2011",  month     = jan,  day       = "17",  language  = "Undefined",  isbn      = "978-959-7174-19-6",  publisher = "Centre for Applied Linguistics",  pages     = "896--905",  editor    = "{Ruiz Miyares}, L. and {Alvarez Silva}, M.R.",  booktitle = "Proceedings 12th International Symposium on Social Communication", }


@book{f3ef34f30aa646619f4c37558fdae4c2,  title     = "SSCS'10: Proceedings of the 2010 ACM Workshop on Searching Spontaneous Conversational Speech",  abstract  = "The spoken word is a valuable source of semantic information. Techniques that exploit the spoken word by making use of speech recognition or spoken audio analysis hold clear potential for improving multimedia search. Nonetheless, speech technology remains underexploited by systems that provide access to spoken audio or video with a speech track. Indexing the spoken audio produced by speakers engaging in conversation or otherwise speaking spontaneously is particularly challenging. The challenges arise due to the wide variability and highly unstructured nature of unplanned, informal speech. Development of approaches that can effectively exploit the semantic content of spontaneous, conversational speech requires integration of speech recognition, audio processing, multimedia analysis and information retrieval. The SSCS workshop series is devoted to providing a forum where scientists engaged in spoken content retrieval research at the intersection of these disciplines can meet, present and discuss recent research results and also formulate a common vision on the future of spoken content retrieval. The research papers presented at SSCS 2010 cluster around topics that are central for spoken content retrieval. Two papers focus on specific indexing techniques applied to spontaneous speech: speaker role recognition and concept detection. Two papers treat Spoken Term Detection, addressing the challenge of terms that cannot be indexed using conventional approaches since they are not contained in the speech recognizer vocabulary (i.e., the so-called Out-Of-Vocabulary problem). Finally, three papers are devoted to topics related to the automatic segmentation of spontaneous conversational content and deal with issues involving the combination of automatic segmentation and information retrieval. SSCS 2010 continues the tradition of past years by including a demonstration session that allows hands-on interaction with systems implementing state-of-the-art approaches to spoken content retrieval. Five demonstration papers give the details of the systems presented. SSCS 2010 includes a number of presentations by invited speakers who address topics related to the user perspective on spoken content retrieval and to domains that are anticipated to give rise to future issues faced by scientists working in the field.",  keywords  = "EWI-19423, METIS-276314",  editor    = "Martha Larson and Roeland Ordelman and Florian Metze and Wessel Kraaij and {de Jong}, Franciska",  note      = "ACM Multimedia 2010",  year      = "2010",  month     = oct,  day       = "29",  language  = "English",  isbn      = "978-1-4503-0162-6",  publisher = "Association for Computing Machinery (ACM)",  address   = "United States", }


@inproceedings{2f97fac5188a4506a97508756fde9823,  title     = "Towards affective state modeling in narrative and conversational settings",  abstract  = "We carry out two studies on affective state modeling for communication settings that involve unilateral intent on the part of one participant (the evoker) to shift the affective state of another participant (the experiencer). The first investigates viewer response in a narrative setting using a corpus of documentaries annotated with viewer-reported narrative peaks. The second investigates affective triggers in a conversational setting using a corpus of recorded interactions, annotated with continuous affective ratings, between a human interlocutor and an emotionally colored agent. In each case, we build a {"}one-sided{"} model using indicators derived from the speech of one participant. Our classification experiments confirm the viability of our models and provide insight into useful features.",  keywords  = "METIS-271081, IR-74046, HMI-SLT: Speech and Language Technology, EWI-18625, Speech Analysis, Affect recognition",  author    = "Bart Jochems and Martha Larson and Ordelman, {Roeland J.F.} and Poppe, {Ronald Walter} and Truong, {Khiet Phuong}",  year      = "2010",  month     = sep,  language  = "English",  isbn      = "1990-9772",  publisher = "International Speech Communication Association (ISCA)",  pages     = "490--493",  booktitle = "Proceedings of Interspeech 2010",  note      = "11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010, INTERSPEECH ; Conference date: 26-09-2010 Through 30-09-2010",  url       = "http://www.interspeech2010.jpn.org/", }


@article{698d0d7654f242c6853e3d5c9cb68cbd,  title     = "Multimedia with a speech track: searching spontaneous conversational speech",  abstract  = "After two successful years at SIGIR in 2007 and 2008, the third workshop on Searching Spontaneous Conversational Speech (SSCS 2009) was held conjunction with the ACM Multimedia 2009. The goal of the SSCS series is to serve as a forum that brings together the disciplines that collaborate on spoken content retrieval, including information retrieval, speech recognition and multimedia analysis. Multimedia collections often contain a speech track, but in many cases it is ignored or not fully exploited for information retrieval. Currently, spoken content retrieval research is expanding beyond highly-conventionalized domains such as broadcast news in to domains involving speech that is produced spontaneously and in conversational settings. Such speech is characterized by wide variability of speaking styles, subject matter and recording conditions. The work presented at SSCS 2009 included techniques for searching meetings, interviews, telephone conversations, podcasts and spoken annotations. The work encompassed a large range of approaches including using subword units, exploiting dialogue structure, fusing retrieval models, modeling topics and integrating visual features. Taken in sum, the workshop demonstrated the high potential of new ideas emerging in the area of speech search and also reinforced the need for concentrated research devoted to the classic challenges of spoken content retrieval, many of which remain yet unsolved.",  keywords  = "EWI-19891, EC Grant Agreement nr.: FP7/216444, IR-76507, METIS-276408, EC Grant Agreement nr.: FP6/033812",  author    = "Martha Larson and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and Joachim Kohler and Wessel Kraaij",  note      = "10.1145/1842890.1842901 ",  year      = "2010",  month     = aug,  doi       = "10.1145/1842890.1842901",  language  = "Undefined",  volume    = "44",  pages     = "76--81",  journal   = "SIGIR forum",  issn      = "0163-5840",  publisher = "Association for Computing Machinery (ACM)",  number    = "1", }


@article{78f937c30c8d4375b1cb543aa4498633,  title     = "A system for the semantic multimodal analysis of news audio-visual content",  abstract  = "News-related content is nowadays among the most popular types of content for users in everyday applications. Although the generation and distribution of news content has become commonplace, due to the availability of inexpensive media capturing devices and the development of media sharing services targeting both professional and user-generated news content, the automatic analysis and annotation that is required for supporting intelligent search and delivery of this content remains an open issue. In this paper, a complete architecture for knowledge-assisted multimodal analysis of news-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately and proposes a novel fusion technique based on the particular characteristics of news-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic annotations.",  keywords  = "EWI-19888, IR-76505, METIS-276406",  author    = "Vasileios Mezaris and Spyros Gidaros and Papadopoulos, {Georgios Th.} and Walter Kasper and Ordelman, {Roeland J.F.} and J{\"o}rg Steffen and M.A.H. Huijbregts and {de Jong}, {Franciska M.G.} and Ioannis Kompatsiaris and Strintzis, {Michael G.}",  note      = "10.1155/2010/645052 ",  year      = "2010",  month     = feb,  doi       = "10.1155/2010/645052",  language  = "Undefined",  volume    = "2010",  pages     = "645052",  journal   = "EURASIP journal on advances in signal processing",  issn      = "1687-6172",  publisher = "Springer",  number    = "47", }


@inproceedings{47b63cc990f843f583517294c28857c6,  title     = "Crowdsourcing rock n' roll multimedia retrieval",  abstract  = "In this technical demonstration, we showcase a multimedia search engine that facilitates semantic access to archival rock n' roll concert video. The key novelty is the crowdsourcing mechanism, which relies on online users to improve, extend, and share, automatically detected results in video fragments using an advanced timeline-based video player. The user-feedback serves as valuable input to further improve automated multimedia retrieval results, such as automatically detected concepts and automatically transcribed interviews. The search engine has been operational online to harvest valuable feedback from rock n' roll enthusiasts.",  keywords  = "METIS-275962, EWI-19889, IR-76506",  author    = "Snoek, {Cees G.M.} and Bauke Freiburg and Johan Oomen and Ordelman, {Roeland J.F.}",  note      = "10.1145/1873951.1874278 ; null ; Conference date: 25-10-2010 Through 29-10-2010",  year      = "2010",  doi       = "10.1145/1873951.1874278",  language  = "Undefined",  isbn      = "978-1-60558-933-6",  publisher = "Association for Computing Machinery (ACM)",  pages     = "1535--1538",  booktitle = "Proceedings of the ACM Multimedia Conference, MM 2010",  address   = "United States",  url       = "http://www.sigmm.org/archive/MM/mm10/www.acmmm10.org/index.html", }


@inproceedings{d51679e3637248f4835d67c306ff2bd6,  title     = "Exploiting Speech Recognition Transcripts for Narrative Peak Detection in Short-Form Documentaries",  abstract  = "Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative ﬂow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers{\textquoteright} feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches deliver top narrative peak detection results. One uses counts of personal pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language.",  keywords  = "METIS-278696, IR-78253, EWI-19371, HMI-SLT: Speech and Language Technology, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "Martha Larson and Bart Jochems and Ewine Smits and Ordelman, {Roeland J.F.}",  note      = "10.1007/978-3-642-15751-6_50 ; null ; Conference date: 30-09-2009 Through 02-10-2009",  year      = "2010",  doi       = "10.1007/978-3-642-15751-6_50",  language  = "Undefined",  isbn      = "978-3-642-15750-9",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "385--392",  editor    = "Carol Peters and Barbara Caputo and Julio Gonzalo",  booktitle = "10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009",  address   = "Netherlands", }


@article{d92f0a84eb194e11901bd50e727b82e0,  title     = "Towards Affordable Disclosure of Spoken Heritage Archives",  abstract  = "This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken heritage archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, we at least want to provide search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition - supporting e.g., within-document search - are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is not yet satisfactory, and requires additional research.",  keywords  = "DB-MMR: MULTIMEDIA RETRIEVAL, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, Speech Indexing, Speech Recognition, Multimedia Retrieval, Workflow, Usability, Cultural heritage archives",  author    = "Roeland Ordelman and Willemijn Heeren and {de Jong}, Franciska and Marijn Huijbregts and Djoerd Hiemstra",  note      = "Special Issue on Information Access to Cultural Heritage",  year      = "2009",  month     = dec,  language  = "English",  volume    = "10",  journal   = "Journal of digital information",  issn      = "1368-7506",  publisher = "British Computer Society",  number    = "6", }


@inproceedings{c555886985c44720b1c71eb037779c55,  title     = "Searching Multimedia Content with a Spontaneous Conversational Speech Track",  abstract  = "Spoken document retrieval research effort invested into developing broadcast news retrieval systems has yielded impressive results. This paper is the introduction the proceedings of the 3rd workshop aiming at the advancement of the field in less explored domains (SSCS2009) which was organized in conjunction to the ACM Multimedia Conference in Beijing.",  keywords  = "METIS-264239, IR-68942, Spoken content, Speech, CR-H.3.1, multimedia access, EWI-17000, audio-visual retrieval, Speech Recognition",  author    = "Martha Larson and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and Wessel Kraaij and Joachim Kohler",  note      = "10.1145/1631127 ; null ; Conference date: 23-10-2009 Through 23-10-2009",  year      = "2009",  month     = oct,  day       = "20",  doi       = "10.1145/1631127",  language  = "Undefined",  isbn      = "978-1-60558-608-3",  publisher = "Association for Computing Machinery (ACM)",  pages     = "2--4",  booktitle = "Proceedings of the Third Workshop on Searching Spontaneous Conversational Speech (SSCS2009)",  address   = "United States", }


@book{3ef7781d3717409fb0fb0712712935e6,  title     = "Unravelling the Voice of Willem Frederik Hermans: an Oral History Indexing Case Study",  keywords  = "EWI-15978, IR-67573, METIS-263981, HMI-SLT: Speech and Language Technology",  author    = "Ordelman, {Roeland J.F.} and M.A.H. Huijbregts and {de Jong}, {Franciska M.G.}",  year      = "2009",  month     = sep,  day       = "3",  language  = "Undefined",  series    = "CTIT Technical Report Series",  publisher = "Centre for Telematics and Information Technology (CTIT)",  number    = "TR-CTIT-05-72",  address   = "Netherlands", }


@article{6fed06e9bbfa465e8f2a49b0471061ca,  title     = "Easy Listening: Spoken Document Retrieval in CHoral",  abstract  = "Given the enormous backlog at audiovisual archives and the generally global level of item description, collection disclosure and item access are both at risk. At the same time, archival practice is seeking to evolve from the analogue to the digital world. CHoral investigates the role automatic annotation and search technology can play in improving disclosure and access of digitized spoken word collections during and after this transfer. The core business of the CHoral project is to design and build technology for spoken document retrieval for heritage collections. In this paper, we will argue that in addition to solving technological issues, closer attention is needed for the work-flow and daily practice at audiovisual archives on the one hand, and the state-of-the-art in technology on the other. Analysis of the interplay is needed to ensure that new developments are mutually beneficial and that continuing cooperation can indeed bring envisioned advancements.",  keywords  = "EWI-15930, IR-67854, METIS-264420",  author    = "W.F.L. Heeren and {van der Werff}, {Laurens Bastiaan} and {de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and T. Verschoor and {van Hessen}, {Adrianus J.} and Mies Langelaar",  note      = "10.1179/174327909X441135 ",  year      = "2009",  month     = sep,  doi       = "10.1179/174327909X441135",  language  = "Undefined",  volume    = "34",  pages     = "236--252",  journal   = "Interdisciplinary science reviews",  issn      = "0308-0188",  publisher = "Maney Publishing",  number    = "2-3", }


@inproceedings{ecb764d157224c919a41d4f4e9090a2e,  title     = "Enhanced multimedia content access and exploitation using semantic speech retrieval",  abstract  = "Techniques for automatic annotation of spoken content making use of speech recognition technology have long been characterized as holding unrealized promise to provide access to archives inundated with undisclosed multimedia material. This paper provides an overview of techniques and trends in semantic speech retrieval, which is taken to encompass all approaches offering meaning-based access to spoken word collections. We present descriptions, examples and insights for current techniques, including facing real-world heterogenity, aligning parallel resources and exploiting collateral collections. We also discuss ways in which speech recognition technology can be used to create multimedia connections that make new modes of access available to users. We conclude with an overview of the challenges for semantic speech retrieval in the workflow of a real-world archive and perspectives on future tasks in which speech retrieval integrates information related to affect and appeal, dimensions that transcend topic.",  keywords  = "METIS-264028, IR-68255, Spoken content, Speech Recognition, HMI-MR: MULTIMEDIA RETRIEVAL, multimedia retrieval and access, Semantics, EWI-16069, HMI-SLT: Speech and Language Technology, Speech retrieval",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and Martha Larson",  year      = "2009",  month     = sep,  doi       = "10.1109/ICSC.2009.80",  language  = "Undefined",  isbn      = "978-0-7695-3800-6",  publisher = "IEEE Computer Society Press",  pages     = "521--528",  booktitle = "Proceedings of the Third IEEE International Conference on Semantic Computing",  note      = "null ; Conference date: 14-09-2009 Through 16-09-2009", }


@inproceedings{2591ae7f90534838800d8e8266d31d3f,  title     = "StreetTiVo: Using a P2P XML Database System to Manage Multimedia Data in Your Living Room",  abstract  = "StreetTiVo is a project that aims at bringing research results into the living room; in particular, a mix of current results in the areas of Peer-to-Peer XML Database Management System (P2P XDBMS), advanced multimedia analysis techniques, and advanced information re- trieval techniques. The project develops a plug-in application for the so-called Home Theatre PCs, such as set-top boxes with MythTV or Windows Media Center Edition installed, that can be considered as programmable digital video recorders. StreetTiVo distributes compute- intensive multimedia analysis tasks over multiple peers (i.e., StreetTiVo users) that have recorded the same TV program, such that a user can search in the content of a recorded TV program shortly after its broad- casting; i.e., it enables near real-time availability of the meta-data (e.g., speech recognition) required for searching the recorded content. Street- TiVo relies on our P2P XDBMS technology, which in turn is based on a DHT overlay network, for distributed collaborator discovery, work coor- dination and meta-data exchange in a volatile WAN environment. The technologies of video analysis and information retrieval are seamlessly integrated into the system as XQuery functions.",  keywords  = "HMI-MR: MULTIMEDIA RETRIEVAL, EWI-15100, multimedia analysis, IR-67995, peer-to-peer, XML database, METIS-265193, Speech Recognition",  author    = "Ying Zhang and {de Vries}, A.P. and P. Boncz and Djoerd Hiemstra and Ordelman, {Roeland J.F.}",  note      = "10.1007/978-3-642-00672-2_36 ; null ; Conference date: 02-04-2009 Through 04-04-2009",  year      = "2009",  month     = apr,  doi       = "10.1007/978-3-642-00672-2_36",  language  = "Undefined",  isbn      = "978-3-642-00671-5",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "404--415",  editor    = "Qing Li and Ling Feng and Jian Pei and Wang, {Sean X.}",  booktitle = "APWeb/WAIM 2009",  address   = "Netherlands", }


@inproceedings{68b0337332b44db7aa45d20092f5fcd8,  title     = "Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs",  abstract  = "Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, given the growing amount of materials that are being created on a daily basis and the digitization of existing analogue collections, the traditional manual annotation of collections puts heavy demands on resources, especially for large audiovisual archives. One way to address this challenge, is to introduce (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords form textual resources related to the TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. Besides the descriptions of the programs published by the broadcasters on their Websites, Automatic Speech Transcription (ASR) techniques from the CATCH-CHoral project, also provide textual resources that might be relevant for suggesting keywords. This paper investigates the suitability of ASR for generating such keywords, which we evaluate against manual annotations of the documents and against keywords automatically generated from context documents.",  keywords  = "EWI-16997, Keyword extraction, Automatic Speech Recognition, Audiovisual Documents, IR-68941, METIS-264237",  author    = "V{\'e}ronique Malais{\'e} and Luit Gazendam and Willemijn Heeren and Roeland Ordelman and Hennie Brugman",  year      = "2009",  language  = "English",  booktitle = "Actes de la 16{\`e}me Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles, TALN 2009, Senlis (France)",  publisher = "Association pour le Traitement Automatique des Langues (ATALA)",  address   = "France",  note      = "16{\`e}me Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles, TALN 2009, TALN ; Conference date: 24-06-2009 Through 26-06-2009", }


@article{e07a3ed6cd7848c5ba93ab1a1dd7d05a,  title     = "Spoken Content Retrieval: Searching Spontaneous Conversational Speech",  abstract  = "The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval. At the workshop, talks and posters were presented covering a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. Demonstrations of speech-based retrieval systems from a variety of application domains introduced a strong practical emphasis into the workshop program. The workshop concluded with a panel discussion, whose goal it was to identify future research directions for speech retrieval. Among the important challenges identified during the panel discussions were: dealing with large scale multimedia collections, representing audio/video content effectively in the user interface, focusing on perfecting the component technologies on which speech retrieval systems are based, and developing systems and approaches that will enable users (both content seekers and content providers) to actively create their own speech search applications or contribute to the indexability of their content.",  keywords  = "HMI-HF: Human Factors, Spoken document retrieval, EC Grant Agreement nr.: FP6/027685, EC Grant Agreement nr.: FP6/045480, EC Grant Agreement nr.: FP6/033104, EC Grant Agreement nr.: FP6/003812",  author    = "Joachim Kohler and Martha Larson and {de Jong}, Franciska and Roeland Ordelman and Wessel Kraaij",  year      = "2008",  month     = dec,  doi       = "10.1145/1480506.1480518",  language  = "English",  volume    = "42",  pages     = "67--77",  journal   = "SIGIR forum",  issn      = "0163-5840",  publisher = "Association for Computing Machinery (ACM)",  number    = "2",  note      = "ACM/SIGIR Workshop on Searching Spontaneous Conversational Speech, SSCS 2008, SSCS ; Conference date: 24-07-2008 Through 24-07-2008", }


@inproceedings{059549b5e3d84a6693747b6e59bbea20,  title     = "Fast N-Gram Language Model Look-Ahead for Decoders With Static Pronunciation Prefix Trees",  abstract  = "Decoders that make use of token-passing restrict their search space by various types of token pruning. With use of the Language Model Look-Ahead (LMLA) technique it is possible to increase the number of tokens that can be pruned without loss of decoding precision. Unfortunately, for token passing decoders that use single static pronunciation prefix trees, full n-gram LMLA increases the needed number of language model probability calculations considerably. In this paper a method for applying full n-gram LMLA in a decoder with a single static pronunciation tree is introduced. The experiments show that this method improves the speed of the decoder without an increase of search errors.",  keywords  = "HMI-SLT: Speech and Language Technology, Language Model Look-Ahead, Language modeling, EC Grant Agreement nr.: FP6/027413, METIS-255484, IR-65373, Automatic Speech Recognition, Decoding, EWI-15021",  author    = "M.A.H. Huijbregts and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  year      = "2008",  month     = sep,  day       = "22",  language  = "English",  publisher = "International Speech Communication Association (ISCA)",  number    = "412",  pages     = "91",  booktitle = "Proceedings of Interspeech",  note      = "9th Annual Conference of the International Speech Communication Association, INTERSPEECH 2008, INTERSPEECH ; Conference date: 22-09-2008 Through 26-09-2008", }


@inproceedings{d852808a08d74d569d08ccc7ef956860,  title     = "Towards Affordable Disclosure of Spoken Word Archives",  abstract  = "This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken word archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, the least we want to be able to provide is search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition – supporting e.g., within-document search– are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is still far from satisfactory, and requires additional research.",  keywords  = "EWI-13526, HMI-MR: MULTIMEDIA RETRIEVAL, IR-65010, METIS-251204, HMI-SLT: Speech and Language Technology",  author    = "Ordelman, {Roeland J.F.} and W.F.L. Heeren and M.A.H. Huijbregts and Djoerd Hiemstra and {de Jong}, {Franciska M.G.}",  note      = "http://eprints.ewi.utwente.nl/13526 ; null ; Conference date: 18-09-2008 Through 18-09-2008",  year      = "2008",  month     = sep,  day       = "18",  language  = "Undefined",  isbn      = "978-90-813489-1-1",  publisher = "ILPS, University of Amsterdam",  number    = "Supplement",  pages     = "--",  editor    = "M Larson and K Fernie and J Oomen and J. Cigarran",  booktitle = "Proceedings of the ECDL 2008 Workshop on Information Access to Cultural Heritage (IACH2008)", }


@book{92b2743eae8e441a99c4d1c330b11ac9,  title     = "Searching Spontaneous Conversational Speech: Proceedings of ACM SIGIR Workshop (SSCS2008)",  abstract  = "The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval. The proceedings contain papers on a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. A workshop reprot has been published in ACM SIGIR Forum, issue December 2008.",  keywords  = "Spoken document retrieval, Speech recognition",  author    = "M. Larson and W. Kraaij",  editor    = "J. K{\"o}hler and {de Jong}, F.M.G. and R.J.F. Ordelman",  year      = "2008",  month     = jul,  day       = "24",  language  = "English",  isbn      = "978-90-365-2697-5",  publisher = "Centre for Telematics and Information Technology (CTIT)",  address   = "Netherlands",  note      = "ACM/SIGIR Workshop on Searching Spontaneous Conversational Speech, SSCS 2008, SSCS ; Conference date: 24-07-2008 Through 24-07-2008", }


@article{f55b8f11b04b47a7a6f587fd1451d064,  title     = "Access to recorded interviews: A research agenda",  abstract  = "Recorded interviews form a rich basis for scholarly inquiry. Examples include oral histories, community memory projects, and interviews conducted for broadcast media. Emerging technologies offer the potential to radically transform the way in which recorded interviews are made accessible, but this vision will demand substantial investments from a broad range of research communities. This article reviews the present state of practice for making recorded interviews available and the state-of-the-art for key component technologies. A large number of important research issues are identified, and from that set of issues, a coherent research agenda is proposed.",  keywords  = "EC Grant Agreement nr.: FP6/027617, IR-64820, METIS-255945, EWI-12910",  author    = "{de Jong}, {Franciska M.G.} and D.W. Oard and W.F.L. Heeren and Ordelman, {Roeland J.F.}",  note      = "http://eprints.ewi.utwente.nl/12910 ",  year      = "2008",  month     = jun,  doi       = "10.1145/1367080.1367083",  language  = "Undefined",  volume    = "1",  pages     = "3:1--3:27",  journal   = "ACM journal on computing and cultural heritage",  issn      = "1556-4673",  publisher = "Association for Computing Machinery (ACM)",  number    = "1", }


@inproceedings{ef90213f87cc43d6b1626d6938037730,  title     = "Affordable access to multimedia by exploiting collateral data",  abstract  = "In addition to multimedia collections and their metadata, there often is a variety of collateral data sources available on (parts of) a collection. Collateral data – secondary information objects that relate to the primary multimedia documents – can be very useful in the process of automated generation of annotations for multimedia archives in that they reduce both costs and effort in annotation and access. Furthermore, they can be used to enhance result presentation in retrieval engines. To optimally exploit collateral data, methods for automatic indexing as well as changes in the current archiving workflow are proposed.",  keywords  = "METIS-251019, EWI-12904, HMI-MR: MULTIMEDIA RETRIEVAL, IR-62364",  author    = "W.F.L. Heeren and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  note      = "10.1109/CBMI.2008.4564994 ; null ; Conference date: 18-06-2008 Through 20-06-2008",  year      = "2008",  month     = jun,  doi       = "10.1109/CBMI.2008.4564994",  language  = "Undefined",  isbn      = "978-1-4244-2044-5",  publisher = "IEEE",  number    = "274",  pages     = "542--550",  booktitle = "Proceedings of CBMI 2008",  address   = "United States", }


@inproceedings{6c824f8e13a848b982e894e9f8d7f425,  title     = "Knowledge-assisted cross-media analysis of audio-visual content in the news domain",  abstract  = "In this paper, a complete architecture for knowledge-assisted cross-media analysis of News-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately, and proposes a fusion technique based on the particular characteristics of News-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic video annotations.",  keywords  = "EC Grant Agreement nr.: FP6/027685, IR-64879, METIS-251082, EWI-13063",  author    = "Vasileios Mezaris and Spyros Gidaros and Papadopoulos, {Georgios Th.} and Walter Kasper and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and Ioannis Kompatsiaris",  year      = "2008",  month     = jun,  doi       = "10.1109/CBMI.2008.4564958",  language  = "English",  isbn      = "978-1-4244-2043-8",  pages     = "280--287",  booktitle = "Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008.",  publisher = "IEEE Computer Society",  address   = "United States",  note      = "International Workshop on Content-Based Multimedia Indexing, CBMI 2008, CBMI 2008 ; Conference date: 18-06-2008 Through 20-06-2008", }


@inproceedings{c8e3e1f9008d40a1b666addd12d61c3e,  title     = "Mediacampaign: A Multimodal Semantic Analysis System for Advertisement Campaign Detection",  abstract  = "MediaCampaign's scope is on discovering and inter-relating advertisements and campaigns, i.e. to relate advertisements semantically belonging together, across different countries and different media. The project{\textquoteright}s main goal is to automate to a large degree the detection and tracking of advertisement campaigns on television, Internet and in the press. For this purpose we introduce a first prototype of a fully integrated semantic analysis system based on an ontology which automatically detects new creatives and campaigns by utilizing a multimodal analysis system and a framework for the resolution of semantic identity.",  keywords  = "EC Grant Agreement nr.: FP6/027413, IR-64880, METIS-251083, EWI-13066",  author    = "Herwig Rehatschek and Robert Sorschag and Bernhard Rettenbacher and Herwig Zeiner and Julien Nioche and {de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and {van Leeuwen}, {David A.}",  year      = "2008",  month     = jun,  doi       = "10.1109/CBMI.2008.4564932",  language  = "English",  isbn      = "978-1-4244-2043-8",  pages     = "85--92",  booktitle = "Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008.",  publisher = "IEEE Computer Society",  address   = "United States",  note      = "International Workshop on Content-Based Multimedia Indexing, CBMI 2008, CBMI 2008 ; Conference date: 18-06-2008 Through 20-06-2008", }


@inproceedings{7398d937c95e44e3a40a981649adc59b,  title     = "From D-Coi to SoNaR: A reference corpus for Dutch",  abstract  = "The computational linguistics community in The Netherlands and Belgium has long recognized the dire need for a major reference corpus of written Dutch. In part to answer this need, the STEVIN programme was established. To pave the way for the effective building of a 500-million-word reference corpus of written Dutch, a pilot project was established. The Dutch Corpus Initiative project or D-Coi was highly successful in that it not only realized about 10% of the projected large reference corpus, but also established the best practices and developed all the protocols and the necessary tools for building the larger corpus within the confines of a necessarily limited budget. We outline the steps involved in an endeavour of this kind, including the major highlights and possible pitfalls. Once converted to a suitable XML format, further linguistic annotation based on the state-of-the-art tools developed either before or during the pilot by the consortium partners proved easily and fruitfully applicable. Linguistic enrichment of the corpus includes PoS tagging, syntactic parsing and semantic annotation, involving both semantic role labeling and spatiotemporal annotation. D-Coi is expected to be followed by SoNaR, during which the 500-million-word reference corpus of Dutch should be built.",  keywords  = "HMI-SLT: Speech and Language Technology, EWI-15099, LR national/international projects, etc), METIS-255893, organizational/policy issues, IR-62741, Corpus (creation, annotation, Standards for LRs",  author    = "N. Oostdijk and M. Reynaert and P. Monachesi and {van Noord}, G. and Ordelman, {Roeland J.F.} and I. Schuurman and V. Vandeghinste",  note      = "http://eprints.ewi.utwente.nl/15099 ; null ; Conference date: 28-05-2008 Through 30-05-2008",  year      = "2008",  month     = may,  day       = "31",  language  = "Undefined",  isbn      = "2-9517408-4-0",  publisher = "ELRA",  number    = "07-04",  pages     = "1437--1444",  booktitle = "Proceedings on the sixth international conference on language resources and evaluation (LREC 2008)",  url       = "http://www.lrec-conf.org/lrec2008/", }


@inproceedings{bd299a8ef8f44d259789dd5022748c8a,  title     = "The Lowlands team at TRECVID 2007",  abstract  = "In this report we summarize our methods and results for the search tasks in TRECVID 2007. We employ two different kinds of search: purely ASR based and purely concept based search. However, there is not significant difference of the performance of the two systems. Using neighboring shots for the combination of two concepts seems to be beneficial. General preprocessing of queries increased the performance and choosing detector sources helped. However, for all automatic search components we need to perform further investigations.",  keywords  = "EWI-12164, IR-64696, METIS-250924",  author    = "Robin Aly and C. Hauff and W.F.L. Heeren and Djoerd Hiemstra and {de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and T. Verschoor and {de Vries}, A.P.",  year      = "2008",  month     = feb,  language  = "Undefined",  isbn      = "not assigned",  publisher = "NIST",  number    = "69160R",  pages     = "--",  booktitle = "TREC Video Retrieval Evaluation Online Proceedings",  note      = "null ; Conference date: 05-11-2007 Through 09-11-2007", }


@inproceedings{0e96bb4997934dde9c5ef3ba9f8d68ec,  title     = "Browsing and Searching the Spoken Words of Buchenwald Survivors",  abstract  = "The {\textquoteleft}Buchenwald{\textquoteright} project is the successor of the {\textquoteleft}Radio Oranje{\textquoteright} project that aimed at the transformation of a set of World War II related mono-media documents –speeches of the Dutch Queen Wilhelmina, textual transcripts of the speeches, and a database of WWII related photographs– to an attractive online multimedia presentation of the Queen{\textquoteright}s speeches with keyword search functionality [6, 3]. The {\textquoteleft}Buchenwald{\textquoteright} project links up and extends the {\textquoteleft}Radio Oranje{\textquoteright} approach. The goal in the project was to develop a Dutch multimedia information portal on World War II concentration camp Buchenwald1. The portal holds both textual information sources and a video collection of testimonies from 38 Dutch camp survivors with durations between a half and two and a half hours. For each interview, an elaborate description, a speaker profile and a short summary are available.",  author    = "Roeland Ordelman and Willemijn Heeren and {van Hessen}, Arjan and Djoerd Hiemstra and Hendri Hondorp and Marijn Huijbregts and {de Jong}, Franciska and Thijs Verschoor",  year      = "2008",  language  = "English",  series    = "BNAIC: proceedings of the ... Belgium/Netherlands Artificial Intelligence Conference",  publisher = "University of Twente",  pages     = "403--404",  booktitle = "BNAIC 2008",  address   = "Netherlands",  note      = "20th Benelux Conference on Artificial Intelligence, BNAIC 2008, BNAIC ; Conference date: 30-10-2008 Through 31-10-2008", }


@inproceedings{aac12f38f45a44648bfa8d362e9929ad,  title     = "Evaluation of spoken document retrieval for historic speech collections",  abstract  = "The re-use of spoken word audio collections maintained by audiovisual archives is severely hindered by their generally limited access. The CHoral project, which is part of the CATCH program funded by the Dutch Research Council, aims to provide users of speech archives with online, instead of on-location, access to relevant fragments, instead of full documents. To meet this goal, a spoken document retrieval framework is being developed. In this paper the evaluation efforts undertaken so far to assess and improve various aspects of the framework are presented. These efforts include (i) evaluation of the automatically generated textual representations of the spoken word documents that enable word-based search, (ii) the development of measures to estimate the quality of the textual representations for use in information retrieval, and (iii) studies to establish the potential user groups of the to-be-developed technology, and the first versions of the user interface supporting online access to spoken word collections.",  keywords  = "METIS-251003, EWI-12872, IR-64808, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "W.F.L. Heeren and {de Jong}, {Franciska M.G.} and {van der Werff}, {Laurens Bastiaan} and M.A.H. Huijbregts and Ordelman, {Roeland J.F.}",  year      = "2008",  language  = "Undefined",  isbn      = "2-9517408-4-0",  publisher = "European Language Resources Association (ELRA)",  number    = "274",  pages     = "520",  booktitle = "Proceedings of LREC 2008",  note      = "null ; Conference date: 28-05-2008 Through 30-05-2008",  url       = "http://www.lrec-conf.org/lrec2008/", }


@inproceedings{a293634230d54cf39df58ecb174ae8fe,  title     = "Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition",  abstract  = "This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.",  keywords  = "HMI-SLT: Speech and Language Technology, HMI-MR: MULTIMEDIA RETRIEVAL, EC Grant Agreement nr.: FP6/027685, METIS-245906, EWI-11664, IR-62090, EC Grant Agreement nr.: FP6/027413",  author    = "M.A.H. Huijbregts and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  note      = "10.1007/978-3-540-77051-0_8 ; null ; Conference date: 05-12-2007 Through 07-12-2007",  year      = "2007",  month     = dec,  doi       = "10.1007/978-3-540-77051-0_8",  language  = "Undefined",  isbn      = "978-3-540-77033-6",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  number    = "07CH37910C",  pages     = "78--90",  booktitle = "Proceedings of the Second International Conference on Semantic and Digital Media Technologies, SAMT 2007",  address   = "Netherlands", }


@article{8d41512fefc54e27b73145ad2beb704d,  title     = "Searching Spontaneous Conversational Speech",  abstract  = "The ACM SIGIR Workshop on Searching Spontaneous Conversational Speech was held as part of the 2007 ACM SIGIR Conference in Amsterdam. The workshop program was a mix of elements, including a keynote speech, paper presentations and panel discussions. This brief report describes the organization of this workshop and summarizes the discussions.",  keywords  = "EWI-11498, IR-64502, METIS-245824",  author    = "{de Jong}, {Franciska M.G.} and Oard, {Douglas W.} and Ordelman, {Roeland J.F.} and Stephan Raaijmakers",  year      = "2007",  month     = dec,  language  = "Undefined",  volume    = "41",  pages     = "104--108",  journal   = "SIGIR forum",  issn      = "0163-5840",  publisher = "Association for Computing Machinery (ACM)",  number    = "Supplement/2", }


@inproceedings{ace7ad79ef80411e96f413cb38ef515b,  title     = "XML Information Retrieval from Spoken Word Archives",  abstract  = "This report presents the University of Twente's first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English cross-language spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.",  keywords  = "EWI-10989, IR-61900, METIS-247030",  author    = "Robin Aly and Djoerd Hiemstra and Ordelman, {Roeland J.F.} and {van der Werff}, {Laurens Bastiaan} and {de Jong}, {Franciska M.G.}",  note      = "7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006, Alicante, Spain, September 20-22, 2006, Revised Selected Papers; null ; Conference date: 20-09-2006 Through 22-09-2006",  year      = "2007",  month     = sep,  doi       = "10.1007/978-3-540-74999-8_97",  language  = "Undefined",  isbn      = "978-3-540-74998-1",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  pages     = "770--777",  booktitle = "Evaluation of Multilingual and Multi-modal Information Retrieval",  address   = "Netherlands", }


@inproceedings{65f231a8c4f44f2da3f1955fc77c555d,  title     = "Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections",  abstract  = "In this paper we discuss the speech activity detection system that we used for detecting speech regions in the Dutch TRECVID video collection. The system is designed to filter non-speech like music or sound effects out of the signal without the use of predefined non-speech models. Because the system trains its models on-line, it is robust for handling out-of-domain data. The speech activity error rate on an out-of-domain test set, recordings of English conference meetings, was 4.4%. The overall error rate on twelve randomly selected five minute TRECVID fragments was 11.5%.",  keywords  = "IR-64329, Speech activity detection, EC Grant Agreement nr.: FP6/027685, METIS-241881, EC Grant Agreement nr.: FP6/027413, EWI-11003, EC Grant Agreement nr.: FP6/506811",  author    = "M.A.H. Huijbregts and Chuck Wooters and Ordelman, {Roeland J.F.}",  year      = "2007",  month     = aug,  day       = "27",  language  = "English",  isbn      = "1990-9772",  publisher = "International Speech Communication Association (ISCA)",  number    = "LNCS4549",  pages     = "FrC.P3--4",  booktitle = "Proceedings of Interspeech 2007",  note      = "8th Annual Conference of the International Speech Communication Association, INTERSPEECH 2007, INTERSPEECH ; Conference date: 27-08-2007 Through 31-08-2007",  url       = "https://www.interspeech2007.org/", }


@inproceedings{582858066bfc4623baa68b5971305a4b,  title     = "Building Detectors to Support Searches on Combined Semantic Concepts",  abstract  = "Bridging the semantic gap is one of the big challenges in multimedia information retrieval. It exists between the extraction of low-level features of a video and its conceptual contents. In order to understand the conceptual content of a video a common approach is building concept detectors. A problem of this approach is that the number of detectors is impossible to determine. This paper presents a set of 8 methods on how to combine two existing concepts into a new one, which occurs when both concepts appear at the same time. The scores for each shot of a video for the combined concept are computed from the output of the underlying detectors. The findings are evaluated on basis of the output of the 101 detectors including a comparison to the theoretical possibility to train a classifier on each combined concept. The precision gains are significant, specially for methods which also consider the chronological surrounding of a shot promising.",  keywords  = "EWI-10923, IR-64301, METIS-241850, semantic concept, concept combination",  author    = "Robin Aly and Djoerd Hiemstra and Ordelman, {Roeland J.F.}",  year      = "2007",  month     = aug,  language  = "Undefined",  isbn      = "not assigned",  publisher = "Yahoo! Research",  number    = "LNCS4549",  pages     = "40--45",  booktitle = "Proceedings of the Multimedia Information Retrieval Workshop",  note      = "null ; Conference date: 27-08-2007 Through 27-08-2007", }


@book{d70f9c4cc3ec4389b680a215bbe26368,  title     = "Proceedings of the ACM SIGIR Workshop ''Searching Spontaneous Conversational Speech''",  abstract  = "The Proceedings contain the contributions to the workshop on Searching Spontaneous Conversational Speech organized in conjunction with the 30th ACM SIGIR, Amsterdam 2007. The papers reflect some of the emerging focus areas and cross-cutting research topics, together addressing evaluation metrics, segmentation methods, workflow aspects, rich transcription, and robustness.",  keywords  = "HMI-SLT: Speech and Language Technology, EWI-11332, IR-64441, METIS-242031",  author    = "Stephan Raaijmakers",  editor    = "{de Jong}, {Franciska M.G.} and Douglas Oard and Ordelman, {Roeland J.F.}",  note      = "A report on the workshop will be published in ACM SIGIR Forum. Draft available via http://hmi.ewi.utwente.nl/spraakgroep/documents/sscs2007/sscsreport.pdf",  year      = "2007",  month     = jul,  day       = "27",  language  = "Undefined",  isbn      = "978-90-365-2542-8",  publisher = "Centre for Telematics and Information Technology (CTIT)",  number    = "LNCS4549/63",  address   = "Netherlands", }


@inproceedings{9a5c4956d83a42a4bc3fe916ab36ad52,  title     = "Radio Oranje: Searching the Queen's speech(es)",  abstract  = "The `Radio Oranje' demonstrator shows an attractive multimedia user experience in the cultural heritage domain based on a collection of mono-media audio documents. It supports online search and browsing of the collection using indexing techniques, specialized content visualizations and a related photo database.",  keywords  = "EWI-10825, IR-61872, Multimedia Retrieval, METIS-241815, Information Visualization, Spoken Document Retrieval, Graphical User Interfaces (GUI)",  author    = "W.F.L. Heeren and {van der Werff}, {Laurens Bastiaan} and Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.}",  note      = "10.1145/1277741.1277971 ; null ; Conference date: 23-07-2007 Through 27-07-2007",  year      = "2007",  month     = jul,  doi       = "10.1145/1277741.1277971",  language  = "Undefined",  isbn      = "978-1-59593-597-7",  publisher = "Association for Computing Machinery (ACM)",  number    = "LNCS4549",  pages     = "903",  editor    = "C.L.A. Clarke and N. Fuhr and N. Kando and W. Kraaij and {de Vries}, A.",  booktitle = "Proceedings of the 30th ACM SIGIR",  address   = "United States", }


@book{7b27046ff9254ab38d6cf614d36c145b,  title     = "Speech-based Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition",  abstract  = "This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.",  keywords  = "HMI-SLT: Speech and Language Technology, EWI-9783, Information Retrieval, Automatic Speech Recognition, IR-95701, METIS-241618, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "M.A.H. Huijbregts and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  note      = "http://eprints.ewi.utwente.nl/9783 ",  year      = "2007",  month     = may,  day       = "9",  language  = "Undefined",  series    = "CTIT Technical Report Series",  publisher = "Centre for Telematics and Information Technology (CTIT)",  number    = "WP07-01/TR-CTIT-07-30",  address   = "Netherlands", }


@inproceedings{4482823629124484af5f285b702aaaf8,  title     = "Radio Oranje: Enhanced Access to a Historical Spoken Word Collection",  abstract  = "Access to historical audio collections is typically very restricted: content is often only available on physical (analog) media and the metadata is usually limited to keywords, giving access at the level of relatively large fragments, e.g., an entire tape. Many spoken word heritage collections are now being digitized, which allows the introduction of more advanced search technology. This paper presents an approach that supports online access and search for recordings of historical speeches. A demonstrator has been built, based on the so-called Radio Oranje collection, which contains radio speeches by the Dutch Queen Wilhelmina that were broadcast during World War II. The audio has been aligned with its original 1940s manual transcriptions to create a time-stamped index that enables the speeches to be searched at the word level. Results are presented together with related photos from an external database.",  keywords  = "HMI-SLT: Speech and Language Technology, HMI-MR: MULTIMEDIA RETRIEVAL",  author    = "{van der Werff}, {Laurens Bastiaan} and W.F.L. Heeren and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  year      = "2007",  month     = jan,  day       = "12",  language  = "English",  isbn      = "978-90-78328-41-4",  series    = "LOT Occasional Series",  publisher = "Landelijke Onderzoekschool Taalwetenschap",  number    = "7",  pages     = "207--218",  editor    = "Peter Dirx and Ineke Schuurman and Vincent Vandeghinste and {van Eynde}, Frank",  booktitle = "Computational Linguistics in the Netherlands",  note      = "17th Meeting of Computational Linguistics in the Netherlands, CLIN 2006, CLIN ; Conference date: 12-01-2007 Through 12-01-2007", }


@inbook{6621d99c26af419e8741fc1b079d5357,  title     = "Speech Indexing",  abstract  = "This chapter will focus on the automatic extraction of information from the speech in multimedia documents. This approach is often referred to as speech indexing and it can be regarded as a subfield of audio indexing that also incorporates for example the analysis of music and sounds. If the objective of the recognition of the words spoken is to support retrieval, one commonly speaks of spoken document retrieval (SDR). If the objective is on the coupling of various media types the term media mining or even cross-media mining is used. Most attention in this chapter will go to SDR. The focus is less on searching (an index of ) a multimedia database, but on enabling multiple views on the data by cross-linking all the available multifaceted information sources in a multimedia database. In section 1.6 cross-media mining will be discussed in more detail.",  keywords  = "EWI-11008, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, IR-61901, Audio search, Speech Indexing, Speech Recognition, Spoken Document Retrieval, METIS-241883",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and {van Leeuwen}, D.A.",  note      = "http://eprints.ewi.utwente.nl/11008 ",  year      = "2007",  doi       = "10.1007/978-3-540-72895-5_7",  language  = "Undefined",  isbn      = "978-3-540-72894-8",  series    = "Data-Centric Systems and Applications",  publisher = "Springer",  number    = "LNCS4549",  pages     = "199--224",  editor    = "Henk Blanken and {de Vries}, A.P. and H.E. Blok and L. Feng",  booktitle = "Multimedia Retrieval",  address   = "Netherlands", }


@article{42e3c5016cab421281a9029a774fffae,  title    = "TwNC: a Multifaceted Dutch News Corpus",  abstract = "This contribution describes the Twente News Corpus (TwNC), a multifaceted corpus for Dutch that is being deployed in a number of NLP research projects among which tracks within the Dutch national research programme MultimediaN, the NWO programme CATCH, and the Dutch-Flemish programme STEVIN. The development of the corpus started in 1998 within a predecessor project DRUID and has currently a size of 530M words. The text part has been built from texts of four different sources: Dutch national newspapers, television subtitles, teleprompter (auto-cues) files, and both manually and automatically generated broadcast news transcripts along with the broadcast news audio. TwNC plays a crucial role in the development and evaluation of a wide range of tools and applications for the domain of multimedia indexing, such as large vocabulary speech recognition, cross-media indexing, cross-language information retrieval etc. Part of the corpus was fed into the Dutch written text corpus in the context of the Dutch-Belgian STEVIN project D-COI that was completed in 2007. The sections below will describe the rationale that was the starting point for the corpus development; it will outline the cross-media linking approach adopted within MultimediaN, and finally provide some facts and figures about the corpus.",  keywords = "HMI-MR: MULTIMEDIA RETRIEVAL, EWI-15098, IR-68090, Speech Recognition, Text corpora, Multimedia Retrieval",  author   = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and {van Hessen}, {Adrianus J.} and G.H.W. Hondorp",  year     = "2007",  language = "English",  volume   = "12",  journal  = "ELRA Newsletter",  number   = "3-4", }


@inproceedings{413754af753940d0aef4cc52b9361ba0,  title     = "Automated speech and audio analysis for semantic access to multimedia",  abstract  = "The deployment and integration of audio processing tools can enhance the semantic annotation of multimedia content, and as a consequence, improve the effectiveness of conceptual access tools. This paper overviews the various ways in which automatic speech and audio analysis can contribute to increased granularity of automatically extracted metadata. A number of techniques will be presented, including the alignment of speech and text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value of the techniques and their potential contribution to the content value chain will be illustrated by the description of two (complementary) demonstrators for browsing broadcast news archives.",  keywords  = "HMI-SLT: Speech and Language Technology, HMI-MR: MULTIMEDIA RETRIEVAL, EC Grant Agreement nr.: FP6/506811, IR-66586, EWI-8073, EC Grant Agreement nr.: FP6/027413, METIS-237582, EC Grant Agreement nr.: FP6/027685",  author    = "{de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and M.A.H. Huijbregts",  note      = "10.1007/11930334_18 ; null ; Conference date: 06-12-2006",  year      = "2006",  month     = dec,  day       = "6",  doi       = "10.1007/11930334_18",  language  = "Undefined",  isbn      = "3-540-49335-2",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  number    = "06EX1521",  pages     = "226--240",  editor    = "Y. Avrithis and Y. Kompatsiaris and S. Staab and {O' Connor}, N.E.",  booktitle = "Proceedings of the First International Conference on Semantic and Digital Media Technologies, SAMT 2006",  address   = "Netherlands", }


@inproceedings{7a3a0a34aa5c43c7a3c04d40d551e411,  title     = "Audio Indexing Technology for the Exploration of Audiovisual Heritage Collections",  keywords  = "EWI-8244, IR-63710, METIS-237644",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and W.F.L. Heeren and {van Hessen}, {Adrianus J.}",  year      = "2006",  month     = oct,  day       = "5",  language  = "English",  series    = "Belgium-Dutch Artificial Intelligence Conference",  publisher = "Namur University Press",  pages     = "413--414",  editor    = "Pierre-Yves Schobbens and Wim Vanhoof and Gabriel Schwanen",  booktitle = "BNAIC{\textquoteright}06",  note      = "18th Belgium-Dutch Conference on Artificial Intelligence, BNAIC 2006, BNAIC ; Conference date: 05-10-2006 Through 06-10-2006",  url       = "https://staff.info.unamur.be/wva/BNAIC06/index.php", }


@inproceedings{f0ce056c2a4a4735968a2dc271eaaafc,  title     = "The role of automated speech and audio analysis in semantic multimedia annotation",  abstract  = "This paper overviews the various ways in which automatic speech and audio analysis can be deployed to enhance the semantic annotation of multimedia content, and as a consequence to improve the effectiveness of conceptual access tools. A number of techniques will be presented, including the alignment of text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value will be illustrated by the description of two complementary demonstrators for browsing broadcast news archieves.",  keywords  = "IR-66682, EC Grant Agreement nr.: FP6/506811, METIS-237667, EWI-8337, EC Grant Agreement nr.: FP6/027685",  author    = "{de Jong}, {Franciska M.G.} and Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.}",  year      = "2006",  month     = sep,  language  = "Undefined",  isbn      = "0-86341-671-3",  publisher = "The Institute of Engineering and Technology, London",  number    = "10",  pages     = "6",  booktitle = "International IET Conference on Visual Information Engineering (VIE 2006)",  note      = "null ; Conference date: 01-09-2006", }


@inproceedings{5777ddfc14e54f7c8fd1b3f787fac076,  title     = "Exploration of audiovisual heritage using audio indexing technology",  abstract  = "This paper discusses audio indexing tools that have been implemented for the disclosure of Dutch audiovisual cultural heritage collections. It explains the role of language models and their adaptation to historical settings and the adaptation of acoustic models for homogeneous audio collections. In addition to the benefits of cross-media linking, the requirements for successful tuning and improvement of available tools for indexing the heterogeneous A/V collections from the cultural heritage domain are reviewed. And finally the paper argues that research is needed to cope with the varying information needs for different types of users.",  keywords  = "IR-66587, METIS-237716, EWI-8424",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and W.F.L. Heeren",  note      = "First European Workshop on Intelligent Technologies for Cultural Heritage Exploitation was held at the 17th European Conference on Artificial Intelligence, Riva del Garda, Italy, August 28, 2006 [Proceedings on CD-ROM only] ; null ; Conference date: 28-08-2006 Through 28-08-2006",  year      = "2006",  month     = aug,  day       = "28",  language  = "Undefined",  isbn      = "not assigned",  publisher = "Universit{\`a} di Trento",  number    = "TuC2",  pages     = "36--39",  editor    = "L Bordoni and A. Kr{\"u}ger and M Zancanaro",  booktitle = "Proceedings of the First European Workshop on Intelligent Technologies for Cultural Heritage Exploitation", }


@article{39bb0a19dece49d19a6749106e64e487,  title     = "Generating Expressive Speech for Storytelling Applications",  abstract  = "Work on expressive speech synthesis has long focused on the expression of basic emotions. In recent years, however, interest in other expressive styles has been increasing. The research presented in this paper aims at the generation of a storytelling speaking style, which is suitable for storytelling applications and more in general, for applications aimed at children. Based on an analysis of human storytellers' speech, we designed and implemented a set of prosodic rules for converting {"}neutral{"} speech, as produced by a text-to-speech system, into storytelling speech. An evaluation of our storytelling speech generation system showed encouraging results.",  keywords  = "Speech analysis, Child-directed speech, Speech synthesis, Expressive prosody, Expressive speech",  author    = "Mariet Theune and Koen Meijs and Dirk Heylen and Roeland Ordelman",  year      = "2006",  month     = jul,  doi       = "10.1109/TASL.2006.876129",  language  = "English",  volume    = "14",  pages     = "1137--1144",  journal   = "IEEE transactions on audio, speech and language processing",  issn      = "1558-7916",  publisher = "IEEE",  number    = "4", }


@inproceedings{daf38fa1ccb44959a3f241f6694c452b,  title     = "Annotating Emotions in Meetings",  abstract  = "We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.",  keywords  = "EWI-8381, METIS-237690, EC Grant Agreement nr.: FP6/506811, IR-63746",  author    = "Dennis Reidsma and Heylen, {Dirk K.J.} and Ordelman, {Roeland J.F.}",  year      = "2006",  month     = may,  language  = "Undefined",  isbn      = "2-9517408-2-4",  publisher = "ELRA",  number    = "10",  pages     = "1117--1122",  booktitle = "Proc. of the fifth international conference on Language Resources and Evaluation, LREC 2006",  note      = "null ; Conference date: 22-05-2006 Through 28-05-2006",  url       = "http://www.lrec-conf.org/lrec2006/", }


@inproceedings{6086ebec963044a0bec1bbb4e24af255,  title     = "Annotating State of Mind in Meeting Data",  abstract  = "We discuss the annotation procedure for mental state and emotion that is under development for the AMI (Augmented Multiparty Interaction) corpus. The categories that were found to be most appropriate relate not only to emotions but also to (meta-)cognitive states and interpersonal variables. The history of the development of the annotation scheme is briefly described. The discussion centers around the presentation of the procedure.",  keywords  = "EC Grant Agreement nr.: FP6/506811, IR-63742, METIS-237686, EWI-8376",  author    = "Heylen, {Dirk K.J.} and Dennis Reidsma and Ordelman, {Roeland J.F.}",  year      = "2006",  month     = may,  language  = "Undefined",  isbn      = "not assigned",  publisher = "ELRA",  number    = "10",  pages     = "84--87",  editor    = "L. Devillers and J-C. Martin and R. Cowie and A. Batliner",  booktitle = "Proc. of the LREC2006 Workshop on Corpora for Research on Emotion and Affect",  note      = "null ; Conference date: 22-05-2006 Through 28-05-2006", }


@article{cec0ab534c4d472d870f14f1ce7d0682,  title     = "De stem van Willem Frederik Hermans ontrafeld. Audiovisuele archieven ge{\"i}ndexeerd",  keywords  = "IR-65020, EWI-13538",  author    = "Ordelman, {Roeland J.F.}",  year      = "2005",  month     = oct,  day       = "3",  language  = "Undefined",  volume    = "3e jaargang",  pages     = "15--17",  journal   = "Dixit",  issn      = "1572-6037",  publisher = "Dialoog",  number    = "3", }


@conference{e0cf3be5bfae4e90842802ac3b495b93,  title    = "Affect in Meeting Interaction",  keywords = "METIS-227403",  author   = "Dennis Reidsma and Heylen, {Dirk K.J.} and Ordelman, {Roeland J.F.}",  note     = "Reidsma509:2005 Type=Poster at MLMI 2005, invited=N, keynote=N, location={Edinburgh, Schotland} ; null ; Conference date: 12-07-2005",  year     = "2005",  month    = jul,  day      = "12",  language = "Undefined",  pages    = "--", }


@inproceedings{4354bc3aafdb4eaaae5b3f0aa2669ff3,  title     = "A Spoken Document Retrieval Application in the Oral History Domain",  abstract  = "The application of automatic speech recognition in the broadcast news domain is well studied. Recognition performance is generally high and accordingly, spoken document retrieval can successfully be applied in this domain, as demonstrated by a number of commercial systems. In other domains, a similar recognition performance is hard to obtain, or even far out of reach, for example due to lack of suitable training material. This is a serious impediment for the successful application of spoken document retrieval techniques for other data then news. This paper outlines our first steps towards a retrieval system that can automatically be adapted to new domains. We discuss our experience with a recently implemented spoken document retrieval application attached to a web-portal that aims at the disclosure of a multimedia data collection in the oral history domain. The paper illustrates that simply deploying an off-theshelf broadcast news system in this task domain will produce error rates that are too high to be useful for retrieval tasks. By applying adaptation techniques on the acoustic level and language model level, system performance can be improved considerably, but additional research on unsupervised adaptation and search interfaces is required to create an adequate search environment based on speech transcripts.",  keywords  = "EWI-1836, METIS-227318, IR-65566",  author    = "M.A.H. Huijbregts and Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.}",  note      = "Imported from HMI; null ; Conference date: 01-01-2005 Through 01-01-2005",  year      = "2005",  language  = "Undefined",  isbn      = "5-7452-0110-x",  series    = "2",  publisher = "University of Patras/ WCL Moscow State Linguistics Uni.",  pages     = "699--702",  booktitle = "Proceedings of 10th international conference Speech and Computer, Patras, Greece (SPECOM 2005)", }


@inproceedings{5268581029e446f3920af79b32d0ddc0,  title     = "InfoLink: analysis of Dutch broadcast news and cross-media browsing",  abstract  = "In this paper, a cross-media browsing demonstrator named InfoLink is described. InfoLink automatically links the content of Dutch broadcast news videos to related information sources in parallel collections containing text and/or video. Automatic segmentation, speech recognition and available meta-data are used to index and link items. The concept is visualised using SMIL-scripts for presenting the streaming broadcast news video and the information links.",  author    = "Jeroen Morang and Roeland Ordelman and {de Jong}, Franciska and {van Hessen}, Arjan",  note      = "Imported from HMI; IEEE International Conference on Multimedia and Expo, ICME 2005 ; Conference date: 06-07-2005 Through 06-07-2005",  year      = "2005",  doi       = "10.1109/ICME.2005.1521738",  language  = "English",  isbn      = "0-7803-9331-7",  pages     = "1582--1585",  booktitle = "Proceedings of IEEE International Conference on Multimedia and Expo (ICME 2005)",  publisher = "IEEE",  address   = "United States", }


@inproceedings{dd6411785af240c2b197fef1147c0a1e,  title     = "Robust Audio Indexing for Dutch Spoken-word Collections",  abstract  = "Abstract—Whereas the growth of storage capacity is in accordance with widely acknowledged predictions, the possibilities to index and access the archives created is lagging behind. This is especially the case in the oral history domain and much of the rich content in these collections runs the risk to remain inaccessible for lack of robust search technologies. This paper addresses the history and development of robust audio indexing technology for searching Dutch spoken-word collections and compares Dutch audio indexing in the well-studied broadcast news domain with an oral-history case-study. It is concluded that despite significant advances in Dutch audio indexing technology and demonstrated applicability in several domains, further research is indispensable for successful automatic disclosure of spoken-word collections.",  keywords  = "EWI-1834, METIS-227324, IR-65565",  author    = "Ordelman, {Roeland J.F.} and {de Jong}, {Franciska M.G.} and M.A.H. Huijbregts and {van Leeuwen}, David",  note      = "Imported from HMI; null ; Conference date: 14-09-2005 Through 17-09-2005",  year      = "2005",  language  = "Undefined",  isbn      = "90-6984-456-7",  publisher = "Koninklijke Nederlandse Academie van Wetenschappen",  pages     = "215--223",  booktitle = "Proceedings of the XVIth International Conference of the Association for History and Computing (AHC2005)", }


@inproceedings{f06315d162444519b96c4dde9b96d259,  title     = "The 2005 AMI System for the Transcription of Speech in Meetings",  abstract  = "In this paper we describe the 2005 AMI system for the transcription of speech in meetings used for participation in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve very competitive performance.",  keywords  = "IR-65564, METIS-227319, EC Grant Agreement nr.: FP6/506811, EWI-1829",  author    = "Thomas Hain and Lukas Burget and John Dines and Giulia Gaurau and Martin Karafiat and Mike Lincoln and Iain McCowan and Ordelman, {Roeland J.F.} and Darren Moore and Vincent Wan and Steve Renals",  note      = "Imported from HMI; null ; Conference date: 11-07-2005 Through 13-07-2005",  year      = "2005",  doi       = "10.1007/11677482_38",  language  = "Undefined",  isbn      = "978-3-540-32549-9",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  number    = "3869",  pages     = "450--462",  booktitle = "2nd International Workshop on Machine Learning for Multimodal Interaction, MLMI 2005",  address   = "Netherlands", }


@inproceedings{44c0b02554d44b6b9acc39f05e9d29f6,  title     = "The Development of the AMI System for the Transcription of Speech in Meetings",  abstract  = "The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. This paper describes the development of a baseline automatic speech transcription system for meetings in the context of the AMI (Augmented Multiparty Interaction) project. We present several techniques important to processing of this data and show the performance in terms of word error rates (WERs). An important aspect of transcription of this data is the necessary flexibility in terms of audio pre-processing. Real world systems have to deal with flexible input, for example by using microphone arrays or randomly placed microphones in a room. Automatic segmentation and microphone array processing techniques are described and the effect on WERs is discussed. The system and its components presented in this paper yield competitive performance and form a baseline for future research in this domain.",  keywords  = "EWI-1830, METIS-227320, IR-89653",  author    = "Thomas Hain and Lukas Burget and John Dines and Iain McCowan and Giulia Garau and Martin Karafiat and Mike Lincoln and Ordelman, {Roeland J.F.} and Darren Moore and Vincent Wan and Steve Renals",  note      = "Imported from HMI; null ; Conference date: 11-07-2005 Through 13-07-2005",  year      = "2005",  doi       = "10.1007/11677482_30",  language  = "Undefined",  isbn      = "978-3-540-32549-9",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  number    = "3869",  pages     = "344--356",  editor    = "Steve Renals and Samy Bengio",  booktitle = "Proceedings 2nd Workshop on Multimodal Interaction and Related Machine Learning Algorithms",  address   = "Netherlands", }


@inproceedings{e7aae5e94aa74c4c858ca739a0ea027c,  title     = "Transcription of Conference Room Meetings: an Investigation",  abstract  = "The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. In this paper we explore the use of various meeting corpora for the purpose of automatic speech recognition. In particular we investigate the similarity of these resources and how to efficiently use them in the construction of a meeting transcription system. The analysis shows distinctive features for each resource. However the benefit in pooling data and hence the similarity seems sufficient to speak of a generic ‿conference meeting domain‿. In this context this paper also presents work on development for the AMI meeting transcription system, a joint effort by seven sites working on the AMI (augmented multi-party interaction) project.",  keywords  = "EWI-1831, METIS-227321, IR-89652",  author    = "Thomas Hain and John Dines and Giulia Garau and Martin Karafiat and Darren Moore and Vincent Wan and Ordelman, {Roeland J.F.} and Steve Renals",  note      = "Imported from HMI; 9th European Conference on Speech Communication and Technology, INTERSPEECH 2005 - EUROSPEECH, INTERSPEECH ; Conference date: 04-09-2005 Through 08-09-2005",  year      = "2005",  language  = "English",  isbn      = "not assigned",  pages     = "1661--1664",  booktitle = "Proceedings of Interspeech 2005",  publisher = "International Speech Communication Association (ISCA)", }


@inproceedings{9df63b88cee048bab456c505cbffd9e8,  title     = "Speech-based information retrieval for Dutch.",  keywords  = "METIS-217550",  author    = "Ordelman, {Roeland J.F.}",  year      = "2003",  month     = dec,  day       = "8",  language  = "Undefined",  pages     = "--",  booktitle = "DIR2003",  note      = "null ; Conference date: 08-12-2003 Through 09-12-2003", }


@phdthesis{1164b9ce6dd546118d402c25af8546fd,  title     = "Dutch Speech Recognition in Multimedia Information Retrieval",  abstract  = "As data storage capacities grow to nearly unlimited sizes thanks to ever ongoing hardware and software improvements, an increasing amount of information is being stored in multimedia and spoken-word collections. Assuming that the intention of data storage is to use (portions of) it some later time, these collections must also be searchable in one way or another. For multimedia and spoken-word collections, traditional text-oriented information retrieval (IR) strategies inevitably fall short, as the amount of textual information included with these types of documents is usually very limited. However, when automatic speech recognition (ASR) can be used to convert the speech occurring in these documents into text, textual representations can be created that in turn can be searched using the traditional text-based search strategies. As ASR systems label recognized words with exact time information as a standard accessory, detailed searching within multimedia and spoken-word collections can be enabled. This type of retrieval is commonly referred to as Spoken Document Retrieval (SDR). Typically, large vocabulary speaker independent continuous speech recognition systems (LVCSR) are deployed for creating textual representations of the spoken audio in multimedia an spoken-word collections. For Dutch however, such a system was not available when this research was started. As creating a Dutch system from scratch was not feasible given the available resources, an existing English system, refered to as the ABBOT system, was ported to Dutch. A significant part of this thesis is dedicated to a complete run-down of the porting work, involving the collection and preparation of suitable training data and the actual training and evaluation of the acoustic models and language models. The broadcast news domain was chosen as domain of focus, as this domain has also been extensively used as a benchmark domain for both international ASR research and SDR. A complicating factor for ASR in the news domain, is that word usage is highly variable. As a consequence, besides using large vocabularies, it is important to adjust these vocabularies regularly, so that they reflect the content of the news programs well. Therefore, it has been investigated which word selection strategies are best suited for making these vocabulary adjustments. Moreover, as dynamic vocabularies require a flexible generation of accurate word pronunciations, the development of a grapheme-to-phoneme converter is addressed. Another vocabulary related issue that is investigated, stems from a well-known characteristic of the Dutch language, word compounding: Dutch words can almost freely be joined together to form new words. As a result of this phenomenon, the number of distinct words in Dutch is relatively large, which reduces the coverage of vocabularies compared to those of the same size of other languages, such as English, that do not have word compounding. This thesis investigates whether splitting Dutch compound words could be a remedy for the relatively limited coverage of vocabularies, so that ASR performance could be improved. Next to a brief history of SDR research and a review of possible SDR approaches, this thesis demonstrates the use of a Dutch LVCSR in SDR by providing an illustrative example of an SDR evaluation given a collection of Dutch broadcast news shows. It is shown that Dutch speech recognition can successfully be deployed for content-based retrieval of broadcast news programs. The experience obtained with the research described in this thesis, and the experience that will emerge from future research efforts must contribute to the long-term accessibility of the increasing amount of information being stored in Dutch multimedia and spoken-word collections.",  keywords  = "HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, Speech recognition, Multimedia retrieval",  author    = "Ordelman, {Roeland Jacobus Frederik}",  year      = "2003",  month     = oct,  day       = "10",  doi       = "10.3990/1.9789075296082",  language  = "English",  isbn      = "90-75296-08-8",  series    = "CTIT PhD-thesis series",  publisher = "Twente University Press (TUP)",  address   = "Netherlands",  school    = "University of Twente", }


@inproceedings{4eedbfe2492e470daa926bec23d5c738,  title     = "Compound Decomposition in Dutch Large Vocabulary Speech Recognition",  abstract  = "This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of out-of- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.",  keywords  = "Spoken Document Retrieval, Audio search, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, METIS-217551, IR-63377, EWI-6705",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.}",  note      = "Imported from HMI; null ; Conference date: 01-09-2003 Through 04-09-2003",  year      = "2003",  language  = "Undefined",  publisher = "ISCA",  pages     = "--",  booktitle = "Eurospeech 2003", }


@article{3ddf6a19e76a494fb46720224655d72a,  title     = "Searching multimedia content using the spoken audio.",  keywords  = "METIS-217567",  author    = "Ordelman, {Roeland J.F.}",  year      = "2003",  language  = "English",  volume    = "20",  pages     = "111--111",  journal   = "BNVKI newsletter",  issn      = "1566-8266",  publisher = "Belgisch Nederlandse Vereniging voor Kunstmatige Intelligentie",  number    = "5", }


@inproceedings{eeb646a4d3e54c35adb64725fd2ebe23,  title     = "Speech Recognition for Dutch Spoken Document Retrieval",  keywords  = "METIS-202740",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.} and {van Leeuwen}, D.A.",  year      = "2001",  month     = sep,  day       = "18",  language  = "Undefined",  pages     = "143--150",  editor    = "R. Leonardi",  booktitle = "Proceedings of CBMI'01: Content-based Multimedia Indexing",  note      = "null ; Conference date: 19-09-2001 Through 21-09-2001", }


@book{a11ea1d2c1954ecdbe10d7f590623609,  title     = "Lexicon Optimization for Dutch Speech Recognition in Spoken Document Retrieval",  abstract  = "In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: the collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.}",  note      = "Imported from CTIT",  year      = "2001",  month     = jun,  language  = "English",  series    = "CTIT technical report series",  publisher = "Centre for Telematics and Information Technology (CTIT)",  address   = "Netherlands", }


@inproceedings{8ead7d5ecc6d4d4abbbd2f5313e03b14,  title     = "Lexicon optimization for Dutch speech recognition in spoken document retrieval",  abstract  = "In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: the collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.}",  year      = "2001",  language  = "English",  isbn      = "87-90834-09-7",  pages     = "1085--1088",  editor    = "P. Dalsgaard and B. Lindberg and H. Benner",  booktitle = "Proceedings of Eurospeech 2001 - Scandinavia",  note      = "Conferentie in Aalborg, Denmark : Proceedings of Eurospeech 2001 - Scandinavia ; Conference date: 01-01-1900", }


@inbook{bb111204457740feb413aefe852d1c98,  title     = "Speech Recognition Issues for Dutch Spoken Document Retrieval",  abstract  = "In this paper, ongoing work on the development of the speech recognition modules of a multimedia retrieval environment for Dutch is described. The work on the generation of acoustic models and language models along with their current performance is presented. Some characteristics of the Dutch language and of the target video archives that require special treatment are discussed.",  keywords  = "Audio search, IR-63372, EWI-6683, HMI-MR: MULTIMEDIA RETRIEVAL, Spoken Document Retrieval, HMI-SLT: Speech and Language Technology, METIS-205756, Speech Recognition",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {de Jong}, {Franciska M.G.}",  note      = "Imported from HMI; null ; Conference date: 10-09-2001 Through 13-09-2001",  year      = "2001",  doi       = "10.1007/3-540-44805-5_34",  language  = "Undefined",  isbn      = "978-3-540-42557-1",  series    = "Lecture Notes in Computer Science",  publisher = "Springer",  number    = "2166",  pages     = "258--266",  editor    = "Vaclav Matousek and Pavel Mautner and Roman Moucek and Karel Tauser",  booktitle = "Proceedings of 4th International Conference on Text Speech and Dialogue",  address   = "Netherlands", }


@article{9b11f4fca5a143b3957b78ba9ba49304,  title     = "Zoeken in historisch videomateriaal",  abstract  = "On attaching automatic search functionality to historical video archives",  keywords  = "IR-63342, Audio search, Cultural Heritage, METIS-121914, HMI-MR: MULTIMEDIA RETRIEVAL, HMI-SLT: Speech and Language Technology, EWI-6597",  author    = "Ordelman, {Roeland J.F.}",  note      = "Imported from HMI",  year      = "2000",  language  = "Undefined",  volume    = "4",  pages     = "24--28",  journal   = "Informatie professional",  issn      = "1385-5328",  publisher = "Otto Cramwinckel Uitgever",  number    = "12", }


@inproceedings{256a8871047f486781e1cc8e82b22ec9,  title     = "Dealing with Phrase Level Co-Articulation (PLC) in speech recognition: a first approach",  abstract  = "Whereas nowadays within-word co-articulation effects are usually sufficiently dealt with in automatic speech recognition, this is not always the case with phrase level co-articulation effects (PLC). This paper describes a first approach in dealing with phrase level co-articulation by applying these rules on the reference transcripts used for training our recogniser and by adding a set of temporary PLC phones that later on will be mapped on the original phones. In fact we temporarily break down acoustic context into a general and a PLC context. With this method, more robust models could be trained because phones that are confused due to PLC effects like for example /v/-/f/ and /z/-/s/, receive their own models. A first attempt to apply this method is described.",  keywords  = "METIS-119585, IR-89651",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {van Leeuwen}, {David A.}",  year      = "1999",  month     = feb,  day       = "19",  language  = "English",  isbn      = "0-903428-09-1",  publisher = "ESCA",  pages     = "64--68",  editor    = "Tony Robinson and Steve Renals",  booktitle = "Proceedings of the ESCA ETRW Workshop Accessing Information in Spoken Audio",  note      = "ESCA ETRW Workshop Accessing Information in Spoken Audio : Proceedings of the ESCA ETRW Workshop Accessing Information in Spoken Audio ; Conference date: 19-02-1999 Through 20-04-1999", }


@inproceedings{99a7c06dba7b419d821774495783e9b7,  title     = "Improving Recognition Performance Using Co-articulation Rules on the Phrase Level: A First Approach",  keywords  = "METIS-119584",  author    = "Ordelman, {Roeland J.F.} and {van Hessen}, {Adrianus J.} and {van Leeuwen}, D.A.",  year      = "1999",  month     = feb,  day       = "4",  language  = "Undefined",  pages     = "1641--1644",  booktitle = "Proceedings of the 14th International Congress of Phonetic Sciences", }

@misc{ordelman_roeland_2022_6597110,
  author       = {Ordelman, Roeland and
                  Sanders, Willemien and
                  Zijdeman, Richard and
                  Klein, Rana and
                  Noordegraaf, Julia and
                  Van Gorp, Jasmijn and
                  Wigham, Mari and
                  Windhouwer, Menzo},
  title        = {{Data Stories in CLARIAH — Developing a Research
                   Infrastructure for Storytelling with Heritage and
                   Culture Data}},
  month        = may,
  year         = 2022,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.6597110},
  url          = {https://doi.org/10.5281/zenodo.6597110}
}
