<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Roeland Ordelman</title> <meta name="author" content="Roeland Ordelman"/> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Roeland  Ordelman"
        },
        "url": "https://roelandordelman.github.io/publications/",
        "@type": "WebSite",
        "description": "publications by categories in reversed chronological order. generated by jekyll-scholar.",
        "headline": "publications",
        "sameAs": ["https://orcid.org/0000-0001-9229-0006", "https://github.com/roelandordelman", "https://www.linkedin.com/in/roelandordelman", "https://twitter.com/roelandordelman", "https://mastodon.utwente.nl/roelandordelman"],
        "name": "Roeland  Ordelman",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://roelandordelman.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NEB83L72RL"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NEB83L72RL");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://roelandordelman.github.io/"><span class="font-weight-bold">Roeland</span> Ordelman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ordelman_roeland_2022_6597110" class="col-sm-8"> <div class="title">Data Stories in CLARIAH — Developing a Research Infrastructure for Storytelling with Heritage and Culture Data</div> <div class="author"> <em>Ordelman, Roeland</em>, Sanders, Willemien, Zijdeman, Richard, Klein, Rana, Noordegraaf, Julia, Van Gorp, Jasmijn, Wigham, Mari, and Windhouwer, Menzo </div> <div class="periodical"> May 2022 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="d5e5c3c3506144ff9afb7a2bb315d550" class="col-sm-8"> <div class="title">Let’s talk about it: Spoken conversational search with a robot for children</div> <div class="author">Beelen, Thomas, Velner, Ella,  <em>Ordelman, Roeland</em>, Truong, Khiet Phuong, Evers, Vanessa, and Huibers, Theo </div> <div class="periodical"> <em>In CUI@CSCW: Inclusive and Collaborative Child-facing Voice Technologies</em> Oct 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this article, we propose a robot to assist children in finding information through conversation. The proposed robot uses clarifying questions to assist children in communicating their information need. We describe the setup of our (unpublished) pilot study that investigated how children perceive robots and the information they provide. We also describe a study we are currently developing that addresses how children’s experience and search outcomes are impacted by a robot asking clarifying questions. We compare a robot using clarifying questions to a robot that replies by directly presenting information, which is how many currently available voice assistants operate. Finally, we describe our future steps. We intend to contribute to the development of children-centered information search and robot technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="b4c787d6b80c41a0aef1e488e9472e90" class="col-sm-8"> <div class="title">Does your robot know? Enhancing children’s information retrieval through spoken conversation with responsible robots</div> <div class="author">Beelen, Thomas Herman Johan, Velner, Ella, Truong, Khiet Phuong, Ordelman, Roeland J.F., Evers, Vanessa, and Huibers, Theo W.C. </div> <div class="periodical"> Jul 2021 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2ee24174001b4ab09c5ac9e9c57c3457" class="col-sm-8"> <div class="title">Automatic Annotations and Enrichments for Audiovisual Archives</div> <div class="author">Noord, Nanne Van, Olesen, Christian,  <em>Ordelman, Roeland</em>, and Noordegraaf, Julia </div> <div class="periodical"> <em>In Proceedings of the 13th International Conference on Agents and Artificial Intelligence - Volume 1: ARTIDIGH</em> Jul 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The practical availability of Audiovisual Processing tools to media scholars and heritage institutions remains limited, despite all the technical advancements of recent years. In this article we present the approach chosen in the CLARIAH project to increase this availability, we discuss the challenges encountered, and introduce the technical solutions we are implementing. Through three use cases focused on the enrichment of AV archives, Pose Analysis, and Automatic Speech Recognition, we demonstrate the potential and breadth of using Audiovisual Processing for archives and Digital Humanities research.</p> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"></ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6c7c8b965268405faae1f258dfe10858" class="col-sm-8"> <div class="title">Media Suite: Unlocking Audiovisual Archives for Mixed Media Scholarly Research</div> <div class="author">Ordelman, Roeland J.F., Melgar Estrada, Liliana, van Gorp, Jasmijn, and Noordegraaf, Julia </div> <div class="periodical"> <em>In CLARIN Annual Conference 2018</em> Feb 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper discusses the rationale behind and approach towards the development of a research environment –the Media Suite– in a sustainable, dynamic, multi-institutional infrastructure that supports mixed media scholarly research with large audiovisual data collections and available multimedia context collections, serving media scholars and digital humanists in general.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f9522016e8e44fa2b5edcdae8ca6aa74" class="col-sm-8"> <div class="title">Jupyter Notebooks for Generous Archive Interfaces</div> <div class="author">Wigham, Mari, Melgar Estrada, Liliana, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In 2018 IEEE International Conference on Big Data (Big Data)</em> Jan 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>To help scholars to extract meaning, knowledge and value from large volumes of archival content, such as the Dutch Common Lab Research Infrastructure for the Arts and Humanities (CLARIAH), we need to provide more ‘generous’ access to the data than can be provided with generalised search and visualisation tools alone. Our approach is to use Jupyter Notebooks in combination with the existing archive APIs (Application Programming Interface). This gives access to both the archive metadata and a wide range of analysis and visualisation techniques. We have created notebooks and modules of supporting functions that enable the overview, investigation and analysis of the archive. We demonstrate the value of our approach in preliminary tests of its use in scholarly research, and give our observations of the potential value for archivists. Finally, we show that good archive knowledge is essential to create correct and meaningful visualisations and statistics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bcec7972c3fd4f42aacebe4cb40a0df1" class="col-sm-8"> <div class="title">The CLARIAH Media Suite: A Hybrid Approach to System Design in the Humanities</div> <div class="author">Melgar-Estrada, Liliana, Koolen, Marijn, Beelen, Kaspar, Huurdeman, Hugo, Wigham, Mari, Martinez-Ortiz, Carlos, Blom, Jaap, and <em>Ordelman, Roeland</em> </div> <div class="periodical"> <em>In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval</em> Jan 2019 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="75f7d6726f0b48bdbaef8fd91cf2d316" class="col-sm-8"> <div class="title">Media Suite: Unlocking Archives for Mixed Media Scholarly Research</div> <div class="author">Ordelman, Roeland J.F., Melgar Estrada, Liliana, Martínez Ortíz, Carlos, and Noordegraaf, Julia </div> <div class="periodical"> <em>In CLARIN 2018 Annual Conference</em> Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper discusses the rationale behind the development of a research environment –the Media Suite– in a sustainable, dynamic, multi-institutional infrastructure that supports mixed media scholarly research with large multimedia data collections, serving media scholars and digital humanists in general.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="e08835da324c42958ac7d7f7cff13094" class="col-sm-8"> <div class="title">Speech Recognition and Scholarly Research: Usability and Sustainability</div> <div class="author">Ordelman, Roeland J.F., and van Hessen, Adrianus J. </div> <div class="periodical"> <em>In CLARIN 2018 Annual Conference</em> Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>For years we have been working on speech recognition (ASR) as a tool for scholarly research. The current state-of-the-art can be useful for many scholarly use cases focusing on audiovisual content, but practically applying ASR is often not so straightforward. In the CLARIAH Media Suite, a secured online portal for scholarly research for audiovisual media, we solved the most important hurdles for the practical deployment of ASR by focusing on usability and sustainability aspects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="4b8387a7d9af43a9b0f6eecb2d0d0159" class="col-sm-8"> <div class="title">Challenges in Enabling Mixed Media Scholarly Research with Multi-media Data in a Sustainable Infrastructure</div> <div class="author"> <em>Ordelman, Roeland</em>, Martínez Ortíz, Carlos, Melgar Estrada, Liliana, Koolen, Marijn, Blom, Jaap, Melder, Willem, van Gorp, Jasmijn, De Boer, Victor, Karavellas, Themistoklis, Aroyo, Lora, Poell, Thomas, Karrouche, Norah, Baaren, Eva, Wassenaar, Johannes, Inel, Oana, and Noordegraaf, Julia </div> <div class="periodical"> <em>In Digital Humanities 2018 Conference</em> Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Big-scale infrastructure projects in the humanities and social sciences such as the Digital Research Infrastructure for the Arts and Humanities (DARIAH) (Edmond et al., 2017), or the Common Language Resources and Technology Infrastructure (CLARIN) (Hinrichs and Krauwer, 2014) aim to provide solutions for both preservation and access to collections and data necessary for scholarly research (Zundert, 2012). Some infrastructure projects build decentralized “atomic” software services, e.g., as in the LLS infrastructure project (Buchler et al., 2016), while others prefer to build more centralized virtual research environments, as in the European Holocaust Research Infrastructure (EHRI) (Lauer, 2014). Also, even within a single infrastructure project, these two models can coexist. This is the case of the CLARIAH infrastructure, where different approaches have been taken to date for serving different user groups, i.e., several specialized tools for linguists (Odijk, Broeder &amp; Barbiers, 2015), or a research environment (the Media Suite) that serves the scholarly needs for working with audiovisual data collections and related mixed-media contextual sources that are maintained at cultural heritage and knowledge institutions. This paper discusses the rationale and challenges behind the development of the Media Suite.</p> </div> </div> </div> </li> </ol> <h2 class="year">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="b1881c0008ee411886d88e85542e3d11" class="col-sm-8"> <div class="title">Multimodal video-to-video linking: Turning to the crowd for insight and evaluation</div> <div class="author">Amsaleg, Laurent, Eskevich, Maria, Gu\dhmundsson, Gylfi \THór, Larson, Martha, Aly, Robin, Gurrin, Cathal, Jónsson, Björn \THór, Sabetghadam, Serwah, Jones, Gareth J.F., Satoh, Shin’ichi, Ordelman, Roeland J.F., and Huet, Benoit </div> <div class="periodical"> <em>In Multimedia Modeling</em> Jan 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Video-to-video linking systems allow users to explore and exploit the content of a large-scale multimedia collection interactively and without the need to formulate specific queries. We present a short introduction to video-to-video linking (also called ‘video hyperlinking’), and describe the latest edition of the Video Hyperlinking (LNK) task at TRECVid 2016. The emphasis of the LNK task in 2016 is on multimodality as used by videomakers to communicate their intended message. Crowdsourcing makes three critical contributions to the LNK task. First, it allows us to verify the multimodal nature of the anchors (queries) used in the task. Second, it enables us to evaluate the performance of video-to-video linking systems at large scale. Third, it gives us insights into how people understand the relevance relationship between two linked video segments. These insights are valuable since the relationship between video segments can manifest itself at different levels of abstraction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2ab06fd4a4b641f6a8cd795b52bff0c6" class="col-sm-8"> <div class="title">TREC video retrieval evaluation TRECVID 2017</div> <div class="author">Awad, George, Soboroff, Ian, Butt, Asad, Ellis, Angela, Dimmick, Darrin, Fiscus, Jonathan, Joy, David, Michel, Martial, Delgado, Andrew, Smeaton, Alan, Kraaij, Wessel, Quénot, Georges,  <em>Ordelman, Roeland</em>, Eskevich, Maria, Jones, Gareth, Huet, Benoit, Strassel, Stephanie, and Li, Xuansong </div> <div class="periodical"> Jan 2017 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="35f08650f8a04f9a9a8eb5e49ac6712b" class="col-sm-8"> <div class="title">Trecvid 2017: Evaluating ad-hoc and instance video search, events detection, video captioning and hyperlinking</div> <div class="author">Awad, George, Butt, Asad A., Fiscus, Jonathan, Joy, David, Delgado, Andrew, Michel, Martial, Smeaton, Alan F, Graham, Yvette, Kraaij, Wessel, Quénot, Georges, Ordelman, Roeland J.F., Jones, Gareth J.F., and Huet, Benoit </div> <div class="periodical"> <em>In Proceedings of TRECVID</em> Jan 2017 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="357afd8a09824da4b89a25ba3ce96b81" class="col-sm-8"> <div class="title">Video hyperlinking (LNK) TRECVID 2017</div> <div class="author">Eskevich, Maria,  <em>Ordelman, Roeland</em>, Jones, Gareth J.F., and Huet, Benoit </div> <div class="periodical"> Jan 2017 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="1c9642873ac0420dae48fe2fb0399c8f" class="col-sm-8"> <div class="title">Evaluating Unsupervised Thesaurus-based Labeling of Audiovisual Content in an Archive Production Environment</div> <div class="author">de Boer, Victor, Ordelman, Roeland J.F., and Schuurman, Josefien </div> <div class="periodical"> <em>International journal on digital libraries</em> Sep 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we report on a two-stage evaluation of unsupervised labeling of audiovisual content using collateral text data sources to investigate how such an approach can provide acceptable results for given requirements with respect to archival quality, authority and service levels to external users. We conclude that with parameter settings that are optimized using a rigorous evaluation of precision and accuracy, the quality of automatic term-suggestion is sufficiently high. We furthermore provide an analysis of the term extraction after being taken into production, where we focus on performance variation with respect to term types and television programs. Having implemented the procedure in our production work-flow allows us to gradually develop the system further and to also assess the effect of the transformation from manual to automatic annotation from an end-user perspective. Additional future work will be on deploying different information sources including annotations based on multimodal video analysis such as speaker recognition and computer vision.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="cca815c3f6b74363b6e891c1824504b4" class="col-sm-8"> <div class="title">Pursuing a moving target: Iterative use of benchmarking of a task to understand the task</div> <div class="author">Eskevich, Maria, Jones, Gareth J.F., Aly, Robin,  <em>Ordelman, Roeland</em>, and Huet, Benoit </div> <div class="periodical"> <em>CEUR workshop proceedings</em> Jan 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Individual tasks carried out within benchmarking initiatives, or campaigns, enable direct comparison of alternative approaches to tackling shared research challenges and ideally promote new research ideas and foster communities of researchers interested in common or related scientific topics. When a task has a clear predefined use case, it might straightforwardly adopt a well established framework and methodology. For example, an ad hoc information retrieval task adopting the standard Cranfield paradigm. On the other hand, in cases of new and emerging tasks which pose more complex challenges in terms of use scenarios or dataset design, the development of a new task is far from a straightforward process. This letter summarises our reections on our experiences as task organisers of the Search and Hyperlinking task from its origins as a Brave New Task at the MediaEval benchmarking campaign (2011-2014) to its current instantiation as a task at the NIST TRECVid benchmark (since 2015). We highlight the challenges encountered in the development of the task over a number of annual iterations, the solutions found so far, and our process for maintaining a vision for the ongoing advancement of the task’s ambition.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="af8cadd8ce5943a0ae698a529544753c" class="col-sm-8"> <div class="title">Developing Benchmarks: The Importance of the Process and New Paradigms</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In MMCommons ’16: Proceedings of the 2016 ACM Workshop on Multimedia COMMONS</em> Jan 2016 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="605c27f37b8c4a22a3c5398cb8cece79" class="col-sm-8"> <div class="title">TREC video retrieval evaluation</div> <div class="author">Awad, George, Smeaton, Alan, Soboroff, Ian, Kraaij, Wessel, Ellis, Angela, Quénot, Georges, Dimmick, Darrin,  <em>Ordelman, Roeland</em>, Aly, Robin, Eskevich, Maria, Larson, Martha, Jones, Gareth, Fiscus, Jonathan, Huet, Benoit, Joy, David, Ritter, Marc, Michel, Martial, Strassel, Stephanie, Delgado, Andrew, and Li, Xuansong </div> <div class="periodical"> Jan 2016 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="db55ef0d9cd84e8099754cf7ee6e3c14" class="col-sm-8"> <div class="title">Video hyperlinking (LNK) TRECVID 2016</div> <div class="author">Eskevich, Maria, Larson, Martha, Aly, Robin,  <em>Ordelman, Roeland</em>, Jones, Gareth J.F., and Huet, Benoit </div> <div class="periodical"> Jan 2016 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6523b8d139b54f85bed7fcb67ce5c0a1" class="col-sm-8"> <div class="title">Convenient Discovery of Archived Video Using Audiovisual Hyperlinking</div> <div class="author">Ordelman, Roeland J.F., Aly, Robin, Eskevich, Maria, Huet, Benoit, and Jones, Gareth J.F. </div> <div class="periodical"> <em>In Proceedings of the Third Edition Workshop on Speech, Language &amp; Audio in Multimedia (SLAM 2015)</em> Oct 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper overviews ongoing work that aims to support end-users in conveniently exploring and exploiting large audiovisual archives by deploying multiple multimodal linking approaches. We present ongoing work on multimodal video hyperlinking, from a perspective of unconstrained link anchor identification and based on the identification of named entities, and recent attempts to implement and validate the concept of outside-in linking that relates current events to archive content. Although these concepts are not new, current work is revealing novel insights, more mature technology, development of benchmark evaluations and emergence of dedicated workshops which are opening many interesting research questions on various levels that require closer collaboration between research communities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bb1d465aa67a48d5828e192d0546d041" class="col-sm-8"> <div class="title">Overview of the 2015 Workshop on Speech, Language and Audio in Multimedia</div> <div class="author">Gravier, Guillaume, Jones, Gareth J.F., Larson, Martha, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of the 23rd ACM International Conference on Multimedia (MM 2015)</em> Oct 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Workshop on Speech, Language and Audio in Multimedia (SLAM) positions itself at at the crossroad of multiple scientific fields - music and audio processing, speech processing, natural language processing and multimedia - to discuss and stimulate research results, projects, datasets and benchmarks initiatives where audio, speech and language are applied to multimedia data. While the first two editions were collocated with major speech events, SLAM’15 is deeply rooted in the multimedia community, opening up to computer vision and multimodal fusion. To this end, the workshop emphasizes video hyperlinking as an showcase where computer vision meets speech and language. Such techniques provide a powerful illustration of how multimedia technologies incorporating speech, language and audio can make multimedia content collections better accessible, and thereby more useful, to users.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="4d6f60f91e134146a9250fc06c8f7e68" class="col-sm-8"> <div class="title">Practice-Oriented Evaluation of Unsupervised Labeling of Audiovisual Content in an Archive Production Environment</div> <div class="author">de Boer, Victor, Ordelman, Roeland J.F., and Schuurman, Josefien </div> <div class="periodical"> <em>In Research and Advanced Technology for Digital Libraries, Proceedings of the 19th International Conference on Theory and Practice of Digital Libraries, TPDL 2015</em> Sep 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we report on an evaluation of unsupervised labeling of audiovisual content using collateral text data sources to investigate how such an approach can provide acceptable results given requirements with respect to archival quality, authority and service levels to external users. We conclude that with parameter settings that are optimized using a rigorous evaluation of precision and accuracy, the quality of automatic term-suggestion are sufficiently high. Having implemented the procedure in our production work-flow allows us to gradually develop the system further and also assess the effect of the transformation from manual to automatic from an end-user perspective. Additional future work will be on deploying different information sources including annotations based on multimodal video analysis such as speaker recognition and computer vision.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="130a111503e34981a303386ab358b333" class="col-sm-8"> <div class="title">SAVA at MediaEval 2015: Search and Anchoring in Video Archives</div> <div class="author">Eskevich, Maria, Aly, Robin, Ordelman, Roeland J.F., Racca, David N., Chen, Shu, and Jones, Gareth J.F. </div> <div class="periodical"> <em>In Working Notes Proceedings of the MediaEval 2015 Workshop</em> Sep 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Search and Anchoring in Video Archives (SAVA) task at MediaEval 2015 consists of two sub-tasks: (i) search for multimedia content within a video archive using multimodal queries referring to information contained in the audio and visual streams/content, and (ii) automatic selection of video segments within a list of videos that can be used as anchors for further hyperlinking within the archive. The task used a collection of roughly 2700 hours of the BBC broadcast TV material for the former sub-task, and about 70 les taken from this collection for the latter sub-task. The search sub-task is based on an ad-hoc retrieval scenario, and is evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk (MTurk). The evaluation used metrics that are variations of MAP adjusted for this task. For the anchor selection sub-task overlapping regions of interest across participants submissions were assessed using MTurk workers, and mean reciprocal rank (MRR), precision and recall were calculated for evaluation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="9258eb74d20c424798f65af643ec3e47" class="col-sm-8"> <div class="title">Defining and Evaluating Video Hyperlinking for Navigating Multimedia Archives</div> <div class="author">Ordelman, Roeland J.F., Eskevich, Maria, Aly, Robin, Huet, Benoit, and Jones, Gareth J.F. </div> <div class="periodical"> <em>In Proceedings of the 24th International Conference on World Wide Web, WWW 2015 Companion</em> May 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Multimedia hyperlinking is an emerging research topic in the context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hyperlinking from a video search perspective in the context of the MediaEval evaluation benchmark for several years. Our task considers a use case of exploring large quantities of video content via an automatically created hyperlink structure at the media fragment level. In this paper we report on our findings, examine the features of the definition of video hyperlinking based on results, and discuss lessons learned with respect to evaluation of hyperlinking in real-life use scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="a039a6b8b4a04e1ab178bd93f66c59e2" class="col-sm-8"> <div class="title">Exploiting program guides for contextualisation</div> <div class="author">Baltussen, Lotte Belice, Karavellas, Themistoklis, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In 2015 Digital Heritage International Congress, Digital Heritage 2015</em> Feb 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Archives of cultural heritage organisations typically consist of collections in various formats (e.g. photos, video, texts) that are inherently related. Often, such disconnected collections represent value in itself but effectuating links between ’core’ and ’context’ collection items in various levels of granularity could result in a ’one-plus-one-makes-three’ scenario both from a contextualisation perspective (public presentations, research) and access perspective. A key issue is the identification of contextual objects that can be associated with objects in the core collections, or the other way around. Traditionally, such associations have been created manually. For most organizations however, this approach does not scale. In this paper, we describe a case in which a semi-automatic approach was employed to create contextual links between television broadcast schedules in program guides (context collection) and the programs in the archive (core collection) of a large audiovisual heritage organisation.</p> </div> </div> </div> </li> </ol> <h2 class="year">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="fc56ac0f0031458897090352729ea1d9" class="col-sm-8"> <div class="title">User perspectives on semantic linking in the audio domain</div> <div class="author">Nadeem, Danish, Ordelman, Roeland J.F., Aly, Robin, and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the 10th International Conference on Signal-Image Technology and Internet-Based Systems (SITIS 2014)</em> Nov 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Semantic linking has a potential to enrich the audiovisual experience for users of television or radio broadcast archives. Recently, automatic semantic linking, has received increased attention, especially as second screen applications for television broadcasts are emerging. Semantic linking for radio broadcasts can enrich radio listening experience in a similar manner in combination with second screen-like applications. While the development of such applications is gaining popularity, little is known about the information in a radio program that may be interesting for link creation from a user perspective. We conducted a user study on semantic linking for radio broadcasts in order to know what information users regard as suitable anchors and what kind of information they like as targets. We found that users often regard topic and person as the best link anchors in the program. Additionally, we found that frequency and timing of information elements in a radio program do not dominate the users’ selection of anchors. Furthermore, we found that there is a low agreement among users on regarding certain information elements as anchors. For practical reasons the study is conducted with 10 minutes of radio broadcast material of a particular program type, and with a total of 22 participants. The insights gained in the user study will help the understanding of user perspectives on semantic linking in the audio domain.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="c15511a5732d4dd7bd98fa9276681870" class="col-sm-8"> <div class="title">The Search and Hyperlinking Task at MediaEval 2014</div> <div class="author">Eskevich, Maria, Aly, Robin, Racca, David N., Ordelman, Roeland J.F., Chen, Shu, and Jones, Gareth J.F. </div> <div class="periodical"> <em>In MediaEval 2014: Multimedia Benchmark Workshop</em> Oct 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Search and Hyperlinking Task at MediaEval 2014 is the third edition of this task. As in previous versions, it consisted of two sub-tasks: (i) answering search queries from a collection of roughly 2700 hours of BBC broadcast TV material, and (ii) linking anchor segments from within the videos to other target segments within the video collection. For MediaEval 2014, both sub-tasks were based on an ad-hoc retrieval scenario, and were evaluated using a pooling procedure across participants submissions with crowdsourcing relevance assessment using Amazon Mechanical Turk.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2bba103395f44b158c48ba96e64a1d4f" class="col-sm-8"> <div class="title">Beyond metadata: searching your archive based on its audio-visual content</div> <div class="author">Tommasi, T., Aly, Robin, McGuinness, K., Chatfield, K., Arandjelovic, R., Parkhi, O., Ordelman, Roeland J.F., Zisserman, A., and Tuytelaars, T. </div> <div class="periodical"> <em>In Proceedings of the 2014 International Broadcasting Convention, IBC 2014</em> Sep 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The EU FP7 project AXES aims at better understanding the needs of archive users and supporting them with systems that reach beyond the state-of-the-art. Our system allows users to instantaneously retrieve content using metadata, spoken words, or a vocabulary of reliably detected visual concepts comprising places, objects and events. Additionally, users can query for new concepts, for which models are learned on-the-fly, using training images obtained from an internet search engine. Thanks to advanced analysis and indexation methods, relevant material can be retrieved within seconds. Our system supports different types of models for object categories (e.g. “bus‿ or “house‿), specific objects (landmarks or logos), person categories (e.g. “people with moustaches‿), or specific persons (e.g. “President Obama‿). Next to text queries, we support query-by-example, which retrieves content containing the same location, objects, or faces shown in provided images. Finally, our system provides alternatives to query-based retrieval by allowing users to browse archives using generated links. Here we evaluate the precision of the retrieved results based on textual queries describing visual content, with the queries extracted from user testing query logs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="5b0d9f5e326f40c1b65f4c996c54a878" class="col-sm-8"> <div class="title">Talking With Scholars: Developing a Research Environment for Oral History Collections</div> <div class="author">Kemman, Max, Scagliola, Stef, de Jong, Franciska M.G., and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of the 2nd International Workshop on Supporting Users Exploration of Digital Libraries (SUEDL 2013), Revised Selected Papers</em> Jul 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Scholars are yet to make optimal use of Oral History collections. For the uptake of digital research tools in the daily working practice of researchers, practices and conventions commonly adhered to in the subfields of the humanities should be taken into account during development, in order to facilitate the uptake of digital research tools in the daily working practice of researchers. To this end, in the Oral History Today project a research tool for exploring Oral History collections is developed in close collaboration with scholarly researchers. This paper describes four stages of scholarly research and the first steps undertaken to incorporate requirements of these stages in a digital research environment.</p> </div> </div> </div> </li> </ol> <h2 class="year">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f712df0f94ab492ba10989dbd19f0be8" class="col-sm-8"> <div class="title">Users Requirements in Audiovisual Search: A Quantitative Approach</div> <div class="author">Nadeem, Danish, Ordelman, Roeland J.F., Aly, Robin, and Verbruggen, Erwin </div> <div class="periodical"> <em>In Proceedings of the International Conference on Theory and Practice of Digital Libraries, TPDL 2013</em> Sep 2013 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper reports on the results of a quantitative analysis of user requirements for audiovisual search that allow the categorisation of requirements and to compare requirements across user groups. The categorisation provides clear directions with respect to the prioritisation of system features from the perspective of the development of systems for specific, single user groups and systems that have a more general target user group.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="753fd33b8f1f43f8a044d2f75d9e932d" class="col-sm-8"> <div class="title">Linking inside a video collection - what and how to measure?</div> <div class="author">Aly, Robin, Ordelman, Roeland J.F., Eskevich, M., Jones, G.J.F, and Chen, S. </div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on World Wide Web Companion, IW3C2 2013</em> May 2013 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Although linking video to additional information sources seems to be a sensible approach to satisfy information needs of user, the perspective of users is not yet analyzed on a fundamental level in real-life scenarios. However, a better understanding of the motivation of users to follow links in video, which anchors users prefer to link from within a video, and what type of link targets users are typically interested in, is important to be able to model automatic linking of audiovisual content appropriately. In this paper we report on our methodology towards eliciting user requirements with respect to video linking in the course of a broader study on user requirements in searching and a series of benchmark evaluations on searching and linking.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="30054c2f851e4a48bf30e66a62a08fc3" class="col-sm-8"> <div class="title">Multimedia information seeking through search and hyperlinking</div> <div class="author">Eskevich, Maria, Jones, Gareth J.F, Aly, Robin, Ordelman, Roeland J.F., Chen, Shu, Nadeem, Danish, Guinaudeau, Camille, Gravier, Guillaume, Sébillot, Pascale, de Nies, Tom, Debevere, Pedro, Van de Walle, Rik, Galuščáková, Petra, Pecina, Pavel, and Larson, Martha </div> <div class="periodical"> <em>In ICMR 2013</em> Apr 2013 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Searching for relevant webpages and following hyperlinks to related content is a widely accepted and eective approach to information seeking on the textual web. Existing work on multimedia information retrieval has focused on search for individual relevant items or on content linking without specic attention to search results. We describe our research exploring integrated multimodal search and hyperlinking for multimedia data. Our investigation is based on the Medi- aEval 2012 Search and Hyperlinking task. This includes a known-item search task using the Blip10000 internet video collection, where automatically created hyperlinks link each relevant item to related items within the collection. The search test queries and link assessment for this task was generated using the Amazon Mechanical Turk crowdsourc- ing platform. Our investigation examines a range of alterna- tive methods which seek to address the challenges of search and hyperlinking using multimodal approaches. The results of our experiments are used to propose a research agenda for developing eective techniques for search and hyperlinking of multimedia content.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="36e66c8fd9b64e938331915f1638dd6d" class="col-sm-8"> <div class="title">Improving cyberbullying detection with user context</div> <div class="author">Dadvar, M., Trieschnigg, Rudolf Berend, Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the 35th European Conference on IR Research, ECIR 2013</em> Mar 2013 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited. Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users’ characteristics and profile information. In this paper we show that taking user context into account improves the detection of cyberbullying.</p> </div> </div> </div> </li> </ol> <h2 class="year">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="9fad262aa7c84f1bac8d9b77a5f38fef" class="col-sm-8"> <div class="title">Search and Hyperlinking Task at MediaEval 2012</div> <div class="author">Eskevich, Maria, Jones, Gareth J.F., Chen, Shu, Aly, Robin, Ordelman, Roeland J.F., and Larson, Martha </div> <div class="periodical"> <em>In MediaEval 2012 Multimedia Benchmark Workshop</em> Oct 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Search and Hyperlinking Task was one of the Brave New Tasks at MediaEval 2012. The Task consisted of two sub- tasks which focused on search and linking in retrieval from a collection of semi-professional video content. These tasks followed up on research carried out within the MediaEval 2011 Rich Speech Retrieval (RSR) Task and the VideoCLEF 2009 Linking Task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="fbb4662a3f5e497582d8db741b166b03" class="col-sm-8"> <div class="title">UTwente does Brave New Tasks for MediaEval 2012: Searching and Hyperlinking</div> <div class="author">Nadeem, Danish, Aly, Robin, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In MediaEval 2012 Multimedia Benchmark Workshop</em> Oct 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we report our experiments and results for the brave new searching and hyperlinking tasks for the MediaEval Benchmark Initiative 2012. The searching task involves nding target video segments based on a short natural language sentence query and the hyperlinking task involves nding links from the target video segments to other related video segments in the collection using a set of anchor segments in the videos that correspond to the textual search queries. To nd the starting points in the video, we only used speech transcripts and metadata as evidence source, however, other visual features (for e.g., faces, shots and keyframes) might also aect results for a query. We indexed speech transcripts and metadata, furthermore, the speech transcripts were indexed at speech segment level and at sentence level to improve the likelihood of nding jump-in-points. For linking video segments, we computed k-nearest neighbours of video segments using euclidean distance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="13d427916a4e4e2b9b00f0bf13e8ae6f" class="col-sm-8"> <div class="title">The Community and the Crowd: Multimedia Benchmark Dataset Development</div> <div class="author">Larson, Martha, Soleymani, Mohammad, Eskevich, Maria, Serdyukov, Pavel, Ordelman, Roeland J.F., and Jones, Gareth </div> <div class="periodical"> <em>IEEE multimedia</em> Jul 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The MediaEval Multimedia Benchmark leveraged community cooperation and crowdsourcing to develop a large Internet video dataset for its Genre Tagging and Rich Speech Retrieval tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="8d80ccc149cb4354989448b597ceca2e" class="col-sm-8"> <div class="title">Comparing Retrieval Effectiveness of Alternative Content Segmentation Methods for Internet Video Search</div> <div class="author">Eskevich, Maria, Jones, Gareth J.F., Larson, Martha, Wartena, Christian, Aly, Robin, Verschoor, Thijs, and <em>Ordelman, Roeland</em> </div> <div class="periodical"> <em>In Proceedings of the 10th Workshop on Content-Based Multimedia Indexing (CMBI 2012)</em> Jun 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We present an exploratory study of the retrieval of semiprofessional user-generated Internet video. The study is based on the MediaEval 2011 Rich Speech Retrieval (RSR) task for which the dataset was taken from the Internet sharing platform blip.tv, and search queries associated with specific speech acts occurring in the video. We compare results from three participant groups using: automatic speech recognition system transcript (ASR), metadata manually assigned to each video by the user who uploaded it, and their combination. RSR 2011 was a known-item search for a single manually identified ideal jump-in point in the video for each query where playback should begin. Retrieval effectiveness is measured using the MRR and mGAP metrics. Using different transcript segmentation methods the participants tried to maximize the rank of the relevant item and to locate the nearest match to the ideal jump-in point. Results indicate that best overall results are obtained for topically homogeneous segments which have a strong overlap with the relevant region associated with the jump-in point, and that use of metadata can be beneficial when segments are unfocused or cover more than one topic.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="27eacd3607ee44108da16164df41fa64" class="col-sm-8"> <div class="title">Towards User Modelling in the Combat Against Cyberbullying</div> <div class="author">Dadvar, M., Ordelman, Roeland J.F., de Jong, Franciska M.G., and Trieschnigg, Rudolf Berend </div> <div class="periodical"> <em>In Proceedings of the17th International Conference on Applications of Natural Language to Information Systems, NLDB 2012</em> Jun 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Friendships, relationships and social communications have all gone to a new level with new definitions as a result of the invention of online social networks. Meanwhile, alongside this transition there is increasing evidence that online social applications have been used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the users involved in cyberbullying. We hypothesis that incorporation of the users’ profile, their characteristics, and post-harassing behaviour, for instance, posting a new status in another social network as a reaction to their bullying experience, will improve the accuracy of cyberbullying detection. Cross-system analyses of the users’ behaviour - monitoring users’ reactions in different online environments - can facilitate this process and could lead to more accurate detection of cyberbullying. This paper outlines the framework for this faceted approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="553903cbff15425b8d9c970c2d6b0440" class="col-sm-8"> <div class="title">Link Anchors in Images: Is there Truth?</div> <div class="author">Aly, Robin, McGuinness, Kevin, Kleppe, Martijn, Ordelman, Roeland J.F., O’Connor, Noel, and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the 12th Dutch Belgian Information Retrieval Workshop (DIR 2012)</em> Feb 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>While automatic linking in text collections is well understood, little is known about links in images. In this work, we investigate two aspects of anchors, the origin of a link, in images: 1) the requirements of users for such anchors, e.g. the things users would like more information on, and 2) possible evaluation methods assessing anchor selection algorithms. To investigate these aspects, we perform a study with 102 users. We find that 59% of the required anchors are image segments, as opposed to the whole image, and most users require information on displayed persons. The agreement of users on the required anchors is too low (often below 30%) for a ground truth-based evaluation, which is the standard IR evaluation method. As an alternative, we propose a novel evaluation method based on improved search performance and user experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="af923d24e041439aab3a7fd01c3a7d55" class="col-sm-8"> <div class="title">Improved cyberbullying detection using gender information</div> <div class="author">Dadvar, M., de Jong, Franciska M.G., Ordelman, Roeland J.F., and Trieschnigg, Rudolf Berend </div> <div class="periodical"> <em>In Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop (DIR 2012)</em> Feb 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As a result of the invention of social networks, friendships, relationships and social communication are all undergoing changes and new definitions seem to be applicable. One may have hundreds of ‘friends’ without even seeing their faces. Meanwhile, alongside this transition there is increasing evidence that online social applications are used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the characteristics of the actors involved in cyberbullying. Social studies on cyberbullying reveal that the written language used by a harasser varies with the author’s features including gender. In this study we used a support vector machine model to train a gender-specific text classifier. We demonstrated that taking gender-specific language features into account improves the discrimination capacity of a classifier to detect cyberbullying.</p> </div> </div> </div> </li> </ol> <h2 class="year">2011</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="30a832137ac24b08901eaf8afd8cfa5d" class="col-sm-8"> <div class="title">AXES at TRECVid 2011</div> <div class="author">McGuinness, Kevin, Aly, Robin, Chen, Shu, Frappier, Mathieu, Kleppe, Martijn, Lee, Hyowon, Ordelman, Roeland J.F., and Arandjelovic, Relja </div> <div class="periodical"> <em>In TREC 2011 Video Retrieval Evaluation Online Proceedings (TRECVid 2011)</em> Dec 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The AXES project participated in the interactive known-item search task (KIS) and the interactive instance search task (INS) for TRECVid 2011. We used the same system architecture and a nearly identical user interface for both the KIS and INS tasks. Both systems made use of text search on ASR, visual concept detectors, and visual similarity search. The user experiments were carried out with media professionals and media students at the Netherlands Institute for Sound and Vision, with media professionals performing the KIS task and media students participating in the INS task. This paper describes the results and findings of our experiments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="1b26c1869b3e45069ad3833adad701ca" class="col-sm-8"> <div class="title">Audiovisual Archive Exploitation in the Networked Information Society</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In MIRUM ’11: Proceedings of the 1st International ACM Workshop on Music Information Retrieval with User-centered and Multimodal Strategies</em> Nov 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Safeguarding the massive body of audiovisual content, including rich music collections, in audiovisual archives and enabling access for various types of user groups is a prerequisite for unlocking the social-economic value of these collections. Data quantities and the need for specific content descriptors however, force archives to re-evaluate their annotation strategies and access models, and incorporate technology in the archival workflow. It is argued that this can only be successfully done provided that user requirements are studied well and that new approaches are introduced in a well-balanced manner, fitting in with traditional archival perspectives, and by bringing the archivist in the technology loop by means of education and by deploying hybrid work-flows for technology aided annotation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="e8e7401babee428c87a5bcb6bfe0ccdc" class="col-sm-8"> <div class="title">Audio-visual Collections and the User Needs of Scholars in the Humanities: a Case for Co-Development</div> <div class="author">de Jong, Franciska M.G., Ordelman, Roeland J.F., and Scagliola, Stef </div> <div class="periodical"> <em>In Proceedings of the 2nd Conference on Supporting Digital Humanities (SDH 2011)</em> Nov 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The aim of this paper is to reflect on the factors that impede a clear communication and a more fruitful collaboration between humanities scholars and ICT developers. One of the observations is that ICT-researchers who design tools for humanities researchers, are less inclined to take into account that each stage of the scholarly research process requires ICT-support in a different manner or through different tools. Likewise scholars in the humanities often have prejudices concerning ICT-tools, based on lack of knowledge and fears of technology-driven agendas. If the potential for methodological innovation of the humanities is to be realized, the gap between the mindset of ICT-researchers and that of archivists and scholars in the humanities needs to be bridged. Our assumption is that a better insight into the variety of uses of digital collections and a user-inspired classification of ICT-tools, can help to achieve a greater conceptual clarity among both users and developers. This paper presents such an overview in the form of a typology for the audio-visual realm: examples of what role digital audio-visual archives can play at various research stages, and an inventory of the challenges for the parties involved.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="aed466456f3e4c16b826f831329a3836" class="col-sm-8"> <div class="title">UTwente does Rich Speech Retrieval at MediaEval 2011</div> <div class="author">Aly, Robin, Verschoor, T., and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Working Notes Proceedings of the MediaEval 2011 Workshop</em> Sep 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper describes the participation of the University of Twente team at the Rich Text Retrieval Task of the Media Eval Benchmark Initiative 2011. The goal of the task is to find entry points of relevant parts of videos to reduce the browsing effort of searchers. This is our first participation, therefore our main focus is to create a baseline system which can be improved in the future. We experiment with different evidence sources (ASR and meta data) together with a basic score combination function. We also experiment with different entry points relative to the segments found by the contained evidence.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2c3f6d63df8a4215b0a7834cd82ab412" class="col-sm-8"> <div class="title">Distributed Access to Oral History collections: Fitting Access Technology to the needs of Collection Owners and Researchers</div> <div class="author">Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Digital Humanities 2011: Conference Abstracts</em> Jun 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In contrast with the large amounts of potential interesting research material in digital multimedia repositories, the opportunities to unveil the gems therein are still very limited. The Oral History project ‘Verteld Verleden’ (Dutch literal translation of Oral History) that is currently running in The Netherlands, focuses on improving access to spoken testimonies in collections, spread over many Dutch cultural heritage institutions, by deploying modern technology both concerning infrastructure and access. Key objective in the project is mapping the various specific requirements of collection owners and researchers regarding both publishing and access by means of current state-of-the-technology. In order to demonstrate the potential, Verteld Verleden develops an Oral History portal that provides access to distributed collections. At the same time, practical step-by-step plans are provided to get to work with modern access technologies. In this way, a solid starting point for sustained access to Oral History collections can be established.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="39c4ce097f2f4807a2caf55244c194a8" class="col-sm-8"> <div class="title">Accessing Audiovisual Heritage: A Roadmap for Collaborative Innovation</div> <div class="author">Oomen, Johan, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>IEEE multimedia</em> Apr 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Digitization of audiovisual archives is opening up a wealth of challenges and possibilities for innovations in science, education, and business. The key to unlocking archives for innovation is multimedia technology. In this article the authors zoom in on one of the largest multimedia archives in Europe, highlight collaborative projects with academia, and call for a mutual research agenda.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="5cf4241a01254a45adb5df2cb1928665" class="col-sm-8"> <div class="title">Automatic Tagging and Geotagging in Video Collections and Communities</div> <div class="author">Larson, Martha, Soleymani, Mohammad, Serdyukov, Pavel, Rudinac, Stevan, Wartena, Christian, Murdock, Vanessa, Ordelman, Roeland J.F., Friedland, Gerald, and Jones, Gareth J.F. </div> <div class="periodical"> <em>In Proceedings of the 1st ACM International Conference on Multimedia Retrieval, ICMR2011</em> Apr 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Automatically generated tags and geotags hold great promise to improve access to video collections and online communi- ties. We overview three tasks oered in the MediaEval 2010 benchmarking initiative, for each, describing its use scenario, denition and the data set released. For each task, a refer- ence algorithm is presented that was used within MediaEval 2010 and comments are included on lessons learned. The Tagging Task, Professional involves automatically matching episodes in a collection of Dutch television with subject la- bels drawn from the keyword thesaurus used by the archive sta. The Tagging Task, Wild Wild Web involves automat- ically predicting the tags that are assigned by users to their online videos. Finally, the Placing Task requires automati- cally assigning geo-coordinates to videos. The specication of each task admits the use of the full range of available in- formation including user-generated metadata, speech recog- nition transcripts, audio, and visual features.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="b797656a3c6842afb5165e83d6e03d8d" class="col-sm-8"> <div class="title">Automated Metadata Extraction for Semantic Access to Spoken Word Archives</div> <div class="author">de Jong, Franciska M.G., Heeren, W.F.L., van Hessen, Adrianus J., Ordelman, Roeland J.F., and Nijholt, Antinus </div> <div class="periodical"> <em>In Proceedings 12th International Symposium on Social Communication</em> Jan 2011 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Archival practice is shifting from the analogue to the digital world. A specific subset of heritage collections that impose interesting challenges for the field of language and speech technology are spoken word archives. Given the enormous backlog at audiovisual archives of unannotated materials and the generally global level of item description, collection disclosure and item access are both at risk, and (semi-)automated methods for analysis and annotation may help to increase the use and reuse of these rich content collections. In several HMI projects the interplay has been investigated between evolving user scenarios and user requirements for spoken audio collections on the one hand, and the potential of automatic annotation and search technology for the improved accessibility and search paradigms on the other hand. In this paper we will present an overview of the state-of-the-art in metadata generation for audio content and explain the crucial importance of involving user groups in the design of research agendas and road maps for novel applications in this domain.</p> </div> </div> </div> </li> </ol> <h2 class="year">2010</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f3ef34f30aa646619f4c37558fdae4c2" class="col-sm-8"> <div class="title">SSCS’10: Proceedings of the 2010 ACM Workshop on Searching Spontaneous Conversational Speech</div> <div class="author"> </div> <div class="periodical"> Oct 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The spoken word is a valuable source of semantic information. Techniques that exploit the spoken word by making use of speech recognition or spoken audio analysis hold clear potential for improving multimedia search. Nonetheless, speech technology remains underexploited by systems that provide access to spoken audio or video with a speech track. Indexing the spoken audio produced by speakers engaging in conversation or otherwise speaking spontaneously is particularly challenging. The challenges arise due to the wide variability and highly unstructured nature of unplanned, informal speech. Development of approaches that can effectively exploit the semantic content of spontaneous, conversational speech requires integration of speech recognition, audio processing, multimedia analysis and information retrieval. The SSCS workshop series is devoted to providing a forum where scientists engaged in spoken content retrieval research at the intersection of these disciplines can meet, present and discuss recent research results and also formulate a common vision on the future of spoken content retrieval. The research papers presented at SSCS 2010 cluster around topics that are central for spoken content retrieval. Two papers focus on specific indexing techniques applied to spontaneous speech: speaker role recognition and concept detection. Two papers treat Spoken Term Detection, addressing the challenge of terms that cannot be indexed using conventional approaches since they are not contained in the speech recognizer vocabulary (i.e., the so-called Out-Of-Vocabulary problem). Finally, three papers are devoted to topics related to the automatic segmentation of spontaneous conversational content and deal with issues involving the combination of automatic segmentation and information retrieval. SSCS 2010 continues the tradition of past years by including a demonstration session that allows hands-on interaction with systems implementing state-of-the-art approaches to spoken content retrieval. Five demonstration papers give the details of the systems presented. SSCS 2010 includes a number of presentations by invited speakers who address topics related to the user perspective on spoken content retrieval and to domains that are anticipated to give rise to future issues faced by scientists working in the field.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2f97fac5188a4506a97508756fde9823" class="col-sm-8"> <div class="title">Towards affective state modeling in narrative and conversational settings</div> <div class="author">Jochems, Bart, Larson, Martha, Ordelman, Roeland J.F., Poppe, Ronald Walter, and Truong, Khiet Phuong </div> <div class="periodical"> <em>In Proceedings of Interspeech 2010</em> Sep 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We carry out two studies on affective state modeling for communication settings that involve unilateral intent on the part of one participant (the evoker) to shift the affective state of another participant (the experiencer). The first investigates viewer response in a narrative setting using a corpus of documentaries annotated with viewer-reported narrative peaks. The second investigates affective triggers in a conversational setting using a corpus of recorded interactions, annotated with continuous affective ratings, between a human interlocutor and an emotionally colored agent. In each case, we build a "one-sided" model using indicators derived from the speech of one participant. Our classification experiments confirm the viability of our models and provide insight into useful features.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="698d0d7654f242c6853e3d5c9cb68cbd" class="col-sm-8"> <div class="title">Multimedia with a speech track: searching spontaneous conversational speech</div> <div class="author">Larson, Martha, Ordelman, Roeland J.F., de Jong, Franciska M.G., Kohler, Joachim, and Kraaij, Wessel </div> <div class="periodical"> <em>SIGIR forum</em> Aug 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>After two successful years at SIGIR in 2007 and 2008, the third workshop on Searching Spontaneous Conversational Speech (SSCS 2009) was held conjunction with the ACM Multimedia 2009. The goal of the SSCS series is to serve as a forum that brings together the disciplines that collaborate on spoken content retrieval, including information retrieval, speech recognition and multimedia analysis. Multimedia collections often contain a speech track, but in many cases it is ignored or not fully exploited for information retrieval. Currently, spoken content retrieval research is expanding beyond highly-conventionalized domains such as broadcast news in to domains involving speech that is produced spontaneously and in conversational settings. Such speech is characterized by wide variability of speaking styles, subject matter and recording conditions. The work presented at SSCS 2009 included techniques for searching meetings, interviews, telephone conversations, podcasts and spoken annotations. The work encompassed a large range of approaches including using subword units, exploiting dialogue structure, fusing retrieval models, modeling topics and integrating visual features. Taken in sum, the workshop demonstrated the high potential of new ideas emerging in the area of speech search and also reinforced the need for concentrated research devoted to the classic challenges of spoken content retrieval, many of which remain yet unsolved.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="78f937c30c8d4375b1cb543aa4498633" class="col-sm-8"> <div class="title">A system for the semantic multimodal analysis of news audio-visual content</div> <div class="author">Mezaris, Vasileios, Gidaros, Spyros, Papadopoulos, Georgios Th., Kasper, Walter, Ordelman, Roeland J.F., Steffen, Jörg, Huijbregts, M.A.H., de Jong, Franciska M.G., Kompatsiaris, Ioannis, and Strintzis, Michael G. </div> <div class="periodical"> <em>EURASIP journal on advances in signal processing</em> Feb 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>News-related content is nowadays among the most popular types of content for users in everyday applications. Although the generation and distribution of news content has become commonplace, due to the availability of inexpensive media capturing devices and the development of media sharing services targeting both professional and user-generated news content, the automatic analysis and annotation that is required for supporting intelligent search and delivery of this content remains an open issue. In this paper, a complete architecture for knowledge-assisted multimodal analysis of news-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately and proposes a novel fusion technique based on the particular characteristics of news-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic annotations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="47b63cc990f843f583517294c28857c6" class="col-sm-8"> <div class="title">Crowdsourcing rock n’ roll multimedia retrieval</div> <div class="author">Snoek, Cees G.M., Freiburg, Bauke, Oomen, Johan, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of the ACM Multimedia Conference, MM 2010</em> Feb 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this technical demonstration, we showcase a multimedia search engine that facilitates semantic access to archival rock n’ roll concert video. The key novelty is the crowdsourcing mechanism, which relies on online users to improve, extend, and share, automatically detected results in video fragments using an advanced timeline-based video player. The user-feedback serves as valuable input to further improve automated multimedia retrieval results, such as automatically detected concepts and automatically transcribed interviews. The search engine has been operational online to harvest valuable feedback from rock n’ roll enthusiasts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="d51679e3637248f4835d67c306ff2bd6" class="col-sm-8"> <div class="title">Exploiting Speech Recognition Transcripts for Narrative Peak Detection in Short-Form Documentaries</div> <div class="author">Larson, Martha, Jochems, Bart, Smits, Ewine, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009</em> Feb 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative ﬂow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers’ feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches deliver top narrative peak detection results. One uses counts of personal pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language.</p> </div> </div> </div> </li> </ol> <h2 class="year">2009</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="d92f0a84eb194e11901bd50e727b82e0" class="col-sm-8"> <div class="title">Towards Affordable Disclosure of Spoken Heritage Archives</div> <div class="author"> <em>Ordelman, Roeland</em>, Heeren, Willemijn, de Jong, Franciska, Huijbregts, Marijn, and Hiemstra, Djoerd </div> <div class="periodical"> <em>Journal of digital information</em> Dec 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken heritage archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, we at least want to provide search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition - supporting e.g., within-document search - are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is not yet satisfactory, and requires additional research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="c555886985c44720b1c71eb037779c55" class="col-sm-8"> <div class="title">Searching Multimedia Content with a Spontaneous Conversational Speech Track</div> <div class="author">Larson, Martha, Ordelman, Roeland J.F., de Jong, Franciska M.G., Kraaij, Wessel, and Kohler, Joachim </div> <div class="periodical"> <em>In Proceedings of the Third Workshop on Searching Spontaneous Conversational Speech (SSCS2009)</em> Oct 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Spoken document retrieval research effort invested into developing broadcast news retrieval systems has yielded impressive results. This paper is the introduction the proceedings of the 3rd workshop aiming at the advancement of the field in less explored domains (SSCS2009) which was organized in conjunction to the ACM Multimedia Conference in Beijing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="3ef7781d3717409fb0fb0712712935e6" class="col-sm-8"> <div class="title">Unravelling the Voice of Willem Frederik Hermans: an Oral History Indexing Case Study</div> <div class="author">Ordelman, Roeland J.F., Huijbregts, M.A.H., and de Jong, Franciska M.G. </div> <div class="periodical"> Sep 2009 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6fed06e9bbfa465e8f2a49b0471061ca" class="col-sm-8"> <div class="title">Easy Listening: Spoken Document Retrieval in CHoral</div> <div class="author">Heeren, W.F.L., van der Werff, Laurens Bastiaan, de Jong, Franciska M.G., Ordelman, Roeland J.F., Verschoor, T., van Hessen, Adrianus J., and Langelaar, Mies </div> <div class="periodical"> <em>Interdisciplinary science reviews</em> Sep 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Given the enormous backlog at audiovisual archives and the generally global level of item description, collection disclosure and item access are both at risk. At the same time, archival practice is seeking to evolve from the analogue to the digital world. CHoral investigates the role automatic annotation and search technology can play in improving disclosure and access of digitized spoken word collections during and after this transfer. The core business of the CHoral project is to design and build technology for spoken document retrieval for heritage collections. In this paper, we will argue that in addition to solving technological issues, closer attention is needed for the work-flow and daily practice at audiovisual archives on the one hand, and the state-of-the-art in technology on the other. Analysis of the interplay is needed to ensure that new developments are mutually beneficial and that continuing cooperation can indeed bring envisioned advancements.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ecb764d157224c919a41d4f4e9090a2e" class="col-sm-8"> <div class="title">Enhanced multimedia content access and exploitation using semantic speech retrieval</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., and Larson, Martha </div> <div class="periodical"> <em>In Proceedings of the Third IEEE International Conference on Semantic Computing</em> Sep 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Techniques for automatic annotation of spoken content making use of speech recognition technology have long been characterized as holding unrealized promise to provide access to archives inundated with undisclosed multimedia material. This paper provides an overview of techniques and trends in semantic speech retrieval, which is taken to encompass all approaches offering meaning-based access to spoken word collections. We present descriptions, examples and insights for current techniques, including facing real-world heterogenity, aligning parallel resources and exploiting collateral collections. We also discuss ways in which speech recognition technology can be used to create multimedia connections that make new modes of access available to users. We conclude with an overview of the challenges for semantic speech retrieval in the workflow of a real-world archive and perspectives on future tasks in which speech retrieval integrates information related to affect and appeal, dimensions that transcend topic.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="2591ae7f90534838800d8e8266d31d3f" class="col-sm-8"> <div class="title">StreetTiVo: Using a P2P XML Database System to Manage Multimedia Data in Your Living Room</div> <div class="author">Zhang, Ying, de Vries, A.P., Boncz, P., Hiemstra, Djoerd, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In APWeb/WAIM 2009</em> Apr 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>StreetTiVo is a project that aims at bringing research results into the living room; in particular, a mix of current results in the areas of Peer-to-Peer XML Database Management System (P2P XDBMS), advanced multimedia analysis techniques, and advanced information re- trieval techniques. The project develops a plug-in application for the so-called Home Theatre PCs, such as set-top boxes with MythTV or Windows Media Center Edition installed, that can be considered as programmable digital video recorders. StreetTiVo distributes compute- intensive multimedia analysis tasks over multiple peers (i.e., StreetTiVo users) that have recorded the same TV program, such that a user can search in the content of a recorded TV program shortly after its broad- casting; i.e., it enables near real-time availability of the meta-data (e.g., speech recognition) required for searching the recorded content. Street- TiVo relies on our P2P XDBMS technology, which in turn is based on a DHT overlay network, for distributed collaborator discovery, work coor- dination and meta-data exchange in a volatile WAN environment. The technologies of video analysis and information retrieval are seamlessly integrated into the system as XQuery functions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="68b0337332b44db7aa45d20092f5fcd8" class="col-sm-8"> <div class="title">Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs</div> <div class="author">Malaisé, Véronique, Gazendam, Luit, Heeren, Willemijn,  <em>Ordelman, Roeland</em>, and Brugman, Hennie </div> <div class="periodical"> <em>In Actes de la 16ème Conférence sur le Traitement Automatique des Langues Naturelles, TALN 2009, Senlis (France)</em> Apr 2009 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, given the growing amount of materials that are being created on a daily basis and the digitization of existing analogue collections, the traditional manual annotation of collections puts heavy demands on resources, especially for large audiovisual archives. One way to address this challenge, is to introduce (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords form textual resources related to the TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. Besides the descriptions of the programs published by the broadcasters on their Websites, Automatic Speech Transcription (ASR) techniques from the CATCH-CHoral project, also provide textual resources that might be relevant for suggesting keywords. This paper investigates the suitability of ASR for generating such keywords, which we evaluate against manual annotations of the documents and against keywords automatically generated from context documents.</p> </div> </div> </div> </li> </ol> <h2 class="year">2008</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="e07a3ed6cd7848c5ba93ab1a1dd7d05a" class="col-sm-8"> <div class="title">Spoken Content Retrieval: Searching Spontaneous Conversational Speech</div> <div class="author">Kohler, Joachim, Larson, Martha, de Jong, Franciska,  <em>Ordelman, Roeland</em>, and Kraaij, Wessel </div> <div class="periodical"> <em>SIGIR forum</em> Dec 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval. At the workshop, talks and posters were presented covering a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. Demonstrations of speech-based retrieval systems from a variety of application domains introduced a strong practical emphasis into the workshop program. The workshop concluded with a panel discussion, whose goal it was to identify future research directions for speech retrieval. Among the important challenges identified during the panel discussions were: dealing with large scale multimedia collections, representing audio/video content effectively in the user interface, focusing on perfecting the component technologies on which speech retrieval systems are based, and developing systems and approaches that will enable users (both content seekers and content providers) to actively create their own speech search applications or contribute to the indexability of their content.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="059549b5e3d84a6693747b6e59bbea20" class="col-sm-8"> <div class="title">Fast N-Gram Language Model Look-Ahead for Decoders With Static Pronunciation Prefix Trees</div> <div class="author">Huijbregts, M.A.H., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of Interspeech</em> Sep 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Decoders that make use of token-passing restrict their search space by various types of token pruning. With use of the Language Model Look-Ahead (LMLA) technique it is possible to increase the number of tokens that can be pruned without loss of decoding precision. Unfortunately, for token passing decoders that use single static pronunciation prefix trees, full n-gram LMLA increases the needed number of language model probability calculations considerably. In this paper a method for applying full n-gram LMLA in a decoder with a single static pronunciation tree is introduced. The experiments show that this method improves the speed of the decoder without an increase of search errors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="d852808a08d74d569d08ccc7ef956860" class="col-sm-8"> <div class="title">Towards Affordable Disclosure of Spoken Word Archives</div> <div class="author">Ordelman, Roeland J.F., Heeren, W.F.L., Huijbregts, M.A.H., Hiemstra, Djoerd, and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the ECDL 2008 Workshop on Information Access to Cultural Heritage (IACH2008)</em> Sep 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper presents and discusses ongoing work aiming at affordable disclosure of real-world spoken word archives in general, and in particular of a collection of recorded interviews with Dutch survivors of World War II concentration camp Buchenwald. Given such collections, the least we want to be able to provide is search at different levels and a flexible way of presenting results. Strategies for automatic annotation based on speech recognition – supporting e.g., within-document search– are outlined and discussed with respect to the Buchenwald interview collection. In addition, usability aspects of the spoken word search are discussed on the basis of our experiences with the online Buchenwald web portal. It is concluded that, although user feedback is generally fairly positive, automatic annotation performance is still far from satisfactory, and requires additional research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="92b2743eae8e441a99c4d1c330b11ac9" class="col-sm-8"> <div class="title">Searching Spontaneous Conversational Speech: Proceedings of ACM SIGIR Workshop (SSCS2008)</div> <div class="author">Larson, M., and Kraaij, W. </div> <div class="periodical"> Jul 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The second workshop on Searching Spontaneous Conversational Speech (SSCS 2008) was held in Singapore on July 24, 2008 in conjunction with the 31st Annual International ACM SIGIR Conference. The goal of the workshop was to bring the speech community and the information retrieval community together. The forum was designed to be conducive to the close interaction and the intense discussion necessary to promote fusion of these fields into a single discipline with a concerted vision of spoken content retrieval. The proceedings contain papers on a wide range of topics including vocabulary independent search, spoken term detection, combination of models/indexes, use of speech recognition lattices for search, segmentation, temporal analysis, benchmarking, exploitation of prosody, speech surrogates for user interfaces and multi-language collections. A workshop reprot has been published in ACM SIGIR Forum, issue December 2008.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f55b8f11b04b47a7a6f587fd1451d064" class="col-sm-8"> <div class="title">Access to recorded interviews: A research agenda</div> <div class="author">de Jong, Franciska M.G., Oard, D.W., Heeren, W.F.L., and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>ACM journal on computing and cultural heritage</em> Jun 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Recorded interviews form a rich basis for scholarly inquiry. Examples include oral histories, community memory projects, and interviews conducted for broadcast media. Emerging technologies offer the potential to radically transform the way in which recorded interviews are made accessible, but this vision will demand substantial investments from a broad range of research communities. This article reviews the present state of practice for making recorded interviews available and the state-of-the-art for key component technologies. A large number of important research issues are identified, and from that set of issues, a coherent research agenda is proposed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ef90213f87cc43d6b1626d6938037730" class="col-sm-8"> <div class="title">Affordable access to multimedia by exploiting collateral data</div> <div class="author">Heeren, W.F.L., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of CBMI 2008</em> Jun 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In addition to multimedia collections and their metadata, there often is a variety of collateral data sources available on (parts of) a collection. Collateral data – secondary information objects that relate to the primary multimedia documents – can be very useful in the process of automated generation of annotations for multimedia archives in that they reduce both costs and effort in annotation and access. Furthermore, they can be used to enhance result presentation in retrieval engines. To optimally exploit collateral data, methods for automatic indexing as well as changes in the current archiving workflow are proposed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6c824f8e13a848b982e894e9f8d7f425" class="col-sm-8"> <div class="title">Knowledge-assisted cross-media analysis of audio-visual content in the news domain</div> <div class="author">Mezaris, Vasileios, Gidaros, Spyros, Papadopoulos, Georgios Th., Kasper, Walter, Ordelman, Roeland J.F., de Jong, Franciska M.G., and Kompatsiaris, Ioannis </div> <div class="periodical"> <em>In Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008.</em> Jun 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper, a complete architecture for knowledge-assisted cross-media analysis of News-related multimedia content is presented, along with its constituent components. The proposed analysis architecture employs state-of-the-art methods for the analysis of each individual modality (visual, audio, text) separately, and proposes a fusion technique based on the particular characteristics of News-related content for the combination of the individual modality analysis results. Experimental results on news broadcast video illustrate the usefulness of the proposed techniques in the automatic generation of semantic video annotations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="c8e3e1f9008d40a1b666addd12d61c3e" class="col-sm-8"> <div class="title">Mediacampaign: A Multimodal Semantic Analysis System for Advertisement Campaign Detection</div> <div class="author">Rehatschek, Herwig, Sorschag, Robert, Rettenbacher, Bernhard, Zeiner, Herwig, Nioche, Julien, de Jong, Franciska M.G., Ordelman, Roeland J.F., and van Leeuwen, David A. </div> <div class="periodical"> <em>In Proceedings of international workshop on Content-Based Multimedia Indexing, CBMI 2008.</em> Jun 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>MediaCampaign’s scope is on discovering and inter-relating advertisements and campaigns, i.e. to relate advertisements semantically belonging together, across different countries and different media. The project’s main goal is to automate to a large degree the detection and tracking of advertisement campaigns on television, Internet and in the press. For this purpose we introduce a first prototype of a fully integrated semantic analysis system based on an ontology which automatically detects new creatives and campaigns by utilizing a multimodal analysis system and a framework for the resolution of semantic identity.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="7398d937c95e44e3a40a981649adc59b" class="col-sm-8"> <div class="title">From D-Coi to SoNaR: A reference corpus for Dutch</div> <div class="author">Oostdijk, N., Reynaert, M., Monachesi, P., van Noord, G., Ordelman, Roeland J.F., Schuurman, I., and Vandeghinste, V. </div> <div class="periodical"> <em>In Proceedings on the sixth international conference on language resources and evaluation (LREC 2008)</em> May 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The computational linguistics community in The Netherlands and Belgium has long recognized the dire need for a major reference corpus of written Dutch. In part to answer this need, the STEVIN programme was established. To pave the way for the effective building of a 500-million-word reference corpus of written Dutch, a pilot project was established. The Dutch Corpus Initiative project or D-Coi was highly successful in that it not only realized about 10% of the projected large reference corpus, but also established the best practices and developed all the protocols and the necessary tools for building the larger corpus within the confines of a necessarily limited budget. We outline the steps involved in an endeavour of this kind, including the major highlights and possible pitfalls. Once converted to a suitable XML format, further linguistic annotation based on the state-of-the-art tools developed either before or during the pilot by the consortium partners proved easily and fruitfully applicable. Linguistic enrichment of the corpus includes PoS tagging, syntactic parsing and semantic annotation, involving both semantic role labeling and spatiotemporal annotation. D-Coi is expected to be followed by SoNaR, during which the 500-million-word reference corpus of Dutch should be built.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bd299a8ef8f44d259789dd5022748c8a" class="col-sm-8"> <div class="title">The Lowlands team at TRECVID 2007</div> <div class="author">Aly, Robin, Hauff, C., Heeren, W.F.L., Hiemstra, Djoerd, de Jong, Franciska M.G., Ordelman, Roeland J.F., Verschoor, T., and de Vries, A.P. </div> <div class="periodical"> <em>In TREC Video Retrieval Evaluation Online Proceedings</em> Feb 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this report we summarize our methods and results for the search tasks in TRECVID 2007. We employ two different kinds of search: purely ASR based and purely concept based search. However, there is not significant difference of the performance of the two systems. Using neighboring shots for the combination of two concepts seems to be beneficial. General preprocessing of queries increased the performance and choosing detector sources helped. However, for all automatic search components we need to perform further investigations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="0e96bb4997934dde9c5ef3ba9f8d68ec" class="col-sm-8"> <div class="title">Browsing and Searching the Spoken Words of Buchenwald Survivors</div> <div class="author"> <em>Ordelman, Roeland</em>, Heeren, Willemijn, van Hessen, Arjan, Hiemstra, Djoerd, Hondorp, Hendri, Huijbregts, Marijn, de Jong, Franciska, and Verschoor, Thijs </div> <div class="periodical"> <em>In BNAIC 2008</em> Feb 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The ‘Buchenwald’ project is the successor of the ‘Radio Oranje’ project that aimed at the transformation of a set of World War II related mono-media documents –speeches of the Dutch Queen Wilhelmina, textual transcripts of the speeches, and a database of WWII related photographs– to an attractive online multimedia presentation of the Queen’s speeches with keyword search functionality [6, 3]. The ‘Buchenwald’ project links up and extends the ‘Radio Oranje’ approach. The goal in the project was to develop a Dutch multimedia information portal on World War II concentration camp Buchenwald1. The portal holds both textual information sources and a video collection of testimonies from 38 Dutch camp survivors with durations between a half and two and a half hours. For each interview, an elaborate description, a speaker profile and a short summary are available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="aac12f38f45a44648bfa8d362e9929ad" class="col-sm-8"> <div class="title">Evaluation of spoken document retrieval for historic speech collections</div> <div class="author">Heeren, W.F.L., de Jong, Franciska M.G., van der Werff, Laurens Bastiaan, Huijbregts, M.A.H., and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of LREC 2008</em> Feb 2008 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The re-use of spoken word audio collections maintained by audiovisual archives is severely hindered by their generally limited access. The CHoral project, which is part of the CATCH program funded by the Dutch Research Council, aims to provide users of speech archives with online, instead of on-location, access to relevant fragments, instead of full documents. To meet this goal, a spoken document retrieval framework is being developed. In this paper the evaluation efforts undertaken so far to assess and improve various aspects of the framework are presented. These efforts include (i) evaluation of the automatically generated textual representations of the spoken word documents that enable word-based search, (ii) the development of measures to estimate the quality of the textual representations for use in information retrieval, and (iii) studies to establish the potential user groups of the to-be-developed technology, and the first versions of the user interface supporting online access to spoken word collections.</p> </div> </div> </div> </li> </ol> <h2 class="year">2007</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="a293634230d54cf39df58ecb174ae8fe" class="col-sm-8"> <div class="title">Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition</div> <div class="author">Huijbregts, M.A.H., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the Second International Conference on Semantic and Digital Media Technologies, SAMT 2007</em> Dec 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="8d41512fefc54e27b73145ad2beb704d" class="col-sm-8"> <div class="title">Searching Spontaneous Conversational Speech</div> <div class="author">de Jong, Franciska M.G., Oard, Douglas W., Ordelman, Roeland J.F., and Raaijmakers, Stephan </div> <div class="periodical"> <em>SIGIR forum</em> Dec 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The ACM SIGIR Workshop on Searching Spontaneous Conversational Speech was held as part of the 2007 ACM SIGIR Conference in Amsterdam. The workshop program was a mix of elements, including a keynote speech, paper presentations and panel discussions. This brief report describes the organization of this workshop and summarizes the discussions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ace7ad79ef80411e96f413cb38ef515b" class="col-sm-8"> <div class="title">XML Information Retrieval from Spoken Word Archives</div> <div class="author">Aly, Robin, Hiemstra, Djoerd, Ordelman, Roeland J.F., van der Werff, Laurens Bastiaan, and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Evaluation of Multilingual and Multi-modal Information Retrieval</em> Sep 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This report presents the University of Twente’s first cross-language speech retrieval experiments in Cross-Language Evaluation Forum (CLEF). It describes the issues our contribution was focusing on, it describes the PF/Tijah XML Information Retrieval system that was used and it discusses the results for both the monolingual English and the Dutch-English cross-language spoken document retrieval (CL-SR) task. The paper concludes with an overview of future research plans.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="65f231a8c4f44f2da3f1955fc77c555d" class="col-sm-8"> <div class="title">Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections</div> <div class="author">Huijbregts, M.A.H., Wooters, Chuck, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of Interspeech 2007</em> Aug 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we discuss the speech activity detection system that we used for detecting speech regions in the Dutch TRECVID video collection. The system is designed to filter non-speech like music or sound effects out of the signal without the use of predefined non-speech models. Because the system trains its models on-line, it is robust for handling out-of-domain data. The speech activity error rate on an out-of-domain test set, recordings of English conference meetings, was 4.4%. The overall error rate on twelve randomly selected five minute TRECVID fragments was 11.5%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="582858066bfc4623baa68b5971305a4b" class="col-sm-8"> <div class="title">Building Detectors to Support Searches on Combined Semantic Concepts</div> <div class="author">Aly, Robin, Hiemstra, Djoerd, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proceedings of the Multimedia Information Retrieval Workshop</em> Aug 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Bridging the semantic gap is one of the big challenges in multimedia information retrieval. It exists between the extraction of low-level features of a video and its conceptual contents. In order to understand the conceptual content of a video a common approach is building concept detectors. A problem of this approach is that the number of detectors is impossible to determine. This paper presents a set of 8 methods on how to combine two existing concepts into a new one, which occurs when both concepts appear at the same time. The scores for each shot of a video for the combined concept are computed from the output of the underlying detectors. The findings are evaluated on basis of the output of the 101 detectors including a comparison to the theoretical possibility to train a classifier on each combined concept. The precision gains are significant, specially for methods which also consider the chronological surrounding of a shot promising.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="d70f9c4cc3ec4389b680a215bbe26368" class="col-sm-8"> <div class="title">Proceedings of the ACM SIGIR Workshop ”Searching Spontaneous Conversational Speech”</div> <div class="author">Raaijmakers, Stephan </div> <div class="periodical"> Jul 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Proceedings contain the contributions to the workshop on Searching Spontaneous Conversational Speech organized in conjunction with the 30th ACM SIGIR, Amsterdam 2007. The papers reflect some of the emerging focus areas and cross-cutting research topics, together addressing evaluation metrics, segmentation methods, workflow aspects, rich transcription, and robustness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="9a5c4956d83a42a4bc3fe916ab36ad52" class="col-sm-8"> <div class="title">Radio Oranje: Searching the Queen’s speech(es)</div> <div class="author">Heeren, W.F.L., van der Werff, Laurens Bastiaan, Ordelman, Roeland J.F., van Hessen, Adrianus J., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of the 30th ACM SIGIR</em> Jul 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The ‘Radio Oranje’ demonstrator shows an attractive multimedia user experience in the cultural heritage domain based on a collection of mono-media audio documents. It supports online search and browsing of the collection using indexing techniques, specialized content visualizations and a related photo database.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="7b27046ff9254ab38d6cf614d36c145b" class="col-sm-8"> <div class="title">Speech-based Annotation of Heterogeneous Multimedia Content Using Automatic Speech Recognition</div> <div class="author">Huijbregts, M.A.H., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> May 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper reports on the setup and evaluation of robust speech recognition system parts, geared towards transcript generation for heterogeneous, real-life media collections. The system is deployed for generating speech transcripts for the NIST/TRECVID-2007 test collection, part of a Dutch real-life archive of news-related genres. Performance figures for this type of content are compared to figures for broadcast news test data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="4482823629124484af5f285b702aaaf8" class="col-sm-8"> <div class="title">Radio Oranje: Enhanced Access to a Historical Spoken Word Collection</div> <div class="author">van der Werff, Laurens Bastiaan, Heeren, W.F.L., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Computational Linguistics in the Netherlands</em> Jan 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Access to historical audio collections is typically very restricted: content is often only available on physical (analog) media and the metadata is usually limited to keywords, giving access at the level of relatively large fragments, e.g., an entire tape. Many spoken word heritage collections are now being digitized, which allows the introduction of more advanced search technology. This paper presents an approach that supports online access and search for recordings of historical speeches. A demonstrator has been built, based on the so-called Radio Oranje collection, which contains radio speeches by the Dutch Queen Wilhelmina that were broadcast during World War II. The audio has been aligned with its original 1940s manual transcriptions to create a time-stamped index that enables the speeches to be searched at the word level. Results are presented together with related photos from an external database.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6621d99c26af419e8741fc1b079d5357" class="col-sm-8"> <div class="title">Speech Indexing</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., and van Leeuwen, D.A. </div> <div class="periodical"> Jan 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This chapter will focus on the automatic extraction of information from the speech in multimedia documents. This approach is often referred to as speech indexing and it can be regarded as a subfield of audio indexing that also incorporates for example the analysis of music and sounds. If the objective of the recognition of the words spoken is to support retrieval, one commonly speaks of spoken document retrieval (SDR). If the objective is on the coupling of various media types the term media mining or even cross-media mining is used. Most attention in this chapter will go to SDR. The focus is less on searching (an index of ) a multimedia database, but on enabling multiple views on the data by cross-linking all the available multifaceted information sources in a multimedia database. In section 1.6 cross-media mining will be discussed in more detail.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="42e3c5016cab421281a9029a774fffae" class="col-sm-8"> <div class="title">TwNC: a Multifaceted Dutch News Corpus</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., van Hessen, Adrianus J., and Hondorp, G.H.W. </div> <div class="periodical"> <em>ELRA Newsletter</em> Jan 2007 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This contribution describes the Twente News Corpus (TwNC), a multifaceted corpus for Dutch that is being deployed in a number of NLP research projects among which tracks within the Dutch national research programme MultimediaN, the NWO programme CATCH, and the Dutch-Flemish programme STEVIN. The development of the corpus started in 1998 within a predecessor project DRUID and has currently a size of 530M words. The text part has been built from texts of four different sources: Dutch national newspapers, television subtitles, teleprompter (auto-cues) files, and both manually and automatically generated broadcast news transcripts along with the broadcast news audio. TwNC plays a crucial role in the development and evaluation of a wide range of tools and applications for the domain of multimedia indexing, such as large vocabulary speech recognition, cross-media indexing, cross-language information retrieval etc. Part of the corpus was fed into the Dutch written text corpus in the context of the Dutch-Belgian STEVIN project D-COI that was completed in 2007. The sections below will describe the rationale that was the starting point for the corpus development; it will outline the cross-media linking approach adopted within MultimediaN, and finally provide some facts and figures about the corpus.</p> </div> </div> </div> </li> </ol> <h2 class="year">2006</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="413754af753940d0aef4cc52b9361ba0" class="col-sm-8"> <div class="title">Automated speech and audio analysis for semantic access to multimedia</div> <div class="author">de Jong, Franciska M.G., Ordelman, Roeland J.F., and Huijbregts, M.A.H. </div> <div class="periodical"> <em>In Proceedings of the First International Conference on Semantic and Digital Media Technologies, SAMT 2006</em> Dec 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The deployment and integration of audio processing tools can enhance the semantic annotation of multimedia content, and as a consequence, improve the effectiveness of conceptual access tools. This paper overviews the various ways in which automatic speech and audio analysis can contribute to increased granularity of automatically extracted metadata. A number of techniques will be presented, including the alignment of speech and text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value of the techniques and their potential contribution to the content value chain will be illustrated by the description of two (complementary) demonstrators for browsing broadcast news archives.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="7a3a0a34aa5c43c7a3c04d40d551e411" class="col-sm-8"> <div class="title">Audio Indexing Technology for the Exploration of Audiovisual Heritage Collections</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., Heeren, W.F.L., and van Hessen, Adrianus J. </div> <div class="periodical"> <em>In BNAIC’06</em> Oct 2006 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f0ce056c2a4a4735968a2dc271eaaafc" class="col-sm-8"> <div class="title">The role of automated speech and audio analysis in semantic multimedia annotation</div> <div class="author">de Jong, Franciska M.G., Ordelman, Roeland J.F., and van Hessen, Adrianus J. </div> <div class="periodical"> <em>In International IET Conference on Visual Information Engineering (VIE 2006)</em> Sep 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper overviews the various ways in which automatic speech and audio analysis can be deployed to enhance the semantic annotation of multimedia content, and as a consequence to improve the effectiveness of conceptual access tools. A number of techniques will be presented, including the alignment of text resources, large vocabulary speech recognition, key word spotting and speaker classification. The applicability of techniques will be discussed from a media crossing perspective. The added value will be illustrated by the description of two complementary demonstrators for browsing broadcast news archieves.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="5777ddfc14e54f7c8fd1b3f787fac076" class="col-sm-8"> <div class="title">Exploration of audiovisual heritage using audio indexing technology</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., and Heeren, W.F.L. </div> <div class="periodical"> <em>In Proceedings of the First European Workshop on Intelligent Technologies for Cultural Heritage Exploitation</em> Aug 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper discusses audio indexing tools that have been implemented for the disclosure of Dutch audiovisual cultural heritage collections. It explains the role of language models and their adaptation to historical settings and the adaptation of acoustic models for homogeneous audio collections. In addition to the benefits of cross-media linking, the requirements for successful tuning and improvement of available tools for indexing the heterogeneous A/V collections from the cultural heritage domain are reviewed. And finally the paper argues that research is needed to cope with the varying information needs for different types of users.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="39bb0a19dece49d19a6749106e64e487" class="col-sm-8"> <div class="title">Generating Expressive Speech for Storytelling Applications</div> <div class="author">Theune, Mariet, Meijs, Koen, Heylen, Dirk, and <em>Ordelman, Roeland</em> </div> <div class="periodical"> <em>IEEE transactions on audio, speech and language processing</em> Jul 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Work on expressive speech synthesis has long focused on the expression of basic emotions. In recent years, however, interest in other expressive styles has been increasing. The research presented in this paper aims at the generation of a storytelling speaking style, which is suitable for storytelling applications and more in general, for applications aimed at children. Based on an analysis of human storytellers’ speech, we designed and implemented a set of prosodic rules for converting "neutral" speech, as produced by a text-to-speech system, into storytelling speech. An evaluation of our storytelling speech generation system showed encouraging results.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="daf38fa1ccb44959a3f241f6694c452b" class="col-sm-8"> <div class="title">Annotating Emotions in Meetings</div> <div class="author">Reidsma, Dennis, Heylen, Dirk K.J., and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proc. of the fifth international conference on Language Resources and Evaluation, LREC 2006</em> May 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="6086ebec963044a0bec1bbb4e24af255" class="col-sm-8"> <div class="title">Annotating State of Mind in Meeting Data</div> <div class="author">Heylen, Dirk K.J., Reidsma, Dennis, and Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In Proc. of the LREC2006 Workshop on Corpora for Research on Emotion and Affect</em> May 2006 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We discuss the annotation procedure for mental state and emotion that is under development for the AMI (Augmented Multiparty Interaction) corpus. The categories that were found to be most appropriate relate not only to emotions but also to (meta-)cognitive states and interpersonal variables. The history of the development of the annotation scheme is briefly described. The discussion centers around the presentation of the procedure.</p> </div> </div> </div> </li> </ol> <h2 class="year">2005</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="cec0ab534c4d472d870f14f1ce7d0682" class="col-sm-8"> <div class="title">De stem van Willem Frederik Hermans ontrafeld. Audiovisuele archieven geïndexeerd</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>Dixit</em> Oct 2005 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="e0cf3be5bfae4e90842802ac3b495b93" class="col-sm-8"> <div class="title">Affect in Meeting Interaction</div> <div class="author">Reidsma, Dennis, Heylen, Dirk K.J., and Ordelman, Roeland J.F. </div> <div class="periodical"> Jul 2005 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="4354bc3aafdb4eaaae5b3f0aa2669ff3" class="col-sm-8"> <div class="title">A Spoken Document Retrieval Application in the Oral History Domain</div> <div class="author">Huijbregts, M.A.H., Ordelman, Roeland J.F., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of 10th international conference Speech and Computer, Patras, Greece (SPECOM 2005)</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The application of automatic speech recognition in the broadcast news domain is well studied. Recognition performance is generally high and accordingly, spoken document retrieval can successfully be applied in this domain, as demonstrated by a number of commercial systems. In other domains, a similar recognition performance is hard to obtain, or even far out of reach, for example due to lack of suitable training material. This is a serious impediment for the successful application of spoken document retrieval techniques for other data then news. This paper outlines our first steps towards a retrieval system that can automatically be adapted to new domains. We discuss our experience with a recently implemented spoken document retrieval application attached to a web-portal that aims at the disclosure of a multimedia data collection in the oral history domain. The paper illustrates that simply deploying an off-theshelf broadcast news system in this task domain will produce error rates that are too high to be useful for retrieval tasks. By applying adaptation techniques on the acoustic level and language model level, system performance can be improved considerably, but additional research on unsupervised adaptation and search interfaces is required to create an adequate search environment based on speech transcripts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="5268581029e446f3920af79b32d0ddc0" class="col-sm-8"> <div class="title">InfoLink: analysis of Dutch broadcast news and cross-media browsing</div> <div class="author">Morang, Jeroen,  <em>Ordelman, Roeland</em>, de Jong, Franciska, and van Hessen, Arjan </div> <div class="periodical"> <em>In Proceedings of IEEE International Conference on Multimedia and Expo (ICME 2005)</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper, a cross-media browsing demonstrator named InfoLink is described. InfoLink automatically links the content of Dutch broadcast news videos to related information sources in parallel collections containing text and/or video. Automatic segmentation, speech recognition and available meta-data are used to index and link items. The concept is visualised using SMIL-scripts for presenting the streaming broadcast news video and the information links.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="dd6411785af240c2b197fef1147c0a1e" class="col-sm-8"> <div class="title">Robust Audio Indexing for Dutch Spoken-word Collections</div> <div class="author">Ordelman, Roeland J.F., de Jong, Franciska M.G., Huijbregts, M.A.H., and van Leeuwen, David </div> <div class="periodical"> <em>In Proceedings of the XVIth International Conference of the Association for History and Computing (AHC2005)</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Abstract—Whereas the growth of storage capacity is in accordance with widely acknowledged predictions, the possibilities to index and access the archives created is lagging behind. This is especially the case in the oral history domain and much of the rich content in these collections runs the risk to remain inaccessible for lack of robust search technologies. This paper addresses the history and development of robust audio indexing technology for searching Dutch spoken-word collections and compares Dutch audio indexing in the well-studied broadcast news domain with an oral-history case-study. It is concluded that despite significant advances in Dutch audio indexing technology and demonstrated applicability in several domains, further research is indispensable for successful automatic disclosure of spoken-word collections.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="f06315d162444519b96c4dde9b96d259" class="col-sm-8"> <div class="title">The 2005 AMI System for the Transcription of Speech in Meetings</div> <div class="author">Hain, Thomas, Burget, Lukas, Dines, John, Gaurau, Giulia, Karafiat, Martin, Lincoln, Mike, McCowan, Iain, Ordelman, Roeland J.F., Moore, Darren, Wan, Vincent, and Renals, Steve </div> <div class="periodical"> <em>In 2nd International Workshop on Machine Learning for Multimodal Interaction, MLMI 2005</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we describe the 2005 AMI system for the transcription of speech in meetings used for participation in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve very competitive performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="44c0b02554d44b6b9acc39f05e9d29f6" class="col-sm-8"> <div class="title">The Development of the AMI System for the Transcription of Speech in Meetings</div> <div class="author">Hain, Thomas, Burget, Lukas, Dines, John, McCowan, Iain, Garau, Giulia, Karafiat, Martin, Lincoln, Mike, Ordelman, Roeland J.F., Moore, Darren, Wan, Vincent, and Renals, Steve </div> <div class="periodical"> <em>In Proceedings 2nd Workshop on Multimodal Interaction and Related Machine Learning Algorithms</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. This paper describes the development of a baseline automatic speech transcription system for meetings in the context of the AMI (Augmented Multiparty Interaction) project. We present several techniques important to processing of this data and show the performance in terms of word error rates (WERs). An important aspect of transcription of this data is the necessary flexibility in terms of audio pre-processing. Real world systems have to deal with flexible input, for example by using microphone arrays or randomly placed microphones in a room. Automatic segmentation and microphone array processing techniques are described and the effect on WERs is discussed. The system and its components presented in this paper yield competitive performance and form a baseline for future research in this domain.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="e7aae5e94aa74c4c858ca739a0ea027c" class="col-sm-8"> <div class="title">Transcription of Conference Room Meetings: an Investigation</div> <div class="author">Hain, Thomas, Dines, John, Garau, Giulia, Karafiat, Martin, Moore, Darren, Wan, Vincent, Ordelman, Roeland J.F., and Renals, Steve </div> <div class="periodical"> <em>In Proceedings of Interspeech 2005</em> Jul 2005 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. In this paper we explore the use of various meeting corpora for the purpose of automatic speech recognition. In particular we investigate the similarity of these resources and how to efficiently use them in the construction of a meeting transcription system. The analysis shows distinctive features for each resource. However the benefit in pooling data and hence the similarity seems sufficient to speak of a generic ‿conference meeting domain‿. In this context this paper also presents work on development for the AMI meeting transcription system, a joint effort by seven sites working on the AMI (augmented multi-party interaction) project.</p> </div> </div> </div> </li> </ol> <h2 class="year">2004</h2> <ol class="bibliography"></ol> <h2 class="year">2003</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="9df63b88cee048bab456c505cbffd9e8" class="col-sm-8"> <div class="title">Speech-based information retrieval for Dutch.</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>In DIR2003</em> Dec 2003 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="1164b9ce6dd546118d402c25af8546fd" class="col-sm-8"> <div class="title">Dutch Speech Recognition in Multimedia Information Retrieval</div> <div class="author">Ordelman, Roeland Jacobus Frederik </div> <div class="periodical"> Oct 2003 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As data storage capacities grow to nearly unlimited sizes thanks to ever ongoing hardware and software improvements, an increasing amount of information is being stored in multimedia and spoken-word collections. Assuming that the intention of data storage is to use (portions of) it some later time, these collections must also be searchable in one way or another. For multimedia and spoken-word collections, traditional text-oriented information retrieval (IR) strategies inevitably fall short, as the amount of textual information included with these types of documents is usually very limited. However, when automatic speech recognition (ASR) can be used to convert the speech occurring in these documents into text, textual representations can be created that in turn can be searched using the traditional text-based search strategies. As ASR systems label recognized words with exact time information as a standard accessory, detailed searching within multimedia and spoken-word collections can be enabled. This type of retrieval is commonly referred to as Spoken Document Retrieval (SDR). Typically, large vocabulary speaker independent continuous speech recognition systems (LVCSR) are deployed for creating textual representations of the spoken audio in multimedia an spoken-word collections. For Dutch however, such a system was not available when this research was started. As creating a Dutch system from scratch was not feasible given the available resources, an existing English system, refered to as the ABBOT system, was ported to Dutch. A significant part of this thesis is dedicated to a complete run-down of the porting work, involving the collection and preparation of suitable training data and the actual training and evaluation of the acoustic models and language models. The broadcast news domain was chosen as domain of focus, as this domain has also been extensively used as a benchmark domain for both international ASR research and SDR. A complicating factor for ASR in the news domain, is that word usage is highly variable. As a consequence, besides using large vocabularies, it is important to adjust these vocabularies regularly, so that they reflect the content of the news programs well. Therefore, it has been investigated which word selection strategies are best suited for making these vocabulary adjustments. Moreover, as dynamic vocabularies require a flexible generation of accurate word pronunciations, the development of a grapheme-to-phoneme converter is addressed. Another vocabulary related issue that is investigated, stems from a well-known characteristic of the Dutch language, word compounding: Dutch words can almost freely be joined together to form new words. As a result of this phenomenon, the number of distinct words in Dutch is relatively large, which reduces the coverage of vocabularies compared to those of the same size of other languages, such as English, that do not have word compounding. This thesis investigates whether splitting Dutch compound words could be a remedy for the relatively limited coverage of vocabularies, so that ASR performance could be improved. Next to a brief history of SDR research and a review of possible SDR approaches, this thesis demonstrates the use of a Dutch LVCSR in SDR by providing an illustrative example of an SDR evaluation given a collection of Dutch broadcast news shows. It is shown that Dutch speech recognition can successfully be deployed for content-based retrieval of broadcast news programs. The experience obtained with the research described in this thesis, and the experience that will emerge from future research efforts must contribute to the long-term accessibility of the increasing amount of information being stored in Dutch multimedia and spoken-word collections.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="4eedbfe2492e470daa926bec23d5c738" class="col-sm-8"> <div class="title">Compound Decomposition in Dutch Large Vocabulary Speech Recognition</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Eurospeech 2003</em> Oct 2003 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of out-of- vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="3ddf6a19e76a494fb46720224655d72a" class="col-sm-8"> <div class="title">Searching multimedia content using the spoken audio.</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>BNVKI newsletter</em> Oct 2003 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2002</h2> <ol class="bibliography"></ol> <h2 class="year">2001</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eeb646a4d3e54c35adb64725fd2ebe23" class="col-sm-8"> <div class="title">Speech Recognition for Dutch Spoken Document Retrieval</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., de Jong, Franciska M.G., and van Leeuwen, D.A. </div> <div class="periodical"> <em>In Proceedings of CBMI’01: Content-based Multimedia Indexing</em> Sep 2001 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="a11ea1d2c1954ecdbe10d7f590623609" class="col-sm-8"> <div class="title">Lexicon Optimization for Dutch Speech Recognition in Spoken Document Retrieval</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and de Jong, Franciska M.G. </div> <div class="periodical"> Jun 2001 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: the collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="8ead7d5ecc6d4d4abbbd2f5313e03b14" class="col-sm-8"> <div class="title">Lexicon optimization for Dutch speech recognition in spoken document retrieval</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and de Jong, Franciska M.G. </div> <div class="periodical"> <em>In Proceedings of Eurospeech 2001 - Scandinavia</em> Jun 2001 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: the collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bb111204457740feb413aefe852d1c98" class="col-sm-8"> <div class="title">Speech Recognition Issues for Dutch Spoken Document Retrieval</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and de Jong, Franciska M.G. </div> <div class="periodical"> Jun 2001 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper, ongoing work on the development of the speech recognition modules of a multimedia retrieval environment for Dutch is described. The work on the generation of acoustic models and language models along with their current performance is presented. Some characteristics of the Dutch language and of the target video archives that require special treatment are discussed.</p> </div> </div> </div> </li> </ol> <h2 class="year">2000</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="9b11f4fca5a143b3957b78ba9ba49304" class="col-sm-8"> <div class="title">Zoeken in historisch videomateriaal</div> <div class="author">Ordelman, Roeland J.F. </div> <div class="periodical"> <em>Informatie professional</em> Jun 2000 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>On attaching automatic search functionality to historical video archives</p> </div> </div> </div> </li></ol> <h2 class="year">1999</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="256a8871047f486781e1cc8e82b22ec9" class="col-sm-8"> <div class="title">Dealing with Phrase Level Co-Articulation (PLC) in speech recognition: a first approach</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and van Leeuwen, David A. </div> <div class="periodical"> <em>In Proceedings of the ESCA ETRW Workshop Accessing Information in Spoken Audio</em> Feb 1999 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Whereas nowadays within-word co-articulation effects are usually sufficiently dealt with in automatic speech recognition, this is not always the case with phrase level co-articulation effects (PLC). This paper describes a first approach in dealing with phrase level co-articulation by applying these rules on the reference transcripts used for training our recogniser and by adding a set of temporary PLC phones that later on will be mapped on the original phones. In fact we temporarily break down acoustic context into a general and a PLC context. With this method, more robust models could be trained because phones that are confused due to PLC effects like for example /v/-/f/ and /z/-/s/, receive their own models. A first attempt to apply this method is described.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="99a7c06dba7b419d821774495783e9b7" class="col-sm-8"> <div class="title">Improving Recognition Performance Using Co-articulation Rules on the Phrase Level: A First Approach</div> <div class="author">Ordelman, Roeland J.F., van Hessen, Adrianus J., and van Leeuwen, D.A. </div> <div class="periodical"> <em>In Proceedings of the 14th International Congress of Phonetic Sciences</em> Feb 1999 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">1998</h2> <ol class="bibliography"></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Roeland Ordelman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: September 15, 2022. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>